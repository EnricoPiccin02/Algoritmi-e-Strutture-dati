\documentclass[a4paper]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\selectlanguage{italian}
\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage{circuitikz}
\usetikzlibrary{positioning, circuits.logic.US}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary {shapes.gates.logic.US, shapes.gates.logic.IEC, calc}
\tikzset {branch/.style={fill, shape = circle, minimum size = 3pt, inner sep = 0pt}}
\usetikzlibrary{matrix,calc}
\usepackage{multirow}
\usepackage{float}
\usepackage{geometry}
\usepackage{tabularx}
\usepackage{pgf-pie}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color, soul}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\graphicspath{ {./img/} }
\newtheorem{theorem}{Teorema}[section]
\newtheorem{corollary}{Corollario}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

% Specifiche
\geometry{
 a4paper,
 top=20mm,
 left=30mm,
 right=30mm,
 bottom=30mm
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO]{\nouppercase{\leftmark}}
\fancyfoot[CE, CO]{\thepage}
\addtolength{\headheight}{1em}
\addtolength{\footskip}{-0.5em}

\newcommand{\quotes}[1]{``#1''}
\renewcommand\tabularxcolumn[1]{>{\vspace{\fill}}m{#1}<{\vspace{\fill}}}
\renewcommand\arraystretch{}
\newcolumntype{P}{>{\centering\arraybackslash}X}

\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
\algtext*{Indent}
\algtext*{EndIndent}

\title{\textbf{Università di Trieste\\ \vspace{1em}
Laurea in ingegneria elettronica e informatica}}
\author{Enrico Piccin - Corso di Algoritmi e Strutture dati - Prof. Andrea Sgarro}
\date{Anno Accademico 2021/2022 - 2 Marzo 2022}

\begin{document}

\vspace{-10mm}
\maketitle

\tableofcontents
\newpage

\noindent
\begin{center}
  2 Marzo 2022
\end{center}

\section{Introduzione}
\textbf{Algoritmo} è una parola molto antica, non connessa all'utilizzo e all'invenzione del calcolatore.\\
Alla base della teoria della computazione si pone il \textbf{Liber abaci} (tradotto \quotes{Libro della computazione}), scritto nel $1200$ da Leonardo Bonacci, in contatto con la popolazione araba, in quanto mercante; egli è venuto a conoscenza della \textbf{numerazione araba}, introducendola in Occidente e spiegandola dettagliatamente all'interno del \textbf{Liber abaci}.\\
La numerazione araba è uno straordinario passo in avanti nella scienza, in quanto con essa viene introdotto il concetto di \textbf{notazione posizionale}, così come l'importanza del numero $0$: i numeri non servono solamente per contare, come si pensava in precedenza, e per questo rinnegando il numero $0$.\\
A Firenze, sempre negli stessi anni, ci fu una \textbf{protesta sindacale} contro l'innovazione tecnologica, contro questa nuova scoperta, facendo pressione affinché il governo abolisse il nuovo sistema di numerazione, in quanto avrebbe fatto perdere il posto di lavoro a tutti coloro che prima eseguivano difficili calcoli con la numerazione romana: tuttavia, tale proteste, com'é noto, possono rallentare il progresso, ma mai arrestarlo.\\
Leonardo Bonacci, nei suoi viaggi in Oriente, venne a conoscenza del \textbf{Liber abaci} di Al-Gorasmy, proveniente dalla Coresmia, ma che parlava persiano, da cui poi sarebbe stato tratto il nome \textbf{Algoritmo}, che letteralmente significa \textbf{procedimento di calcolo}.\\

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{ALGORITMO}}\\
    \parbox{\linewidth}{Algoritmo significa letteralmente \textbf{procedimento di calcolo}. Tuttavia, bisogna chiarire che un algoritmo è un procedimento di calcolo non necessariamente numerico, ma molto più generale, che va ben al di là dei numeri.\\
    Un altro importante elemento che contraddistingue l'algoritmo è la \textbf{meccanicità}, ovvero la sua esecuzione può essere affidata ad una macchina: ciò significa che un algoritmo non deve necessariamente essere meccanizzato, ma deve essere \textbf{meccanizzabile}; in altre parole, l'esecuzione (bada bene, l'esecuzione e non la sua ideazione) dell'algoritmo è completamente \textbf{stupida}.
    \vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\vspace{1em}
\noindent
\textbf{Esempio 1}: L'algoritmo della moltiplicazione è molto chiaro e semplice: basta solamente accedere ad una \textbf{base di dati} in cui sono memorizzati i prodotti elementari fra numeri molto piccoli, e quindi molto più semplici da trattare.\\
Una volta eseguite le operazioni di moltiplicazione tramite quanto esposto in precedenza, è necessario eseguire delle operazioni di addizione, che era necessario aver precedentemente memorizzato.\\
Ecco che quello che si è appena eseguito è un \textbf{procedimento di calcolo}.

\vspace{1em}
\noindent
\textbf{Esempio 2}: L'algoritmo della divisione fa sempre uso di una base di dati, nella quale devono essere memorizzati i risultati del prodotto del divisore con tutti i numeri decimali da $0$ a $9$ e confrontare ciascun prodotto con il termine da dividere per ottenere il quoziente.\\
Alternativamente, si sarebbe potuto creare un ciclo da $0$ a $9$ in cui per ogni indice si sarebbe dovuto verificare se questo fosse il fattore moltiplicativo corretto per ottenere la quantità giusta da sottrarre.

\vspace{1em}
\noindent
\textbf{Osservazione}: In ciascuno di tali esempi è essenziale la meccanicità del processo esecutivo, che appare evidente.

\vspace{1em}
\noindent
Quando si rappresentano delle quantità e, a maggior ragione, quando si effettuano dei calcoli, è fondamentale fissare una base di rappresentazione, da cui poi dipendono le cifre che si possono impiegare per la rappresentazione stessa.\\
La notazione posizionale permette anche di comprendere la rappresentazione di qualsiasi quantità con qualsiasi base, effettuando anche delle conversioni di base a seconda della maggiore o minore convenienza di rappresentazione.\\
Per esempio, volendo convertire una quantità rappresentata in base $\mathcal{B} = 7$ in una base $\mathcal{C} = 10$ si deve procedere come segue
\[\left(5203\right)_7 = 5 \cdot 7^3 + 2 \cdot 7^2 + 0 \cdot 7^1 + 3 \cdot 7^0 = 1715 + 98 + 0 + 3 = \left(1816\right)_{10}\]
Ovviamente le basi di rappresentazione sono almeno binarie, in quanto la \textbf{base unaria} non può, per ovvie ragioni, rappresentare alcuna quantità se non quella unica che viene permessa dalla base scelta, ossia lo $0$.\\
Tuttavia, il processo inverso, atto a passare dalla rappresentazione di una quantità in base $10$ ad una in base $3$, non risulta essere così immediato.\\
Per cercare un algoritmmo che permette di effettuare tale conversione, si effettua un primo \textbf{passaggio controintuitivo} (che suggerisce, tuttavia, il corretto processo esecutivo), che prevede di rappresentare una quantità in base $10$ in una quantità ancora in base $10$, tramite un processo di divisioni successive. Si consideri, a tal proposito
\[\left(3412\right)_{10}\]
e si divida progessivamente tale numero per $10$, come segue

\begin{table}[H]
  \centering
  \rowcolors{1}{white}{white}
  \begin{tabular}{r|l}
    $3412$ & $10$\\
    \hline
    $341$ & $2$\\
    $34$  & $1$\\
    $3$   & $4$\\
    $0$   & $3$
  \end{tabular}
\end{table}

\vspace{1em}
\noindent
Leggendo, ora, i resti, al contrario si ottiene il numero cercato all'inizio. Se ora si prova a considerare un'altra base, come $3$, l'operazione porta ad un risultato analogo

\begin{table}[H]
  \centering
  \rowcolors{1}{white}{white}
  \begin{tabular}{r|l}
    $3412$ & $3$\\
    \hline
    $1137$ & $1$\\
    $379$  & $0$\\
    $126$  & $1$\\
    $42$   & $0$\\
    $14$   & $0$\\
    $4$    & $2$\\
    $1$    & $1$\\
    $0$    & $1$\\
  \end{tabular}
\end{table}

\vspace{1em}
\noindent
Per cui si è ottenuto
\[\left(3412\right)_{10} = \left(11200101\right)_3\]
Scegliendo la base $2$ si ottiene, per esempio

\begin{table}[H]
  \centering
  \rowcolors{1}{white}{white}
  \begin{tabular}{r|l}
    $241$ & $2$\\
    \hline
    $120$ & $1$\\
    $60$  & $0$\\
    $30$  & $0$\\
    $15$  & $0$\\
    $7$   & $1$\\
    $3$   & $1$\\
    $1$   & $1$\\
    $0$   & $1$\\
  \end{tabular}
\end{table}

\vspace{1em}
\noindent
Per cui si è ottenuto
\[\left(241\right)_{10} = \left(11110001\right)_2\]
Ovviamente la lunghezza di rappresentazione in base $2$ prevede un numero di cifre pari a circa il triplo di quelle impiegate per rappresentare la medesima quantità in base $10$, proprio perché
\[\log_2(10) \cong 3.3\]
Per passare da base $10$ a base $100$, le operazioni sono molto semplici
\[\left(375712\right)_{10} = \left[\left(37\right) \left(57\right) \left(12\right)\right]_{100}\]
usando come simboli
\[\left(00\right), \left(01\right), ..., \left(75\right), ..., \left(99\right)\]
Si consideri, ora la base $8$ e si scriva un numero binario in base ottale:
\[\left(010101010\right)_2 = \left[\left(010\right) \left(101\right) \left(010\right) \right]_8 = \left(252\right)_8\]
Ancora una volta, le cifre impiegate per la rappresentazione sono state ridotte ad un terzo, sempre perché
\[\log_2(8) = 3\]
E se ora si volesse impiegare la base $16$ si otterrebbe:
\[\left(010101010\right)_2 = \left[\left(1010\right) \left(1010\right) \right]_{16} = \left(\text{AA}\right)_{16}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si consideri una lunghezza $l = 5$. Allora usando $5$ cifre, non tutte nulle, in base $10$, i numeri $n$ che si possono rappresentare sono
\[10000 \leq n \leq 99999 \hspace{1em} \equiv \hspace{1em} 10^4 \leq n < 10^5 \hspace{1em} \equiv \hspace{1em} 10^{l - 1} \leq n < 10^l\]
Da ciò si può estrapolare un risultato importante
\[\log_{10}\left(10^{l - 1}\right) \leq \log_{10}\left(n\right) < \log_{10}\left(10^l\right) \hspace{1em} \equiv \hspace{1em} l - 1 \leq \log_{10}(n) < l\]
che è una relazione esatta. Tuttavia, approssimativamente, si può scrivere che
\[l_{10}(n) \cong \log_{10}(n)\]

\newpage
\noindent
\begin{center}
  3 Marzo 2022
\end{center}
Com'è noto, il matematico indiano \textbf{Ramanujan} ha affermato che la matematica esatta non rappresenta una base solida per la realtà, mentre la matematica vera è fatta di approssimazioni.\\
\textbf{Hardy} scoprì quanto fosse importante lo studio di \textbf{Ramanujan} e insieme a lui portò avanti la teoria dei numeri, una teoria \textbf{asintotica} che, come lui stesso affermava, non può essere esatta, ma fatta di approssimazioni.\\
Se, per esempio, si considera una quantità scritta in base $5$, quale $n = \left(412\right)_5$
\[\left(412\right)_5 = 4 \cdot 5^2 + 1 \cdot 5^1 + 2 \cdot 5^0 = (107)_{10}\]
Se, ora, si fissa una lunghezza $l = 4$, una lunghezza rigida, senza considerare zeri in testa, si può capire che con $4$ cifre si possono rappresentare, in base $5$ numeri $n$ nell'intervallo
\[1000 \leq n < 10000\]
ovvero tale per cui
\[5^{l - 1} \leq n < 5^l\]
e ciò funziona con qualsiasi base, per cui, in generale, fissata una lunghezza $l$ e una base $\mathcal{B}$ si ha che le quantità che possono essere rappresentate con $l$ cifre, non tutte uguali a $0$ è
\[\mathcal{B}^{l - 1} \leq n < \mathcal{B}^l\]
Traducendo tale risultato tramite il logaritmo in base $\mathcal{B}$, sfruttando la crescenza in senso stretto della funzione logaritmica, si ottiene, equivalentemente
\[l - 1 \leq \log_{\mathcal{B}}\left(n\right) < l\]
in cui, ovviamente,
\[\log_{\mathcal{B}}\left(n\right) < l \leq \log_{\mathcal{B}}\left(n\right) + 1\]
che si può scrivere che
\[\l_{\mathcal{B}}\left(n\right) \cong \log_{\mathcal{B}}\left(n\right)\]
Per cui l'\textbf{errore massimo} che si può commettere è di $1$ cifra in base $\mathcal{B}$, nel caso peggiore, ma sarà sempre un po' maggiore del $\log_{\mathcal{B}}\left(n\right)$, per cui il logaritmo è una \textbf{sottostima della lunghezza}. Tuttavia, nello spirito di Hardy, sarà utile anche scrivere che la lunghezza binaria di $n$ è circa uguale al logaritmo binario di $n$, ovvero
\[\log_\mathcal{B}(n) \cong \log_\mathcal{B}(n)\]
in cui si può interpetare il $\log_\mathcal{B}(n)$ come una \textbf{lunghezza analogica}, mentre $l_\mathcal{B}(n)$ è una \textbf{lunghezza digitale}, in quanto \textbf{intera}, con precisione alla cifra (senza nulla in mezzo): in molti casi sarà più utile la lunghezza analogica di quella digitale, in quanto molto più precisa.\\
Grazie a questa formula è possibile capire facilmente come si alterano le lunghezze quando si effettua un cambiamento di base. Per esempio, si può osservare che
\[\log_2(10) \cong 3.38\]
per cui la lunghezza in base $2$ è circa tre volte la lunghezza in base $10$.

\vspace{2em}
\noindent
\textbf{Esempio}: Per trasformare un numero da base $10$ in base $5$ si deve procedere per divisioni successive per $5$, considerando i resti (per questo si parla di \emph{divisione intera}). Per esempio si ha che
\[10 \div 3 = 3 \text{ con resto di } 1\]
in cui, ovviamente, il resto $r$ può essere
\[0 \leq r < D\]
con $D$ divisore. Convertendo $32$ da base $10$ a base $5$ ci si aspetta di ottenere un resto $0 \leq r \leq 4$.

\vspace{1em}
\noindent
\subsection{Architettura dei calcolatori}
Il calcolatore, naturalmente, si basa sulla logica binaria, ovvero opera impiegando la rappresentazione in base $2$.\\
Il metodo più utilizzato per rappresentare caratteri diversi da quelli binari, tramite una codifica binaria, è il metodo ASCII (dall'inglese, American Standard Code For Information Interchange). Naturalmente, siccome la codifica tramite ASCII fa uso di soli $7$ bit (sarebbero $8$, ma un bit è riservato alla parità, per la rilevazione degli errori), il numero di $n$-uple binarie che si possono ottenere è $2^7$, un numero certamente irrisorio per la rappresentazione di tutti i caratteri alfanumerici necessari per la comunicazione multilinugua.\\
In generale, fissata una lunghezza $n$, il numero di $n$-uple binarie distinte è, ovviamente, $2^n$, che rappresenta una crescita esponenziale, praticamente infinita, anche se, ovviamente, in teoria sono un numero ben limitato.\\

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri una macchina calcolatrice che considera delle istruzioni di $9$ bit come di seguito esposto:

\begin{table}[H]
  \centering
  \renewcommand\arraystretch{1.2}
  \begin{tabularx}{0.6 \textwidth}{|P|P|}
    \hline
    NOME ISTRUZIONE & ISTRUZIONE\\
    \hline
    ADD & $010\times\times\times\times\times\times$\\
    \hline
    PUNCH & $100\times\times\times\times\times\times$\\
    \hline
  \end{tabularx}
  \caption{Tabella di istruzioni operative per un calcolatore}
  \label{tab:tabella_istruzioni_calcolatore}
\end{table}

\noindent
Naturalmente, tale linguaggio è \textbf{Assembly}, ovvero un linguaggio molto simile al linguaggio macchina, che risulta particolarmente complesso da impiegare per lo sviluppo di software.

\vspace{1em}
\subsection{Diagramma di flusso}
Si consideri il seguente \textbf{flowchart}, o \textbf{diagramma di flusso}:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[node distance=2cm]
    % start
    \tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!30]
    % input/output
    \tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
    % process
    \tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=orange!30]
    % if
    \tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
    % arrow
    \tikzstyle{arrow} = [thick,->,>=stealth]

    \node (start) [startstop] {Start};
    \node (in1) [io, below of=start] {Input};
    \node (pro1) [process, below of=in1] {Processo 1};
    \node (dec1) [decision, below of=pro1] {Decisione 1};

    \draw [arrow] (start) -- (in1);
    \draw [arrow] (in1) -- (pro1);
    \draw [arrow] (pro1) -- (dec1);
  \end{tikzpicture}
  \caption{Diagramma di flusso}
  \label{fig:diagramma_flusso}
\end{figure}

\noindent
Tuttavia, tale tecnica di progettazione algoritmica è oramai superata, lasciando il posto allo \textbf{pseudocodice}, ossia un linguaggio di definizione delle istruzioni slegato da qualsiasi specifico linguaggio di programmazione di riferimento, che permette di esporre una serie di istruzioni esecutive molto simili a quelle di un programma vero e proprio.

\newpage
\section{Ordinamento (sorting)}
Si espongono, di seguito, i principi di algoritmica dei più importanti algoritmi di ordinamento.\\
Ciascuno di tali algoritmi prevede di effettuare l'ordinamento di $n$ numeri forniti come input, in modo debolmente crescente, in caso di uguaglianza.

\vspace{1em}
\subsection{Bubble-Sort}
Si espone di seguito l'algoritmo di ordinamento \textbf{bubble-sort} impiegando lo \emph{pseudocodice}:

\begin{algorithm}[H]
  \caption{Bubble-sort}\label{euclid}
  \begin{algorithmic}[1]
    \State \textbf{do} the following $n-1$ times
    \Indent
      \State \emph{point} to the $1^{\text{st}}$ element
      \State \textbf{do} the following $n-1$ times
      \Indent
      \State \emph{compare} with next
      \State \textbf{if} wrong order \emph{exchange}
      \State \emph{point} to the next
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
Naturalmente tale algoritmo è corretto e lo si può verificare immediatamente, considerando, per esempio, i seguenti $5$ elementi, così ordinati:
\[\boxed{2} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5}\]
Ovviamente il procedimento ci porta ad eseguire l'algoritmo $4$ volte. Nella prima iterazione si ottiene
\[\boxed{2} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5}\]
la seconda iterazione, invece, porta ad ottenere
\[\boxed{1} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5}\]
mentre le ultime due iterazioni sono superflue. Si capisce facilmente che tale algoritmo non risulta essere pienamente efficiente, in quante alcune iterazioni potrebbero essere evitate, tramite un \textbf{flag}, per esempio. Allo stato attuale, il numero delle iterazioni da eseguire è
\[\#\text{iterazioni} = \left(n-1\right)\cdot\left(n-1\right)\]
Se, invece, si facesse in modo di evitare alcune iterazioni si avrebbe un numero di iterazioni
\[\left(n-1\right) \leq \#\text{iterazioni} \leq \left(n-1\right)^2\]
considerando $n-1 \cong n$, e quindi $\left(n-1\right)^2 \cong n^2$ si può dire che la complessità del bubble-sort è \textbf{quadratica}, in quanto il numero delle iterazioni è $n^2$.

\newpage
\noindent
\begin{center}
  4 Marzo 2022
\end{center}
L'algoritmo bubble-sort non viene utilizzato, ad oggi, così come non si impiega lo pseudocodice in \emph{pseudo-english} (da leggere psude-inglish). Esso è funzionale, ma non efficiente, in quanto la sua complessità è $n^2$.\\
Ecco che per definire un algoritmo di ordinamento non è necessaria solamente la sua funzionalità, ma anche l'efficienza.

\vspace{1em}
\subsection{Insertion-sort}
L'insertion-sort è un algoritmo di ordinamento che prevede di considerare ciascuna quantità da ordinare ad una ad una e di effettuare un confronto solo quando ci sono dei cambiamenti.\\
La prima quantità è ovviamente già in ordine con se stessa. Se la seconda è più piccola della prima, si effettua uno scambio, per cui ora i primi due numeri sono ordinati. Si considera, ora, il terzo numero e se questo è più piccolo del secondo si effettua uno scambio e un nuovo confronto tra la seconda e la prima e così via.\\
Pertanto si effettuano tutti i confronti solamente quando si ha uno scambio delle quantità: questo comporta che il minimo numero di iterazioni è $n-1$, se $n$ è il numero delle quantità da ordinare. Se, invece, tutte le quantità sono in disordine si effettua un numeri di iterazioni pari
\[1 + 2 + ... + (n - 2) + (n - 1) = \frac{n \cdot (n + 1)}{2}\]
una formula molto semplice che Gauss determinò come segue, ovverosia scrivendo la somma dei numeri da $1$ a $k$ in ordine crescente e poi decrescente, come mostrato di seguito:

\begin{table}[H]
  \centering
  \rowcolors{1}{white}{white}
  \setlength{\tabcolsep}{5pt}
  \begin{tabular}{cccccccccccccccccccccc}
    $1$ & $+$ & $2$ & $+$ & $3$ & $+$ & $...$ & $+$ & $k-1$ & $+$ & $k$\\
    $k$ & $+$ & $k-1$ & $+$ & $k-2$ & $+$ & $...$ & $+$ & $2$ & $+$ & $1$
  \end{tabular}
\end{table}

\noindent
essendo $k$ numeri in ambedue le righe, sommando i due termini corrispondenti, uno sotto l'altro, si ottiene sempre $k+1$ che, sommato per $k$ volte produce $k \cdot (k+1)$. Tuttavia, dal momento che tale quantità è il doppio di quella richiesta si ottiene
\[\frac{k \cdot (k + 1)}{2}\]
Pertanto si ha che il numero di iterazioni dell'algoritmo \emph{insertion-sort} è
\[n \cong n - 1 \leq \#\text{iterazioni} \leq \frac{n \cdot (n - 1)}{2} \cong n^2\]
che, in maniera approssimata è
\[n \leq \#\text{iterazioni} \leq n^2\]
pertanto, nel caso migliore, la \textbf{complessità è lineare}, mentre nel caso peggiore, la \textbf{complessità è quadratica}.

\vspace{1em}
\noindent
\subsection{Pseudocodice}
Per la scrittura dello pseudocodice si devono impiegare delle notazioni e dei simboli ben specifici, che di seguito vengono riportati.

\vspace{1em}
\subsubsection{Assegnazione}
L'\textbf{assegnazione} viene indicata con il simbolo $=$ (oppure $:=$ o $\leftarrow$), anche se l'assegnazione non è un'uguaglianza. Per esempio, la notazione
\[A = 3\]
significa che nella cella di memoria $A$ viene inserito il valore $3$. Analogamente, se si scrive
\[A = A + 1\]
significa che il valore presente nella cella di memoria $A$ viene incrementato di $1$ unità.

\vspace{1em}
\subsubsection{Condizione}
La specifica della \textbf{condizione} avviene tramite l'istruzione \textbf{if}, secondo la notazione seguente:
\begin{center}
  \textbf{if} $C$ \textbf{then}\\
  \hspace{4em} istruzioni\\
  \hspace{-2.5em} \textbf{else}\\
  \hspace{4em} istruzioni\\
\end{center}

\vspace{1em}
\subsubsection{Ciclo for}
Il \textbf{ciclo for} è un'istruzione di ciclo in cui vengono indicate specificatamente le iterazioni che devono essere eseguite, secondo la notazione seguente
\begin{center}
  \textbf{for} $i = 0$ \textbf{to} $n$ \textbf{do}\\
\end{center}

\vspace{1em}
\subsubsection{Ciclo while}
Il \textbf{ciclo while} è un'istruzione di ciclo in cui si effettuano le istruzioni fintantoché la condizione specifcata è vera
\begin{center}
  \textbf{while} $C$ \textbf{do}\\
\end{center}

\vspace{1em}
\noindent
Si consideri lo pseudocodice dell'algoritmo \textbf{INSERTION-SORT(A)}, esposto di seguito, dove \textbf{A} sta ad indicare \textbf{array}, ovvero un record di $length[A] = n$ valori da ordinare, già forniti in input.\\
Di seguito si espone lo pseuodocodice, in cui si parte da $1$ come posizione iniziale dell'array:

\begin{algorithm}[H]
  \caption{Insertion-sort}\label{euclid}
  \begin{algorithmic}[1]
    \State \textbf{for} $j = 2$ \textbf{to} $length[A] = n$
    \Indent
      \State \textbf{do } $key = A[j]$
        \Indent
          \State ...
          \State $i = j - 1$
        \EndIndent
        \State \textbf{while} $i > 0 \wedge A[i] > key$
        \Indent
          \State \textbf{do } $A[i+1] = A[i]$
          \Indent
            \State $i = i - 1$
            \State $A[i+1] = key$
          \EndIndent
        \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
Tale codice é concluso e il suo funzionamento può essere facilmente verificato come segue, considerando l'array $A$ di lunghezza $length[A] = 6$:
\[\boxed{5} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{3}\]
Partendo con $j=2$ si fissa $key=A[j]=2$ e imponendo $i=1$, si entra all'interno del ciclo \emph{while}, in quanto $i>0$ e $A[i]>key$ e si effettua l'istruzione $A[i+1] = A[i]$ e $A[i+1] = key$, trovandosi nella configurazione seguente
\[\boxed{2} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{3}\]
Terminato il ciclo while e il ciclo for si procede con $j=3$ si fissa $key=A[j]=4$ e imponendo $i=2$, si entra all'interno del ciclo \emph{while}, in quanto $i>0$ e $A[i]>key$ e si effettua l'istruzione $A[i+1] = A[i]$ e $A[i+1] = key$, trovandosi nella configurazione seguente
\[\boxed{2} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{3}\]
Adesso, terminato il ciclo while e for, si considera $j=4$, specificando $key=A[j]=6$ e $i=3$. In questo caso, tuttavia, non si entra nel ciclo while, in quanto $i > 0$, ma $A[i] < key$. Si procede direttamene con $j=5$, $key=A[j]=1$ e $i=4$ e si entra nel ciclo while, compiendo tutte le iterazioni
\[\boxed{2} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{3}\]
\[\boxed{2} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{3}\]
\[\boxed{2} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{3}\]
\[\boxed{1} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{3}\]
e così via fino ad arrivare all'ordinamento finale
\[\boxed{1} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{6}\]
Ecco che, come si può vedere, tale algoritmo ha una complessità che, nel caso migliore, è \textbf{lineare} ($n$) e nel caso peggiore è \textbf{quadratica} ($n^2$).\\
Mentre si chiama \textbf{complessità tipica} la \textbf{complessità media}, ovvero la complessità dell'algoritmo nel caso intermedio. In questo caso la complessità tipica è $n^2$, che può essere calcolata, in maniera non propriamente corretta, come segue
\[\frac{n + n^2}{2} = n^2\]
Esiste, infine, anche una \textbf{complessità empirica}, basata sull'uso pratico dell'algoritmo: in particolare, l'algoritmo insertion-sort funziona particolarmente bene quando il \textbf{numero degli elementi da ordinare è ridicolmente basso}, il che potrebbe essere un controsenso; tuttavia, potrebbe essere particolarmente utile ricorrere all'ordinamento di pochi numeri all'interno di una procedura particolarmente complessa: ecco, allora, che l'utilizzo di insertion-sort diviene conveniente (cosa che non accade per bubble-sort).

\vspace{1em}
\noindent
\textbf{Osservazione}: È importante osservare che l'algoritmo di insertion-sort è un \textbf{algoritmo di ordinamento in loco}, ovvero tale per cui non si impiega un altro array per l'ordinamento, ma tutte le operazioni si effettuano sullo stesso array di partenza.

\vspace{1em}
\noindent
\textbf{Osservazione}: Quando si parla di algoritmica, non è possibile parlare di \textbf{completezza} senza parlare di \textbf{complessità}.

\newpage
\section{Grafi}
Il \textbf{grafo} è una \textbf{struttura finita}. Gli elementi costituitvi di un grafo sono i \textbf{vertici} (o \textbf{nodi}) e gli \textbf{archi} (o \textbf{lati}) (dall'inglese \emph{arcs} o \emph{edges}). Per indicare i vertici si impiega la lettera $v$, mentre per indicare gli archi si usa la lettera $\xi$.\\
Un arco collega due vertici distinti che, per il momento, non è orientato, non rappresenta una freccia, in quanto si parla di \textbf{grafi semplici}, come illustrato di seguito:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$a$};
    \node[main node] (b) [below left of=a] {$b$};
    \node[main node] (c) [below right of=b] {$c$};
    \node[main node] (d) [below right of=a] {$d$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (b)
          edge [bend left, out=100, in=85, out looseness=2, in looseness=2] node[left] {} (c)
          % edge [loop above] node {0.1} (1)
      (b) edge node [right] {} (c)
          edge node {} (d)
          % edge [loop left] node {0.4} (2)
          % edge [bend right] node[left] {0.1} (3)
      (c) edge node [right] {} (d)
          % edge [bend right] node[right] {0.2} (4)
      (d) edge node [left] {} (a);
          % edge [loop right] node {0.6} (4)
          % edge [bend right] node[right] {0.2} (1);
  \end{tikzpicture}
  \caption{Esempio di grafo semplice}
  \label{fig:esempio_grafo_semplice}
\end{figure}

\vspace{1em}
\noindent
Il calcolo della copertura dei vertici prende il nome di \textbf{vertex cover} e prevede di definire il numero minimo di vertici essenziali per coprire tutti gli archi. L'ottimizzazione del grafo, in questo caso, prevede di determinare la copertura minima.\\
Si supponga di avere a disposizione $k$ vertici (che si indica come $\left \vert v \right \vert = k$, in cui $\left \vert v \right \vert$ rappresenta la \textbf{cardinalità} dell'insieme dei vertici). Naturalmente, la copertura minima pari a $0$ si ha quando i vertici del grafo sono tutti scollegati, ovvero non ci sono archi.\\
Il numero di archi in un grafo completo è pari a
\[\frac{\left \vert v \right \vert \cdot \left(\left \vert v \right \vert - 1\right)}{2}\]
per cui si potrebbe, in questo caso limite, affermare che la copertura minima sia $k$, ma se si elimina un nodo ancora la copertura sussiste, in quanto su ogni arco vi sarà sempre almeno un nodo coperto. Quindi si può affermare che la \emph{vertex cover}, nel caso generale è compresa tra $0$ e $k-1$, con $k$ numero dei vertici.

\newpage
\noindent
\begin{center}
  9 Marzo 2022
\end{center}
Il problema del \textbf{vertex cover}, ovvero di \quotes{ricoprimento dei vertici}, è un proplema che riguarda la teoria dei grafi.\\
Gli elementi costituitvi di un grafo sono i \textbf{vertici} (o \textbf{nodi}, molto più raramente chiamati \emph{punti}) e gli \textbf{archi} (dall'inglese \emph{edges}, traducibili in \textbf{spigoli} o, più impropriamente, in \emph{lati}), per il momento non orientati, che collegano due vertici, per il momento necessariamente distinti.\\
La notazione per indicare vertici e archi è la seguente
\begin{itemize}
  \item L'insieme dei vertici si denota con $v$
  \item L'insieme degli archi si denota con $\xi$
\end{itemize}
Naturalmente, sussiste la possibilità che in un grafo tutti i vertici siano sconnessi ed \textbf{isolati}, ovvero il numero degli archi sia nullo, per cui si ottiene che $\left \vert \xi \right \vert = 0$.\\
Analogamente, volendo collegare tutti i nodi con un arco (ottenendo un \textbf{grafo completo}), si procede come seugue: partendo da un primo vertice se ne collega un secondo, necessariamente distinto; ma non volendo considerare ogni arco due volte, si divide per due, ottenendo
\[\frac{\left \vert v \right \vert \cdot \left(\left \vert v \right \vert - 1\right)}{2} \cong \left \vert v \right \vert^2\]
da cui si evvince che il numero degli archi, in un grafo, è compreso tra
\[0 \leq \left \vert \xi \right \vert \leq \frac{\left \vert v \right \vert \cdot \left(\left \vert v \right \vert - 1\right)}{2}\]
considerando $\left \vert v \right \vert$ la cardinalità, ossia il numero dei vertici considerati che costituiscono il grafo.\\
Il problema di \textbf{vertex cover} è un problema di ottimizzazione: ridurre il numero minimo di vertici tale per cui nel grafo ad ogni arco deve essere collegato almeno un vertice coperto. Per esempio, in un grafo dove ogni nodo è isolato, il numero minimo dei vertici da coprire è $0$, in quanto non ci sono archi. Nel caso di un \textbf{grafo completo}, come quello esposto di seguito

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$a$};
    \node[main node] (b) [below left of=a] {$b$};
    \node[main node] (c) [below right of=b] {$c$};
    \node[main node] (d) [below right of=a] {$d$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (b)
          edge [bend left, out=100, in=85, out looseness=2, in looseness=2] node[left] {} (c)
          % edge [loop above] node {0.1} (1)
      (b) edge node [right] {} (c)
          edge node {} (d)
          % edge [loop left] node {0.4} (2)
          % edge [bend right] node[left] {0.1} (3)
      (c) edge node [right] {} (d)
          % edge [bend right] node[right] {0.2} (4)
      (d) edge node [left] {} (a);
          % edge [loop right] node {0.6} (4)
          % edge [bend right] node[right] {0.2} (1);
  \end{tikzpicture}
  \caption{Esempio di grafo semplice completo}
  \label{fig:esempio_grafo_semplice_completo}
\end{figure}

\vspace{1em}
\noindent
la copertura minima non è, come contrariamente si potrebbe pensare all'inizio, pari a $\left \vert v \right \vert$, in quanto eliminando uno qualsiasi dei nodi, ancora ad ogni arco sarà collegato almeno un nodo comperto. Pertanto si ha che il numero $\#nodi$ dei nodi che si dovranno coprire al fine di risolvere il problema del vertex cover in un grafo avente $\left \vert v \right \vert$ vertici sarà sempre compreso tra
\[0 \leq \#nodi \leq \left \vert v \right \vert - 1\]
Per la risoluzione meccanica del \textbf{vertex cover} vi sono due algoritmi, di cui solo il secondo realmente efficace. Si consideri, a titolo esemplificativo, il grafo seguete

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$a$};
    \node[main node] (b) [right of=a] {$b$};
    \node[main node] (c) [below of=a] {$c$};
    \node[main node] (d) [below of=b] {$d$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (b)
          edge node[left] {} (c)
          % edge [loop above] node {0.1} (1)
      (b) edge node {} (d)
          % edge [loop left] node {0.4} (2)
          % edge [bend right] node[left] {0.1} (3)
          % edge [bend right] node[right] {0.2} (4)
      (d) edge node [left] {} (a);
          % edge [loop right] node {0.6} (4)
          % edge [bend right] node[right] {0.2} (1);
  \end{tikzpicture}
  \caption{Esempio di grafo semplice}
  \label{fig:esempio_grafo_semplice_1}
\end{figure}

\vspace{1em}
\noindent
in cui, naturalmente, per coprire tutti gli archi sarà sufficiente considerare il vertice $a$ e il vertice $d$ (oppure il nodo $b$), ottenendo, come possibile copertura
\[\boxed{a} \hspace{0.5em} \boxed{b} \hspace{0.5em} \boxed{c} \hspace{0.5em} \boxed{d}\]
\[\boxed{1} \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{1}\]
in cui con $\boxed{1}$ si rappresenta la copertura del rispettivo vertice. Ecco che questa è una $k$-upla binaria di \textbf{peso} $w=1+1=2$, in cui $k=4$: usando tale notazione, quindi, un procedimento meccanico, atto a verificare la corretta copertura prevede di considerare tutti gli archi e verificare per ciascuno di essi che almeno ad un vertice collegato dall'arco in questione corrisponda un $1$; se un arco è collegato a due vertici cui corrisponde $0$, la copertura è scorretta, ovviamente.\\
Tuttavia, la $k$-upla $1001$ è anche una codifica binaria su $4$ bit del numero $(9)_{10}$, che suggerisce la procedura di controllo seguente, definita a partire da $k$ numero di vertici

\begin{algorithm}[H]
  \caption{Vertex-cover}\label{euclid}
  \begin{algorithmic}[1]
    \State \textbf{for} $i = 0$ \textbf{to} $2^k$
    \Indent
      \State $i \to $ \text{ binario}
      \State \emph{check}$(i)$
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
ove \emph{check(i)} è una procedura che svolge il compito precedentemente esposto: presa in ingresso una $k$-upla binaria, cui si fa corrispondere la copertura di rispettivi vertici, verifica per ciascun arco che ad esso sia collegato un vertice effettivamente coperto, ovvero cui corrisponde un $1$ nella $k$-upla binaria considerata; alla fine della procedura si ottiene la copertura di peso minore, ovvero quella con meno $1$ e quindi che prevede meno vertici coperti.\\
Tale algoritmo non risulta propriamente efficiente; pertanto, al fine di eliminare alcune iterazioni, si potrebbe pensare di partire con il peso $w$ ed effettuare il medesimo \emph{check} per tutte $n$-uple di peso $w$, come mostrato di seguito:

\begin{algorithm}[H]
  \caption{Vertex-cover}
  \begin{algorithmic}[1]
    \State \textbf{for} $w = 0$ \textbf{to} $k-1$
    \Indent
      \State \textbf{for} \text{ all}
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
che è un procedimento più razionale, in quanto si controlla progressivamente se sono sufficienti un numero sempre maggiore di vertici da coprire (basta $1$ vertice? Bastano $2$ e così via...) e si esce dal ciclo quando si trova il primo, in quanto quella copertura sarà certamente la minima.\\
Da notare che si cicla fino a $k-1$ e non fino a $k$, in quanto è noto dalla teoria che il numero di vertici che si andranno a coprire per risolvere un qualsiasi problema di vertex-cover è sempre compreso tra $0$ e $k-1$, con $k$ numero di vertici: ma l'unica $n$-upla con peso massimo è quella con tutti $1$, la quale costituisce una sola configurazione tra le $2^k$ possibili. Pertanto, nel caso peggiore, si potrebbe procedere ad effettuare un numero di iterazioni pari a $2^k - 1$, che non è molto dissimile dal caso precedente, in cui si facevano inevitabilmente $2^k$ iterazioni.\\
Questo algoritmo, pertanto, pur essendo corretto, è ispirato al meccanismo dell'\textbf{exaustive search} (dall'inglese, ricerca esauriente), che prevede di controllare tutti gli archi al fine di verificarne la corretta copertura.\\
Il motivo per cui tale algoritmo è inutilizzabile è che presenta un numero di \textbf{iterazioni esponenziale} e, conseguentemente, una \textbf{complessità esponenziale}, la quale è intollerabile, dal momento che il numero delle iterazioni cresce esponenzialmente al variare dell'imput.\\
Di seguito si espone, invece, un nuovo algoritmo che risolve il problema del \textbf{vertex cover}; alla base di tale algoritmo si pone la seguente idea: si considerino dapprima due nodi connessi da un arco e si eliminino tutti gli archi che incidono sul primo e sul secondo vertice e si proceda a considerare un nuovo arco che insiste su due nodi ancora non considerati e si eliminino tutti gli altri archi che incidono sui nodi stessi e così via, fino ad esaurire tutti gli archi a disposizione, tale che alla fine della procedura si ottiene un ricoprimento vero e proprio.\\
Come di consueto, nello pseudocodice esposto di seguito, l'input non viene specificato a priori, ma il numero delle iterazioni per specificare l'input è noto, ossia è pari a $\cong \left \vert v \right \vert + \left \vert \xi \right \vert$.\\
Lo pseudocodice è il seguente:

\begin{algorithm}[H]
  \caption{Vertex-cover}
  \begin{algorithmic}[1]
    \State $C \gets \varnothing$
    \State $E' \gets \xi(\mathcal{G})$
    \State \textbf{while } $E' \neq \varnothing$
    \Indent
      \State \textbf{do } \text{... } $(u,v)$ \text{ in } $E$
      \Indent
        \State $C \gets C \cup \left\{u,b\right\}$
        \State \emph{delete } incidienti
      \EndIndent
    \EndIndent
    \State \textbf{return } $C$
  \end{algorithmic}
\end{algorithm}

\noindent
In cui $C$ è un contenitore, inizialmente vuoto, all'interno del quale successivamente andranno inseriti i vertici necessari per la vertex cover. Invece, $E'$ è un contenitore, inizialmente pieno, in quanto contiene tutti gli archi che dovranno essere esaminati/scartati. Dopodiché, fintantoché non ci sono più archi da analizzare, si considera un primo arco, designato con la notazione $(u,v)$, i cui due estremi, appunto $u$ e $v$ andranno ad essere inseriti all'interno di $C$, mentre verrano eliminati da $E'$ tutti gli archi incidienti in $u$ o in $v$, finché non si avranno più archi a disposizione.\\
Il numero di iterazioni in cui tale algoritmo si impegna è, approssimativamente, pari a $\left \vert v \right \vert + \left \vert \xi \right \vert$, ovvero si ha una \textbf{complessità lineare}: questa è un'ottima notizia, in quanto significa che tale algoritmo è velocissimo; l'unico problema è che esso è scorretto.\\
Si consideri, a titolo di esempio, il seguente grafo semplice:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$a$};
    \node[main node] (b) [right of=a] {$b$};
    \node[main node] (c) [below of=a] {$c$};
    \node[main node] (d) [below of=b] {$d$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (b)
          edge node [left] {} (c)
      (b) edge node [left] {} (d)
      (d) edge node [left] {} (a);
  \end{tikzpicture}
  \caption{Esempio di scorrettezza dell'agoritmmo considerato}
  \label{fig:esempio_scorrettezza_algoritmo}
\end{figure}

\noindent
In questo caso, l'algoritmo, considerando per primo l'arco $a-b$, elimina tutti gli altri archi e produce un risultato corretto.\\
Tuttavia, se il primo arco considerato dall'algoritmo fosse $a-c$, l'algoritmo è costretto a considerare anche l'arco $b-d$, non producendo un risultato corretto.\\
Tale algoritmo prende il nome di \textbf{algoritmo approssimato}, in quanto non sempre produce un risultato corretto, ma mai disastroso.\\
Tuttavia, non esiste un algoritmo che sia corretto e anche accettabile dal punto di vista del numero delle iterazioni e quindi della complessità: pertanto ci si deve accontentare dell'algoritmo approssimato appena esposto.\\
Per capire il range di risposta di tal algoritmo, si assuma che il numero di nodi ottimali necessari alla copertura sia $\left \vert \mathcal{O} \right \vert$ e il numero dei nodi effettivamente ottenuti dalla procedura algoritmica sia $\left \vert \mathcal{P} \right \vert$, legati dalla seguente relazione, dimostrata con l'esempio precedente:
\[\left \vert \mathcal{O} \right \vert \leq \left \vert \mathcal{P} \right \vert\]
Naturalmente, per quanto visto con l'esempio precedente, tale disuguaglianza può anche essere stretta. Ovviamente, se ora si considerano gli archi privilegiati dall'algoritmo esposto, denotati con $\left \vert \mathcal{A} \right \vert$, ovverosia l'insieme degli archi che l'algoritmo ha via via considerato, esso, naturalmente, presenta la seguente cardinalità
\[\left \vert \mathcal{A} \right \vert = \frac{\left \vert \mathcal{P} \right \vert}{2}\]
in quanto su ogni arco vi sono due nodi ($u$ e $v$ nello pseudocodice), per cui basta considerarne la metà. Ora, il numero di nodi ottimale $\left \vert \mathcal{O} \right \vert$ deve essere necessariamente almeno uguale al numero di archi privilegiati $\left \vert \mathcal{A} \right \vert$, dal momento che ciascuno di tali archi deve insistere almeno su un vertice coperto. Pertanto $\left \vert \mathcal{o} \right \vert$ e $\left \vert \mathcal{A} \right \vert$ sono legati dalla seguente relazione
\[\left \vert \mathcal{O} \right \vert \geq \left \vert \mathcal{A} \right \vert\]
per cui si ottiene che
\[\frac{\left \vert \mathcal{P} \right \vert}{2} \leq \left \vert \mathcal{O} \right \vert \leq \left \vert \mathcal{P} \right \vert\]

\newpage
\section{Algoritmi aritmetici}
L'\textbf{algoritmo di Euclide} permette di calcolare il \textbf{Massimo Comune Divisore} (\textbf{M.C.D.}) tra due numeri, ma non procedendo alla fattorizzazione in fattori primi tra le due quantità considerate.\\
Infatti, normalmente, per determinare l'M.C.D. tra due quantità è necessario procedere alla scoposizione delle due in fattori primi, come mostrato di seguito per i numeri $12$ e $9$:
\begin{flalign*}
  12 & = 3 \cdot 2 \cdot 2\\
  9 & = 3 \cdot 3\\
\end{flalign*}
da cui si evince che
\[\text{MCD}(12,9)=(12,9)=3\]
Tuttavia, non è possibile procedere attraverso la fattorizzazione per la risoluzione di tale problema, in quanto gli algoritmi per la fattorizzazione sono estremamente lenti. L'algoritmo di Euclide, invece, risolve tale problematica in maniera corretta, veloce ed efficiente, basandosi sul meccanismo della \textbf{ricorsività} e delle divisioni intere. Infatti, com'è noto, la divisione può essere di due tipologie
\begin{enumerate}
  \item Divisione esatta: $10 \div 3 = 3,\overline{3}$
  \item Divisione intera: $10 \div 3 = 3$ con resto $1$
\end{enumerate}

\newpage
\noindent
\begin{center}
  10 Marzo 2022
\end{center}
Naturalmente, un algortimo non può essere applicato concretamente se ha una crescita esponenziale: esso è inutilizzabile, in quanto il tempo di risposta è troppo elevato per avere un impiego pratico.\\
L'esposto seguente, tratto da un lavoro di Gary \& Johnson, ne dà una fondamentale prova pratica, considerando un calcolatore le cui istruzioni durano $0,000001$ s per essere processate; naturalmente l'algoritmo considera input variabili, di lunghezza $10$, $20$, $30$, $40$, $50$ e $60$ e si suppone, per semplicità, che la lunghezza dell'input determini in modo quanto più preciso e linearmente dipendente il numero delle operazioni che devono essere eseguite: pertanto, se l'input ha lunghezza $60$ significa che sono necessarie $60$ operazione per terminare l'algoritmo e quindi $0,00006$ s sarà il tempo impiegato per l'esecuzione.\\
Se, invece, la complessità dell'algoritmo è quadratica, allora ciò significa che se l'input è di lunghezza $60$, il numero di operazioni diviene $60 \cdot 60 = 3600$ e quindi il numero di secondi diviene $0,0036$ s.\\
Se la complessità è cubica, allora con lunghezza dell'input di $60$ il numero di operazioni diviene $60 \cdot 60 \cdot 60$ e quindi il tempo impiegato è di $0,216$ s.\\
Con una complessità quintica, a $60$ di lunghezza corrispondono $60^5$ istruzioni e quindi $13$ minuti di esecuzione.\\
Passando ad una complessità esponenziale, come $2^n$, con lunghezza dell'imput pari a $60$ si hanno $2^{60}$ istruzioni e quindi un tempo esecutivo di $360$ secoli. Passando appena a $3^n$, con lunghezza dell'imput pari a $60$ si hanno $3^{60}$ istruzioni e quindi un tempo esecutivo di $1,3 \cdot 10^{13}$ secoli.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che considerare un calcolatore $1000$ volte più veloce può essere significativo se la complessità dell'algoritmo è polinomiale, ma è totalmente ininfluente se la complessità è esponenziale.

\vspace{1em}
\subsection{Algoritmo di Euclide}
Si consideri il seguente algoritmo, noto come \textbf{algoritmo di euclide}, estremamente fulmineo:

\begin{algorithm}[H]
  \caption{Euclide}\label{euclide}
  \begin{algorithmic}[1]
    \State \textbf{begin}
    \State $a,b = m,n$
    \State \textbf{while } $b \neq 0$ \textbf{ do } $a,b=b,a \text{ mod } b$
    \State \text{mcd} $=a$
    \State \textbf{end}
  \end{algorithmic}
\end{algorithm}

\noindent
In questo caso l'algoritmo considera come input \textbf{due numeri interi} $m$ e $n$ di cui ha senso determinare l'\textbf{m.c.d}: pertanto essi devono essere necessariamente interi per ipotesi, in quanto nell'algoritmo non è previsto un controllo sintattico a monte. Generalmente si considerano $m > n$, ma ciò è ininfluente, in quanto il programma provvede ad effettuare un cambiamento del loro ordine in automatico.\\
Nell'algoritmo vi sono delle assegnazioni composte, in cui
\[a,b = m,n\]
ovvero ad $a$ si assegna il valore $m$, mentre a $b$ si assegna il valore $n$. Così come in seguito si ha
\[a,b=b,a \text{ mod } b\]
ovvero ad $a$ si assegna il vecchio valore $b$, mentre a $b$ si assegna il resto della divisione intera tra $a$ e $b$. Alla fine del ciclo si ottiene che l'\textbf{m.c.d.} cercato è proprio $a$.\\
Si consideri, a tal proposito, il seguente esempio:
\[\underset{a}{\boxed{12}} \hspace{0.5em} \underset{b}{\boxed{9}}\]
Dopo il primo passo si ottiene
\[\underset{a}{\boxed{9}} \hspace{0.5em} \underset{b}{\boxed{3}}\]
ed infine
\[\underset{a}{\boxed{3}} \hspace{0.5em} \underset{b}{\boxed{0}}\]
ecco che nella prima cella si ha proprio l'm.c.d. cercato. Ora, tuttavia, bisogna verificare se tale algoritmo sia effettivamente corretto e che quella considerata non sia solo una combinazione; inoltre, per determinare la complessità dell'algoritmo è necessario considerare, essenzialmente, il numero di \textbf{iterazioni libere} del ciclo while, in quanto la complessitì dell'algoritmo dipende unicamente da tale fattore.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi, innanzitutto, che il ciclo while si chiude visto che in posizione $b$ sarà presente un resto, ovvero una quantità intera che progressivamente diminuisce fino a diventare $0$.\\
L'ultimo passaggio, naturalmente, è il più semplice, in quanto l'algoritmo, prima dell'ultima iterazione, si ritroverà sempre nella situazione desiderata:
\[\boxed{\alpha \cdot \text{m.c.d.}} \hspace{0.5em} \boxed{\text{m.c.d.}}\]
per cui nell'ultima iterazione, effettuando la divisione intera tra una quantità e un suo multiplo non si può che ottenere resto $0$ e, quindi, nella cella $a$ si ha proprio l'm.c.d. che si sta cercando, come mostrato di seguito:
\[\boxed{\text{m.c.d.}} \hspace{0.5em} \boxed{0}\]
Per la dimostrazione della correttezza dell'algoritmo, anche nelle fasi intermedie, si deve dimostrare essenzialmente che, a qualunque fase del processo esecutivo
\[\text{M.C.D.}(a,b) = \text{M.C.D.}(b,a \text{ mod } b)\]
ovvero che quello che si trova nelle celle $a-b$ nell'istante $t$ e ciò che si trova in $a-b$ all'istante successivo hanno entrambi lo stesso massimo comune divisore: bisogna, di fatto, dimostrare quella che in matematica prende il nome di \textbf{invariante}; in questo caso l'invariante è proprio l'm.c.d. Infatti, le quantità che si trovano progressivamente nelle due celle hanno sempre lo stesso massimo comune divisore, a qualunque istante, fino ad arrivare alla configurazione finale in cui nella prima cella è presente un multiplo del valore della seconda cella.\\
Per dimostrare, quindi, l'invarianza seguente:
\[\text{M.C.D.}(a,b) = \text{M.C.D.}(b,a \text{ mod } b)\]
si deve dimostrare che un diviore di $(a,b)$ è anche divisore di $(b,a \text{ mod } b)$ e viceversa, per cui hanno lo stesso massimo comune divisore. Si supponga, allora, che $\alpha$ divida sia $a$ che $b$, per cui si ha che $a = \alpha a'$ e $b = \alpha b'$; inoltre, considerando $r = a \text{ mod } b$ quale il resto della divisione tra $a$ e $b$, si può scrivere, per il teorema del quoziente e resto
\[a = q b + r\]
Pertanto, sapendo che $\alpha$ divide sia $a$ che $b$, bisogna verificare che $\alpha$ divida anche $r = a \text{ mod } b$ per concludere la dimostrazione dell'invarianza. Tuttavia, il teorema del quoziente e resto permette di ricavare $r$ come segue
in cui, ovviamente, si ha che
\[r = a - qb = \alpha \cdot (a' - q b')\]
in cui è evidente come $\alpha$ sia, effettivamente, un divisore anche di $r = a \text{ mod } b$. Procedendo, ora, al contrario, supponendo che $b = \alpha b'$ e $r = \alpha r'$, sempre per il teorema del quoziente e resto si può scrivere $a = q b + r = \alpha \cdot (qb' + r')$, per cui, ancora una volta, $\alpha$ divide anche $a$, come volevasi dimostrare.

\vspace{1em}
\noindent
\textbf{Osservazione}: Avendo dimostrato la correttezza dell'algoritmo di Euclide, bisogna ora procedere alla verifica della sua straordinara rapidità. Il problema della complessità dell'algoritmo di Euclide, il quale era un alessandrino, venne risolto brillantemente da un matematico francese di nome \textbf{Lamé}, nel $1844$, il quale, a tutti gli effetti, è da reputarsi il padre della \textbf{teoria della complessità}, quando \textbf{Reynaud}, nel $1811$ lo aveva già preceduto, ma si parla sempre della prima metà dell'$800$.\\
Per comprendere il meccanismo alla base dello studio della complessità, si consideri un numero rappresentato in base $10$, quale il seguente $n = (37855)_{10}$, il quale viene progressivammente ridotto di \textbf{almeno} $10$ volte ad ogni iterazione, portandolo progressivamente a $3785$, $378$, $37$, $3$ ed infine $0$.\\
Pertanto, al più, il numero di iterazioni necessarie per annientare questo valore e portarlo a $0$ è pari alla \textbf{lunghezza decimale} del numero $n$ (in questo caso pari a $5$) che, con una buona dose di approssimazione può essere considerata
\[l_{10}(n) \cong \log_{10}(n)\]
Se, ora, il numero $n$ considerato viene scritto in binario e ad ogni iterazione viene annientato della metà, al più serviranno un numero di iterazioni pari alla lunghezza binaria del numero $n$ considerato per annientarlo, ovvero
\[\#\text{iterazioni} \leq l_2(n) \cong \log_2(n)\]
Con questa premessa, considerando l'algoritmo di Euclide e procedendo con i calcoli di \textbf{Lamé}, si prendano ad esempio tre iterazioni successive del ciclo while, che producono i seguenti tre stati corrispondenti a tre istanti consecutivi dell'esecuzione:
\[\boxed{a} \hspace{0.5em} \boxed{b}\]
\[\boxed{b} \hspace{0.5em} \boxed{c}\]
\[\boxed{c} \hspace{0.5em} \boxed{d}\]
in cui, ovviamente, si ha che $a \geq b \geq c \geq d$; inoltre, per come sono stati ottenuti $a,b,c$ e $d$ appare evidente che
\[a = bq + c \hspace{0.5em} \text{e} \hspace{0.5em} b = q c + d\]
A parte il caso iniziale in cui i due numeri $a \leq b$, cui l'algoritmo provvede cambiandoli d'ordine, in generale si ha sempre che $a > b$ e, quindi, il \textbf{quoziente} della divisione tra i due numeri è sempre \textbf{almeno $\boldsymbol{1}$}, quindi, siccome $q \geq 1$ deve essere che
\[b = q c + d \geq c + d\]
ma non solo: è anche noto che il valore della prima posizione è sempre maggiore o uguale del valore nella seconda posizione, ovvero $c \geq d$, per cui si ha che
\[b = q c + d \geq c + d \geq 2d\]
Questo significa che in ogni passo doppio, ovvero ogni due iterazioni, il contenuto della seconda cella di memoria è dimezzato, o peggio, in quanto si ha che $b \geq 2d$. Volendo far sì che l'elemento nella seconda cella divenga zero, poiché ad ogni doppia iterazione il suo valore viene almeno dimezzato, il numero di passi doppi necessari è al più uguale al logaritmo in base $2$ di $n$, quindi
\[\# \text{passi doppi} \cong \log_2(n) \longrightarrow \# \text{passi singoli} < 2 \log_2(n)\]
pertanto, il numero di passi di cui si necessità per terminare il ciclo while è estremamente piccolo, anche nel caso in cui il numero $n$ considerato è enormemente grande.

\vspace{1em}
\noindent
\textbf{Osservazione}: Un altro modo per descrivere l'algoritmo di Euclide è quello che riguarda la \textbf{ricorsività}, come mostrato di seguito

\begin{algorithm}[H]
  \caption{Euclide}\label{euclide}
  \begin{algorithmic}[1]
    \State \textbf{Procedura } Euclid(a,b)
    \State \textbf{if } $b=0$ \textbf{ then}
      \Indent
        \State \textbf{return } $a$
      \EndIndent
    \State \textbf{else}
    \Indent
      \State \textbf{return } Euclid($b,a \text{ mod } b$)
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
\textbf{Esempio}: Per esempio, si considerino le iterazioni seguenti
\[\text{Euclid}(30,21)\longrightarrow\text{Euclid}(21,9)\longrightarrow\text{Euclid}(9,3)\longrightarrow\text{Euclid}(3,0)=3 \]

\vspace{1em}
\subsection{Notazione $\boldsymbol{O}$ grande}
Si considerno due funzione $f(x)$ e $g(x)$ infinite a $+ \infty$, ovvero
\[\lim_{x \to +\infty} f(x) = +\infty \hspace{1em} \text{e} \hspace{1em} \lim_{x \to +\infty} g(x) = +\infty\]
Allora le due funzioni si rassomiglieranno per $x \to +\infty$ quando hanno lo stesso ordine di infinito, ovvero
\[\lim_{x \to +\infty} \frac{f(x)}{g(x)} = \alpha \in \mathbb{R}^+ - \{0\}\]
Tuttavia, taluna è una generalizzazione molto spartana, ma la notazione $O$ grande lo è ancora di più. Si considerino, nuovamente, due funzioni $f(x)$ e $g(x)$, le quali non debbono più essere infinite a $+\infty$, ma è sufficiente che esse siano \textbf{definitivamente positive}, ovvero
\[\exists x_n : \forall x > x_n, f(x) > 0 \wedge g(x) > 0\]
In questo caso, pertanto, affermare che $f(x)$ \quotes{assomiglia} a $g(x)$ significa affermare che $f(x)$ appartiene alla stessa \textbf{classe di equivalenza} di $g(x)$ (ovvero la classe delle funzioni che rassomigliano a $g(x)$), che si indica come segue
\[f(x) \in \Theta \left(g(x)\right)\]
che, generalmente, verrà denotato con
\[f(x) = \Theta \left(g(x)\right)\]
nonostante sia più propriamente corretto impiegare un segno di appartenenza in luogo di uno di uguaglianza.

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{APPARTENENZA ALLA CLASSE DI EQUIVALENZA DI UNA FUNZIONE}}\\
    \parbox{\linewidth}{Si considerino due funzioni $f(x)$ e $g(x)$ \textbf{definitivamente positive}; si dirà che $f(x)$ appartiene alla stessa classe di equivalenza di $g(x)$ e si scriverà
    \[f(x) = \Theta \left(g(x)\right)\]
    se
    \[\exists c_1 > 0, c_2 > 0, \overline{x} : c_1 \cdot g(x) \leq f(x) \leq c_2 \cdot g(x), \hspace{1em} \forall x \geq \overline{x}\]
    ovvero si riesce a descrivere ipoteticamente, con la funzione $g(x)$, una guaina all'interno della quale racchiudere la funzione $f(x)$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\noindent
\textbf{Osservazione}: Dalla definizione appena fornita, appare evidente come questa sia a tutti gli effetti una \textbf{relazione di equivalenza}, in quanto è soddisfatta la proprietà \textbf{riflessiva}
\[g(x) = \Theta \left(g(x)\right)\]
in quanto basta considerare $c_1 = c_2 = 1$; inoltre è soddisfatta anche la proprietà \textbf{simmetrica}:
\[f(x) = \Theta \left(g(x)\right) \longrightarrow g(x) = \Theta \left(f(x)\right)\]
giacché se si ha che
\[c_1 \cdot g(x) \leq f(x) \leq c_2 \cdot g(x)\]
allora si può scrivere
\[\frac{1}{c_2} \cdot f(x) \leq g(x) \leq \frac{1}{c_1} \cdot f(x)\]
e infine quella \textbf{transitiva}
\[f(x) = \Theta \left(g(x)\right), g(x) = \Theta \left(h(x)\right) \longrightarrow f(x) = \Theta \left(h(x)\right)\]
in quanto se
\[c_1 \cdot g(x) \leq f(x) \leq c_2 \cdot g(x) \hspace{0.5em} \text{e} \hspace{0.5em} c_3 \cdot h(x) \leq g(x) \leq c_4 \cdot h(x)\]
è facile concludere che
\[c_1 \cdot c_3 \cdot h(x) \leq c_1 \cdot g(x) \leq f(x) \hspace{0.5em} \text{e} \hspace{0.5em} f(x) \leq c_2 \cdot g(x) \leq c_2 \cdot c_4 \cdot h(x)\]
per cui si ottiene che
\[c_1 \cdot c_3 \cdot h(x) \leq f(x) \leq c_2 \cdot c_4 \cdot h(x)\]

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la classe di equivalenza
\[\Theta \left(1\right)\]
Allora ad essa vi apparterranno tutte le costanti positive, così come le funzioni oscillanti che assumono valori positivi, come $f(x) = 2 + \sin(x)$, in quanto
\[2 \cdot 1 \leq f(x) \leq 3 \cdot 1, \hspace{1em} \text{con } g(x) = 1\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che se due funzioni hanno lo stesso ordine di infinito, allora appartengono alla stessa classe di equivalenza. Tuttavia non è vero il contrario.\\
Si considerino, infatti, due funzioni $f(x)$ e $g(x)$ tali che
\[\lim_{x \to +\infty} \frac{f(x)}{g(x)} = \alpha \in \mathbb{R}^+ - \{0\}\]
quindi definitivamente, per $x \geq \overline{x}$, si ha che
\[\alpha - \epsilon \leq \frac{f(x)}{g(x)} \leq \alpha + \epsilon\]
dal momento che dalla definizione di limite sia ha $\forall \epsilon > 0$, è possibile anche considerare $\epsilon = \frac{\alpha}{2}$, da cui
\[\frac{\alpha}{2} \leq \frac{f(x)}{g(x)} \leq \frac{3}{2} \alpha\]
ma allora, moltiplicando ambo i membri per $g(x)$, essendo per definizione necessariamente definitivamente positiva, la disuguaglianza si conserva e diviene
\[\frac{\alpha}{2} \cdot g(x) \leq f(x) \leq \frac{3}{2} \cdot \alpha \cdot g(x)\]
che corrisponde esattamente alla definizione di due funzioni che appartengono alla stessa classe di equivalenza, con
\[c_1 = \frac{\alpha}{2} \hspace{0.5em} \text{e} \hspace{0.5em} c_2 = \frac{3}{2} \alpha\]
Per cui si è dimostrato che avere lo stesso ordine di infinito significa anche appartenere alla stessa classe di equivalenza, ma non è vero il contrario.

\newpage
\noindent
\begin{center}
  11 Marzo 2022
\end{center}
\subsection{Funzioni di complessità}
Si consideri una funzione di complessità temporale $f(n)$ definita in funzione $n$ della lunghezza dell'input: è molto articolato, nonché scarsamente utile, provvedere al calcolo di valori precisi di una funzione di complessità temporale; presumibilmente, però, al crescere di $n$, ossia al crescere della lunghezza dell'input, aumenta anche il valore della funzione stessa.\\
Le funzioni $f(n)$ di complessità temporale oggetto di studio, comunque, sono molto generali e, comunemente, sono \textbf{definitivamente positive} e, generalmente, sono infinite per $x \to +\infty$ e vengono studiate per $x \geq 0$.\\
Com'é noto, è stata definita la classe di complessità $\Theta(f(n))$ come l'insieme di tutte le funzioni che rassomigliano alla funzione $f(n)$. In questo caso, in particolare, affermare che
\[g(n) = \Theta(f(n))\]
significa affermare che $\exists c_1 > 0, c_2 > 0, \overline{n}$ tali che
\[c_1 \cdot f(n) \leq g(n) \leq c_2 \cdot f(n), \hspace{1em} \forall n \geq \overline{n}\]
in cui $\Theta(f(n))$ è, a tutti gli effetti, una \textbf{classe di equivalenza}, in quanto gode delle proprietà di \textbf{riflessività}, \textbf{simmetria} e \textbf{transitività}.\\
Se si considera una lunghezza $l=5$ in base $10$, una lunghezza effettiva senza zeri in testa, allora il numero $n$ che si può descrivere con lunghezza $l=5$ è compreso tra
\[10000 \leq n \leq 100000 - 1 \longrightarrow 10^{l-1} \leq n < 10^l\]
e passando ai logaritmi in base $10$, ciò si traduce in
\[l_{10}(n) - 1 \leq \log_{10}(n) < l_{10}(n)\]
in cui $\log_{10}(n)$ viene considerata una \textbf{lunghezza continua}, decisamente più pratica nel suo utilizzo rispetto alla \textbf{lunghezza intera} $l_{10}(n)$, la quale è la lunghezza effettiva, ma inutilmente precisa. Naturalmente si può scrivere che
\[\frac{1}{2} \cdot l_{10}(n) \leq l_{10}(n) - 1 \leq \log_{10}(n) \leq l_{10}(n)\]
per cui si può osservare come $l_{10}(n)$ e $\log_{10}(n)$ appartengono alla stessa classe di equivalenza $\Theta(l_{10}(n))$ in quanto sono state usate le costanti $c_1 = \frac{1}{2}$ e $c_2 = 1$. Dal punto di vista della notazione $\Theta$, quindi, tali lunghezze sono perfettamente equivalenti.\\ Non solo, ma se si considerano le quantità $\log_{10}(n)$ e $\log_{2}(n)$, è noto che
\[\log_{2}(n) = \frac{\log_{10}(n)}{\log_{10}(2)}\]
in cui
\[\frac{1}{\log_{10}(n)}\]
è una costante certamente positiva, allora si evince come tali quantità siano perfettamente equivalenti secondo la notazione $\Theta$.\\
Quando si impiega la notazione $\Theta$, infatti, una funzione $f(n)$ e $\alpha \cdot f(n), \alpha \geq 0$ appartengono alla stessa classe $\Theta$ ed è per questo che normalmente viene omessa la base dei logaritmi, che comunque deve essere maggiore di $1$, la quale, di fatto, è ininfluente.\\
Inoltre si è osservato come due funzioni che hanno lo stesso ordine di infinito appartegono anche alla stessa classe $\Theta$, semplicemente applicando la definizioen di limite. Pertanto, le $3$ funzioni
\[3n^2 + 1 \hspace{1em} \frac{n^2}{1000} \hspace{1em} 8945 n^2 - n\]
appartengono alla stessa classe $\Theta(n^2)$, in quanto presentano lo stesso ordine di infinito, ovvero il limite del loro rapporto è una costante positiva.\\
Si consideri una funzione $f(n)$ infinita per $x \to +\infty$ e la funzione $g(n) = f(n) \cdot \left[\sin(n) + 2\right]$ anch'essa infinita per $x \to +\infty$ che appartengono alla stessa classe $\Theta$; tuttavia, si ha che
\[\lim_{n \to +\infty} \frac{g(n)}{f(n)} = \lim_{n \to +\infty} \sin(n) + 2 = \nexists\]
per cui due funzioni che appartengono alla stessa classe di equivalenza $\Theta$ non è detto che abbiano lo stesso ordine di infinito.

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{FUNZIONE O-GRANDE DI UN'ALTRA}}\\
    \parbox{\linewidth}{Si considerino due funzioni $f(x)$ e $g(x)$ \textbf{definitivamente positive}; allora indicare
    \[f(n) = O(g(n))\]
    significa affermare che definitivamente si ha che
    \[f(n) \leq g(n)\]
    ovvero che
    \[\exists \overline{n}, c \geq 0 : f(n) \leq c \cdot g(n), \hspace{1em} \forall n \geq \overline{n}\]
    per cui se $f(n)$ e $g(n)$ appartengono alla stessa classe di equivalenza $\Theta$, allora $f(n)$ è O-grande di $g(n)$ e $g(n)$ è O-grande $f(n)$ e viceversa: se $f(n)$ è O-grande di $g(n)$ e $g(n)$ è O-grande $f(n)$ allora $f(n)$ e $g(n)$ appartengono alla stessa classe di equivalenza $\Theta$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\noindent
\textbf{Esempio}: Si osservi che, ovviamente
\[2n^2 = O(n^2) \hspace{1em} \text{e} \hspace{1em} \frac{n}{17} = O(n^2)\]
o ancora, che quando
\[\lim_{n \to +\infty} \frac{f(n)}{g(n)} = +\infty\]
significa affermare che $g(n) = O(f(n))$.

\vspace{1em}
\noindent
\textbf{Ossevazione}: Si osservi, per quanto si è detto, che
\[
  \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{l}
      f(n) = O(g(n))\\
      g(n) = O(f(n))
    \end{array}
  \right.
\]
\textbf{se e solo se} appartengono alla stessa classe $\Theta$. Analogamente si ha che
\[
  \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{l}
      f(n) = O(g(n))\\
      g(n) = O(h(n))
    \end{array}
  \right.
\]
allora si ha che $f(n) = O(h(n))$.

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{FUNZIONE $\boldsymbol{\Omega}$-GRANDE DI UN'ALTRA}}\\
    \parbox{\linewidth}{Si considerino due funzioni $f(x)$ e $g(x)$ \textbf{definitivamente positive}; allora indicare
    \[f(n) = \Omega(g(n))\]
    significa affermare che definitivamente si ha che
    \[f(n) \geq g(n)\]
    ovvero che
    \[\exists \overline{n}, c \geq 0 : f(n) \geq c \cdot g(n), \hspace{1em} \forall n \geq \overline{n}\]
    per cui se $f(n)$ e $g(n)$ appartengono alla stessa classe di equivalenza $\Theta$, allora $f(n)$ è $\Omega$-grande di $g(n)$ e $g(n)$ è $\Omega$-grande $f(n)$ e viceversa: se $f(n)$ è $\Omega$-grande di $g(n)$ e $g(n)$ è $\Omega$-grande $f(n)$ allora $f(n)$ e $g(n)$ appartengono alla stessa classe di equivalenza $\Theta$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\noindent
\textbf{Esempio}: Si osservi che indicare
\[f(n) = \Omega(g(n))\]
\textbf{se e solo se}
\[g(n) = O(f(n))\]
Analogamente si ha che
\[
  \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{l}
      f(n) = O(g(n))\\
      f(n) = \Omega(g(n))
    \end{array}
  \right.
\]
\textbf{se e solo se} appartengono alla stessa classe $\Theta$.

\vspace{1em}
\subsection{Classi di complessità}
Le tre classi $\Theta$ che si incontreranno maggiormente saranno
\begin{enumerate}
  \item La classe \textbf{lineare} $\Theta(n)$;
  \item La classe \textbf{log-lineare} $\Theta(n \cdot \log(n))$;
  \item La classe \textbf{quadratica} $\Theta(n^2)$;
\end{enumerate}
in cui si ha che
\[n = O (n \cdot \log(n)) \hspace{1em} \text{e} \hspace{1em} n \cdot \log(n) = O(n^2)\]
ovvero la complessità log-lineare è comunque poco più complessa rispetto a quella lineare, in quanto la crescita all'infinito è molto lenta.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che la funzione
\[10^{10^{80}}n = \Theta(n)\]
anche se il suo coefficiente è estremamente elevato. Analogamentte, se si ha una funzione
\[2^{0,00000...1} = \Theta(2^n)\]
nonostante la sua crescita è estremamente lenta rispetto ad una funzione esponenziale normale. Tuttavia, tale risultato non costituisce un'anomalia nella notazione $\Theta$, in quanto tali tipologie di funzioni sono impossibili tra trovare nella vita reale, secondo una valenza empirica.

\vspace{1em}
\noindent
Si consideri il listato seguente:
\begin{algorithm}[H]
  \caption{Esempio listato 1}
  \begin{algorithmic}[1]
    \State \textbf{for} $i=1$ \textbf{to} $k$
    \Indent
      \State \textbf{for} $j=1$ \textbf{to} $k$
      \Indent
        \State $a_{i,j} = 0$
      \EndIndent
      \State \textbf{next} $i$
    \EndIndent
    \State \textbf{next} $j$
  \end{algorithmic}
\end{algorithm}

\noindent
Natualmente il numero di iterazioni di tale algoritmo è $k \cdot k = k^2$, così come il numero delle assegnazioni. Ma se il listato fosse stato quello proposto di seguito:

\begin{algorithm}[H]
  \caption{Esempio listato 2}
  \begin{algorithmic}[1]
    \State $s=0$
    \State \textbf{for} $i=1$ \textbf{to} $k$
    \Indent
      \State \textbf{for} $j=1$ \textbf{to} $k$
      \Indent
        \State $s = s + a_{i,j}$
        \State $a_{i,j} = 0$
      \EndIndent
      \State \textbf{next} $i$
    \EndIndent
    \State \textbf{next} $j$
    \State $s = s*s$
  \end{algorithmic}
\end{algorithm}

\noindent
allora il numero della assegnazioni è significativamente aumentato, passando da $k^2$ a $2 + 2k^2$. Tuttavia, la notazione $\Theta$ fa sì che in ogni caso la complessità sia rimasta pari a $k^2$, in quanto il numero di iterazioni, e quindi l'ordine, è \textbf{inalterato}.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che nel caso dell'algoritmo \textbf{insertion-sort}, la complessità nel caso migliore è pari a $\Theta(n)$, in quanto il numero delle iterazioni è pari a $n-1$, mentre nel caso peggiore la complessità è $\Theta(n^2)$, in quanto le iterazioni sono $\frac{n \cdot (n-1)}{2}$. Per quanto concerne la complessità tipica, ovverosia la complessità media, dal punto di vista empirico si osserva che essa è dell'ordine $\Theta(n^2)$.\\
Per quanto concerne l'algoritmo di Euclide, il numero di iterazioni, nel caso peggiore era di
\[1 + 2 \cdot \log_2(\min(m,n))\]
per cui si ha che la complessità effettiva sia $O(\log(n))$, ovvero possibilmente inferiore ad una complessità logaritmica.

\vspace{1em}
\subsection{Merge-Sort}
L'algoritmo \textbf{merge-sort} è un algoritmo la cui idea alla base è molto più complessa dell'algoritmo \textbf{insertion-sort}.\\
L'input, come di consueto, è dato da un \textbf{array} $A$ di lunghezza $length(A)=n$, per cui si dovranno ordinare $n$ numeri. Inoltre, due altri input sono $p$ e $r$, i quali costituiscono delle posizioni dell'array (anche intermedie o coincidenti), tali che $1 \leq p \leq r \leq n$.\\
Di seguito si espone il listato:

\begin{algorithm}[H]
  \caption{Merge-sort}
  \begin{algorithmic}[1]
    \State \text{MERGE-SORT}$(A,p,r)$
    \State \textbf{if} $p<r$
    \Indent
      \State \textbf{then} $q = \left \lfloor {\dfrac{p+r}{2}} \right \rfloor$
      \Indent
        \State \text{MERGE-SORT}$(A,p,q)$
        \State \text{MERGE-SORT}$(A,q+1,r)$
        \State \text{MERGE}$(A,p,q,r)$
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
Dove
\[\left \lfloor {\dfrac{p+r}{2}} \right \rfloor\]
prende il nome di \emph{parte intera inferiore} della semisomma di $p$ e $r$, ovvero viene eliminata la parte dopo la virgola.\\
Inizialmente l'algoritmo opera partendo ponendo $p=1$ e $r=n$, ma successivamente le chiamate ricorsive fanno sì che $p$ e $r$ divengano delle posizioni intermedie, designate con $q$.\\
La procedura MERGE non è ricorsiva, ma presenta due cicli for in grado di ordinare due blocchi di numeri già ordinati. Tale procedura, infatti, inzia ad operare sue due insiemi di valori che sono già stati ordinati separatamente, con lo scopo di ricavare un unico insieme di valori ordinati, sfruttando l'ordinamento dell'insieme dei valori su cui si sta operando; per farlo si confrontano tutti i valori in testa agli insiemi ordinati, prendendo sempre il più piccolo tra i due che vengono confrontati, fino ad arrivare all'ultimo valore (necessariamente molto elevato, appositamente inserito come \quotes{fine-corsa}) che viene appositamente posto alla fine in modo da segnalare il termine dell'insieme di valori; in particolare si ha che $p < q < r$, in cui i valori che stanno tra $[p-q]$ e $[q-r]$ sono già stati ordinati.\\
La procedura viene esposta di seguito:

\begin{algorithm}[H]
  \caption{Merge}
  \begin{algorithmic}[1]
    \State \text{MERGE}$(A,p,q,r)$
    \State $n_1=q-p+1$
    \State $n_2=r-q$
    \State Create two new arrays $L$ and $R$
    \State \textbf{for} $i=1$ \textbf{to} $n_1$
    \Indent
      \State \textbf{do} $L[i]=A[p+i-1]$
      \Indent
        \State $R[j]=A[q+j]$
      \EndIndent
    \EndIndent
    \State ...
    \State $L[n_1+1]=+\infty, R[n_2+1]=+\infty$
    \State $i=1, j=1$
    \State \textbf{for} $k=p$ \textbf{to} $r$
    \Indent
      \State \textbf{do} \textbf{if} $L[i] \leq R[j]$
      \Indent
        \State \textbf{then} $A[k] = L[i]$
        \Indent
          \State $i = i + 1$
        \EndIndent
        \State \textbf{else} $A[k] = R[j]$
        \Indent
          \State $j = j + 1$
        \EndIndent
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
In cui le iterazioni coinvolte, naturalmente, fino alla riga $10$ sono
\[n_1 + n_2 = q - p + 1 + r - q = r - p + 1\]
ovverosia il numero delle posizioni che vanno da $p$ a $r$.\\
Mentre dalla riga $10$ in poi il numero di iterazioni è, ovviamente, ancora una volta $r - p + 1$; pertanto il numero di iterazioni complessive è pari al doppio di $r - p + 1$, quindi la complessità è dell'ordine
\[\Theta(r-p+1)\]

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri il seguente array
\[\boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{7} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{9}\]
in cui $n=8$. Per l'ordinamento di tale array, si divide lo stesso in due parti e poi successivamente ogni parte in ancora due parti e nuovamente in due parti fino ad ottenere una divisione dell'ordinamento riducendosi all'unità, in cui i singoli valori saranno automaticamente ordinati, come mostrato di seguito:
\[\boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{1} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{7} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{9}\]
e poi
\[\boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{1} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{7} \hspace{0.5em} \boxed{2} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{9}\]
ed infine
\[\boxed{4} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{5} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{3} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{1} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{7} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{2} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{0} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{9}\]
Dal momento che i blocchi da $1$ sono già ordinati si procede a ad utilizzare la procedura \emph{Merge} per ordinare i blocchi da $2$, ottenenendo
\[\boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{3} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{7} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{9}\]
e poi procedendo con i blocchi da $4$, applicando la procedura \emph{Merge} si ottiene
\[\boxed{1} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{7} \hspace{0.5em} \boxed{9}\]
e infine si ordina il blocco completo, applicando la procedura \emph{Merge} si ottiene
\[\boxed{0} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{7} \hspace{0.5em} \boxed{9}\]
Questa, naturalmente, è una \textbf{esecuzione parallela}, mentre quella dell'algoritmo è \textbf{lineare}, in quanto va ad analizzare progressiamente ogni ramo di un albero binario, risalendolo progresivamente.

\vspace{1em}
\noindent
\textbf{Osservazione}: Tale algoritmo è \textbf{profondamente rigido}, in quanto il numero delle iterazioni che si devono eseguire è sempre lo stesso, indipendente dal caso migliore, peggiore e generale.\\
Questa non è una buona caratteristica, la quale rende ragionevole l'utilizzo di tale algoritmo quando i numeri sono inzialmente fortemente disordinati; se i numeri iniziali sono parzialmente ordinati è più conveniente impiegare l'algoritmo di insertion-sort.\\
La complessità di tale algoritmo, tuttavia, nel caso peggiore è \textbf{log-lineare} ed è estremamente ottima in quanto un algortimo di ordinamento generale, come il merg-sort (che non richiede una specifica struttura dei dati in ingresso) non è in grado di battere una complessità log-lineare.\\
Il principio di costruzione di tale algoritmo, nonché la tecnica impiegata per la risoluzione di tale problema, prende il nome di \emph{Divide et impera} (o \emph{Divide and conquer}).

\newpage
\noindent
\begin{center}
  16 Marzo 2022
\end{center}
Una componente fondamentale dello studio algoritmico è la \textbf{relazione di ricorrenza}, alla base degli algoritmi che si fondano sulla \textbf{ricorsività}.\\
È noto che la sezione aurea si basa sulla suddivisione di un segmento
\[a + b : a = b : b\]
e ponendo $b=1$ si ottiene che
\[a + 1 : a = 1 : b\]
che dà vita ad una equazione di secondo grado
\[a^2 - a - 1 = 0\]
che produce due soluzioni
\[a_{1,2} = \frac{1 \pm \sqrt{5}}{2}\]
in cui la soluzione più importante è quella naturalmente positiva, ovvero:
\[\psi = \frac{1 + \sqrt{5}}{2} \cong 1.62\]

\vspace{1em}
\noindent
\textbf{Esempio}: Alla base dello studio della ricorsività si pone il problema della crescita demografica dei conigli, partendo dalla condizione iniziale che
\[F_1 = F_2 = 1\]
sapendo che la relazione che lega le due quantità sia
\[F_n = F_{n-2} + F_{n-1}\]
Il problema di determinare una formula chiusa trova soluzione nella seguente formula
\[F_n = \frac{\psi^n - \gamma^n}{\sqrt{5}} \cong \frac{\psi^n}{\sqrt{5}}\]
riuscendo ad ottenere il seguente risultato
\[y = \frac{F_{n+1}}{F_n}\]

\vspace{1em}
\subsection{Master Theorem}
Il \textbf{teorema maestro}, o il \textbf{teorema principale delle relazioni di ricorrenza}, si può applicare solamente in casi ben determinati. Tuttavia, tale teorema inizia ad operare partendo dalla condizione iniziale
\[T(1) = \Theta(1)\]
ovvero $T(1)$ appartiene alla classe di equivalenza $\Theta(1)$, la quale è una definizione decisamente vaga, per cui non vale la pena di scriverla. La relazione, invece, alla base del teorema è il seguente
\[\boxed{T(n) = a_3 \cdot T \left(\frac{n}{\lfloor b \rfloor}\right) + a_2 \cdot T \left(\frac{n}{\lceil b \rceil}\right) + f(n)}\]
Tuttavia, la perte intera inferiore o superiore è ininfluente, per cui si preferisce la formula abbreviata
\[\boxed{T(n) = a \cdot T \left(\frac{n}{b}\right) + f(n)}\]
imponendo che $a \geq 1$ e $b > 1$ (altrimenti non è possibile applicare il teorema considerato).\\
L'applicazione del teorema maestro prevede $3$ differenti casi, a seconda della natura della funzione $f(n)$. Per confrontare tale funzione se ne definisca una secodnda, chiamata \textbf{funzione di confronto}:
\[\psi(n) = n^{\log_b(a)}\]
che assicura che la funzione sia crescente grazie alla base $b>1$ del logaritmo. Pertanto si ha che
\begin{itemize}
  \item La funzione $f(n)$ è \textbf{molto} più piccola della funzione $\psi(n)$
  \item La funzione $f(n)$ rassomiglia alla funzione $\psi(n)$, ovvero
  \[f(n) = \Theta(\psi(n))\]
  \item La funzione $f(n)$ è \textbf{molto} più grande della funzione $\psi(n)$
\end{itemize}
Il caso che viene trattato è solo il secondo, a cui si può applicare il teorema solamente se viene soddisfatta una specifica condizione, la quale non è stobile rispetto alla classe di equivalenza di appartenenza della funzione $f(n)$: è possibile che due funzioni appartenenti alla stessa classe di equivalenza possano l'una soddisfare la condizione, la seconda no.\\
... continua ...

\vspace{1em}
\noindent
\textbf{Osservazione}: Nell'algoritmo \textbf{merge-sort} la procedura \textbf{merge} permette di ordinare dei valori partendo da due insiemi di valori già ordinati, ottenendo un ultimo insieme perfettamente ordinato. La procedura \emph{merge} non è una procedura ricorsiva, mentre l'algoritmo \textit{merge-sort} si basa sul meccanismo della ricorsività, andando a scorrere un albero binario da sinistra verso destra, risalendolo progressivammente.\\
L'algoritmo è assolutamente corretto, dal punto di vista esecutivo, è ciò è evidente, anche solo dal punto di vista empirico; tuttavia, ciò che non appare evidente è la complessità dell'algoritmo considerato.\\
Si ripropone di seguto il listato dell'algoritmo \textbf{merge-sort}:

\begin{algorithm}[H]
  \caption{Merge-sort}
  \begin{algorithmic}[1]
    \State \text{MERGE-SORT}$(A,p,r)$
    \State \textbf{if} $p<r$
    \Indent
      \State \textbf{then} $q = \left \lfloor {\dfrac{p+r}{2}} \right \rfloor$
      \Indent
        \State \text{MERGE-SORT}$(A,p,q)$
        \State \text{MERGE-SORT}$(A,q+1,r)$
        \State \text{MERGE}$(A,p,q,r)$
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
Lo sforzo per ordinare $n$ numeri, naturalmente, è dato dalla seguente espressione
\[T(n) = T \left(\frac{n}{2}\right) + T \left(\frac{n}{2}\right) + f(n)\]
ovvero la somma dello sforzo per ordinare ciascuna metà più l'operazione di \emph{merge} $f(n)$, che, in forma più compatta, può essere scritta come segue
\[T(n) = 2 \cdot T \left(\frac{n}{2}\right) + f(n)\]
ed applicando tale formula a qualunque passo si ottiene
\[T\left(\frac{n}{2}\right) = 2 \cdot T \left(\frac{n}{4}\right) + f(n)\]
\[T\left(\frac{n}{4}\right) = 2 \cdot T \left(\frac{n}{8}\right) + f(n)\]
e via dicendo. Inoltre, per quanto concerne la procedua merge, partendo da un array iniziale $A$, se ne creavano ulteriori due $R$, di dimensione $n_1$ e $L$, di dimensione $n_2$, e per farlo si necessitava di $n = n_1 + n_2$ iterazioni. Per l'ordinamento, ancora una volta, si necessitava di $n = n_1 + n_2$ iterazioni, con un totale di $2n$ iterazioni. Pertanto la complessità della procedura \emph{merge} è dell'ordine $\Theta(n)$.\\
Pertanto, il problema da risolvere è il seguente
\[T(n) = 2 \cdot T \left(\frac{n}{2}\right) + \Theta(n)\]
Considerando la funzione di confronto
\[\psi = n^{\log_b(a)}\]
ma essendo $a=b=2$ si ottiene che
\[\psi = n^{\log_b(a)} = n\]
E per determinare la complessità si applica il \textbf{toerema maestro}, da cui si evince che la \textbf{complessità di merge-sort} è
\[T(n) = \Theta(n \cdot \log(n))\]
ovvero è log-lineare, la quale è una complessità generale e profondamente rigida, in quanto costante sia nel caso migliore che peggiore che medio.\\
Una complessità log-lineare è leggermente peggiore rispetto ad una complessità lineare, ma è comunque una complessità ottima: infatti, un algoritmo di ordinamento generale, è destinato ad avere una complessità migliore e peggiore almeno \textbf{log-lineare}. Inoltre, si ha che tale algoritmo risulta essere particolarmente utile nel suo utilizzo quando i valori da ordinare non sono già ordinati, ma totalmente in disordine; nel caso fossero parzialmente ordinati, sarebbe più conveniente impiegare insertion-sort.

\vspace{1em}
\subsection{Heap-sort}
La struttura di grafo più comunque è quella di \textbf{albero}; in un grafo semplice è sempre possibile costruiire un percorso chiuso, in cui partendo da un primo vertice e senza archi multipli si ritorna al vertice di partenza.

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{ALBERO}}\\
    \parbox{\linewidth}{Un grafo \textbf{senza percorsi chiusi} prende il nome di \textbf{albero}. \vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\noindent
Tuttavia, tale definizione è eccessivamente generica, in quanto molto spesso si preferisce operare con \textbf{alberi radicati} (rooted-tree), in cui è opportuno specificare la \textbf{radice}, ovvero un vertice specifico.\\
Un esempio di \textbf{albero radicato} che verrà considerato sarà raffigurato come segue:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$a$};
    \node[main node] (b) [below left of=a] {$b$};
    \node[main node] (c) [below right of=a] {$c$};
    \node[main node] (d) [below right of=c] {$d$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (b)
          edge node [left] {} (c)
      (c) edge node [right] {} (d);
  \end{tikzpicture}
  \caption{Esempio di albero radicato di profondità $2$}
  \label{fig:esempio_albero_radicato_profondita_due}
\end{figure}

\vspace{1em}
\noindent
Dove per \textbf{profondità} è da intendersi il livello più basso raggiunto dai vertici dell'albero. I vertici che non presentano figli prendono, invece, il nome di \textbf{foglie}.\\
L'albero che maggiormente verrà consideranto per la progettazione algoritmica è un \textbf{albero binario}, in cui i figli di ogni vertice sono \textbf{al più $\boldsymbol{2}$}. Un albero binario ben strutturato prevede che vi siano $n$ vertici e che l'albero sia completo, ovvero vengono saturati tutti i livelli di diversa profondità dell'albero prima di aggiungere al livello successivo, come mostrato di seguito:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$a$};
    \node[main node] (b) [below left of=a] {$b$};
    \node[main node] (c) [below right of=a] {$c$};
    \node[main node] (d) [below right of=c] {$d$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (b)
          edge node [left] {} (c)
      (c) edge node [right] {} (d);
  \end{tikzpicture}
  \caption{Esempio di albero binario completo}
  \label{fig:esempio_albero_binario_completo}
\end{figure}

\vspace{1em}
\noindent
In cui la regola di completamento prevede di procedere da sinistra verso destra. In un albero binario completo è facile capire la profondità dello stesso, semplicemente osservando che
\[2^k \leq n < 2^{k+1}\]
in cui $k$ è la profondità dell'albero, mentre $n$ è il numero di vertici dell'albero. Pertanto si ottiene che
\[k \leq \log_2(n) < k+1\]
ovvero ciò significa che $k$ e $\log_2(n)$ appartengono alla stessa classe $\Theta$.\\
Dire che un albero è \textbf{super completo} significa che $n$ deve essere una potenza di $2$ meno uno, ovvero $n=2^k-1$. Inoltre è facile osservare come il numero di nodi in un albero \textbf{super completo} sia
\[2^0 + 2^1 + 2^2 + 2^3 + ... + 2^k = n\]
in cui $k$ è la profondità dell'albero.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che la progessione geometrica
\[1 + q + q^2 + q^3 + ... + q^k = \sum_{i=0}^k q^i = \frac{q^{k+1} - 1}{q - 1}\]
In cui, nel caso in cui la ragione $q=2$, si ottiene che
\[\sum_{i=0}^k 2^i = 2{k+1} - 1\]
In cui è facile osservare che in un albero binario completo, nel livello delle foglie vi sarà sempre un numero di vertici superiore a tutto il resto dell'albero, ovvero
\[\# \text{ numero vertici} \cong 2 \cdot \# \text{ numero foglie}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Il numero di archi in un albero di $k$ vertici è esattamente pari a $k-1$, per cui dal punto di vista della notazione $\Theta$ il numero di vertici e il numero degli archi sono confondibili.\\
L'algoritmo \textbf{heap-sort} è un algoritmo di ordinamento per cataste (ove per catasta è da intendersi un albero binario completo, non necessariamente super-completo).\\
Per designare i vertici, all'interno della logica dell'\textbf{heap-sort}, si procede assegnando un valore numerico a ciascun vertice, facendo attenzione a far sì che il numero di ogni padre sia maggiore o uguale di ogni suo figlio (di tutti quelli nei livelli sottostanti), ottenendo una \textbf{catasta} (heap), come mostrato di seguito:








\end{document}
