\documentclass[a4paper]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\selectlanguage{italian}
\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage{circuitikz}
\usetikzlibrary{positioning, circuits.logic.US}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary {shapes.gates.logic.US, shapes.gates.logic.IEC, calc}
\tikzset {branch/.style={fill, shape = circle, minimum size = 3pt, inner sep = 0pt}}
\usetikzlibrary{matrix,calc}
\usepackage{multirow}
\usepackage{float}
\usepackage{geometry}
\usepackage{tabularx}
\usepackage{pgf-pie}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color, soul}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\graphicspath{ {./img/} }
\newtheorem{theorem}{Teorema}[section]
\newtheorem{corollary}{Corollario}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

% Specifiche
\geometry{
 a4paper,
 top=20mm,
 left=30mm,
 right=30mm,
 bottom=30mm
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO]{\nouppercase{\leftmark}}
\fancyfoot[CE, CO]{\thepage}
\addtolength{\headheight}{1em}
\addtolength{\footskip}{-0.5em}

\newcommand{\quotes}[1]{``#1''}
\renewcommand\tabularxcolumn[1]{>{\vspace{\fill}}m{#1}<{\vspace{\fill}}}
\renewcommand\arraystretch{}
\newcolumntype{P}{>{\centering\arraybackslash}X}

\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
\algtext*{Indent}
\algtext*{EndIndent}

\title{\textbf{Università di Trieste\\ \vspace{1em}
Laurea in ingegneria elettronica e informatica}}
\author{Enrico Piccin - Corso di Algoritmi e Strutture dati - Prof. Andrea Sgarro}
\date{Anno Accademico 2021/2022 - 2 Marzo 2022}

\begin{document}

\vspace{-10mm}
\maketitle

\tableofcontents
\newpage

\noindent
\begin{center}
  2 Marzo 2022
\end{center}

\section{Introduzione}
\textbf{Algoritmo} è una parola molto antica, non connessa all'utilizzo e all'invenzione del calcolatore.\\
Alla base della teoria della computazione si pone il \textbf{Liber abaci} (tradotto \quotes{Libro della computazione}), scritto nel $1200$ da Leonardo Bonacci, in contatto con la popolazione araba, in quanto mercante; egli è venuto a conoscenza della \textbf{numerazione araba}, introducendola in Occidente e spiegandola dettagliatamente all'interno del \textbf{Liber abaci}.\\
La numerazione araba è uno straordinario passo in avanti nella scienza, in quanto con essa viene introdotto il concetto di \textbf{notazione posizionale}, così come l'importanza del numero $0$: i numeri non servono solamente per contare, come si pensava in precedenza, e per questo rinnegando il numero $0$.\\
A Firenze, sempre negli stessi anni, ci fu una \textbf{protesta sindacale} contro l'innovazione tecnologica, contro questa nuova scoperta, facendo pressione affinché il governo abolisse il nuovo sistema di numerazione, in quanto avrebbe fatto perdere il posto di lavoro a tutti coloro che prima eseguivano difficili calcoli con la numerazione romana: tuttavia, tale proteste, com'é noto, possono rallentare il progresso, ma mai arrestarlo.\\
Leonardo Bonacci, nei suoi viaggi in Oriente, venne a conoscenza del \textbf{Liber abaci} di Al-Gorasmy, proveniente dalla Coresmia, ma che parlava persiano, da cui poi sarebbe stato tratto il nome \textbf{Algoritmo}, che letteralmente significa \textbf{procedimento di calcolo}.\\

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{ALGORITMO}}\\
    \parbox{\linewidth}{Algoritmo significa letteralmente \textbf{procedimento di calcolo}. Tuttavia, bisogna chiarire che un algoritmo è un procedimento di calcolo non necessariamente numerico, ma molto più generale, che va ben al di là dei numeri.\\
    Un altro importante elemento che contraddistingue l'algoritmo è la \textbf{meccanicità}, ovvero la sua esecuzione può essere affidata ad una macchina: ciò significa che un algoritmo non deve necessariamente essere meccanizzato, ma deve essere \textbf{meccanizzabile}; in altre parole, l'esecuzione (bada bene, l'esecuzione e non la sua ideazione) dell'algoritmo è completamente \textbf{stupida}.
    \vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\vspace{1em}
\noindent
\textbf{Esempio 1}: L'algoritmo della moltiplicazione è molto chiaro e semplice: basta solamente accedere ad una \textbf{base di dati} in cui sono memorizzati i prodotti elementari fra numeri molto piccoli, e quindi molto più semplici da trattare.\\
Una volta eseguite le operazioni di moltiplicazione tramite quanto esposto in precedenza, è necessario eseguire delle operazioni di addizione, che era necessario aver precedentemente memorizzato.\\
Ecco che quello che si è appena eseguito è un \textbf{procedimento di calcolo}.

\vspace{1em}
\noindent
\textbf{Esempio 2}: L'algoritmo della divisione fa sempre uso di una base di dati, nella quale devono essere memorizzati i risultati del prodotto del divisore con tutti i numeri decimali da $0$ a $9$ e confrontare ciascun prodotto con il termine da dividere per ottenere il quoziente.\\
Alternativamente, si sarebbe potuto creare un ciclo da $0$ a $9$ in cui per ogni indice si sarebbe dovuto verificare se questo fosse il fattore moltiplicativo corretto per ottenere la quantità giusta da sottrarre.

\vspace{1em}
\noindent
\textbf{Osservazione}: In ciascuno di tali esempi è essenziale la meccanicità del processo esecutivo, che appare evidente.

\vspace{1em}
\noindent
Quando si rappresentano delle quantità e, a maggior ragione, quando si effettuano dei calcoli, è fondamentale fissare una base di rappresentazione, da cui poi dipendono le cifre che si possono impiegare per la rappresentazione stessa.\\
La notazione posizionale permette anche di comprendere la rappresentazione di qualsiasi quantità con qualsiasi base, effettuando anche delle conversioni di base a seconda della maggiore o minore convenienza di rappresentazione.\\
Per esempio, volendo convertire una quantità rappresentata in base $\mathcal{B} = 7$ in una base $\mathcal{C} = 10$ si deve procedere come segue
\[\left(5203\right)_7 = 5 \cdot 7^3 + 2 \cdot 7^2 + 0 \cdot 7^1 + 3 \cdot 7^0 = 1715 + 98 + 0 + 3 = \left(1816\right)_{10}\]
Ovviamente le basi di rappresentazione sono almeno binarie, in quanto la \textbf{base unaria} non può, per ovvie ragioni, rappresentare alcuna quantità se non quella unica che viene permessa dalla base scelta, ossia lo $0$.\\
Tuttavia, il processo inverso, atto a passare dalla rappresentazione di una quantità in base $10$ ad una in base $3$, non risulta essere così immediato.\\
Per cercare un algoritmmo che permette di effettuare tale conversione, si effettua un primo \textbf{passaggio controintuitivo} (che suggerisce, tuttavia, il corretto processo esecutivo), che prevede di rappresentare una quantità in base $10$ in una quantità ancora in base $10$, tramite un processo di divisioni successive. Si consideri, a tal proposito
\[\left(3412\right)_{10}\]
e si divida progessivamente tale numero per $10$, come segue

\begin{table}[H]
  \centering
  \rowcolors{1}{white}{white}
  \begin{tabular}{r|l}
    $3412$ & $10$\\
    \hline
    $341$ & $2$\\
    $34$  & $1$\\
    $3$   & $4$\\
    $0$   & $3$
  \end{tabular}
\end{table}

\vspace{1em}
\noindent
Leggendo, ora, i resti, al contrario si ottiene il numero cercato all'inizio. Se ora si prova a considerare un'altra base, come $3$, l'operazione porta ad un risultato analogo

\begin{table}[H]
  \centering
  \rowcolors{1}{white}{white}
  \begin{tabular}{r|l}
    $3412$ & $3$\\
    \hline
    $1137$ & $1$\\
    $379$  & $0$\\
    $126$  & $1$\\
    $42$   & $0$\\
    $14$   & $0$\\
    $4$    & $2$\\
    $1$    & $1$\\
    $0$    & $1$\\
  \end{tabular}
\end{table}

\vspace{1em}
\noindent
Per cui si è ottenuto
\[\left(3412\right)_{10} = \left(11200101\right)_3\]
Scegliendo la base $2$ si ottiene, per esempio

\begin{table}[H]
  \centering
  \rowcolors{1}{white}{white}
  \begin{tabular}{r|l}
    $241$ & $2$\\
    \hline
    $120$ & $1$\\
    $60$  & $0$\\
    $30$  & $0$\\
    $15$  & $0$\\
    $7$   & $1$\\
    $3$   & $1$\\
    $1$   & $1$\\
    $0$   & $1$\\
  \end{tabular}
\end{table}

\vspace{1em}
\noindent
Per cui si è ottenuto
\[\left(241\right)_{10} = \left(11110001\right)_2\]
Ovviamente la lunghezza di rappresentazione in base $2$ prevede un numero di cifre pari a circa il triplo di quelle impiegate per rappresentare la medesima quantità in base $10$, proprio perché
\[\log_2(10) \cong 3.3\]
Per passare da base $10$ a base $100$, le operazioni sono molto semplici
\[\left(375712\right)_{10} = \left[\left(37\right) \left(57\right) \left(12\right)\right]_{100}\]
usando come simboli
\[\left(00\right), \left(01\right), ..., \left(75\right), ..., \left(99\right)\]
Si consideri, ora la base $8$ e si scriva un numero binario in base ottale:
\[\left(010101010\right)_2 = \left[\left(010\right) \left(101\right) \left(010\right) \right]_8 = \left(252\right)_8\]
Ancora una volta, le cifre impiegate per la rappresentazione sono state ridotte ad un terzo, sempre perché
\[\log_2(8) = 3\]
E se ora si volesse impiegare la base $16$ si otterrebbe:
\[\left(010101010\right)_2 = \left[\left(1010\right) \left(1010\right) \right]_{16} = \left(\text{AA}\right)_{16}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si consideri una lunghezza $l = 5$. Allora usando $5$ cifre, non tutte nulle, in base $10$, i numeri $n$ che si possono rappresentare sono
\[10000 \leq n \leq 99999 \hspace{1em} \equiv \hspace{1em} 10^4 \leq n < 10^5 \hspace{1em} \equiv \hspace{1em} 10^{l - 1} \leq n < 10^l\]
Da ciò si può estrapolare un risultato importante
\[\log_{10}\left(10^{l - 1}\right) \leq \log_{10}\left(n\right) < \log_{10}\left(10^l\right) \hspace{1em} \equiv \hspace{1em} l - 1 \leq \log_{10}(n) < l\]
che è una relazione esatta. Tuttavia, approssimativamente, si può scrivere che
\[l_{10}(n) \cong \log_{10}(n)\]

\newpage
\noindent
\begin{center}
  3 Marzo 2022
\end{center}
Com'è noto, il matematico indiano \textbf{Ramanujan} ha affermato che la matematica esatta non rappresenta una base solida per la realtà, mentre la matematica vera è fatta di approssimazioni.\\
\textbf{Hardy} scoprì quanto fosse importante lo studio di \textbf{Ramanujan} e insieme a lui portò avanti la teoria dei numeri, una teoria \textbf{asintotica} che, come lui stesso affermava, non può essere esatta, ma fatta di approssimazioni.\\
Se, per esempio, si considera una quantità scritta in base $5$, quale $n = \left(412\right)_5$
\[\left(412\right)_5 = 4 \cdot 5^2 + 1 \cdot 5^1 + 2 \cdot 5^0 = (107)_{10}\]
Se, ora, si fissa una lunghezza $l = 4$, una lunghezza rigida, senza considerare zeri in testa, si può capire che con $4$ cifre si possono rappresentare, in base $5$ numeri $n$ nell'intervallo
\[1000 \leq n < 10000\]
ovvero tale per cui
\[5^{l - 1} \leq n < 5^l\]
e ciò funziona con qualsiasi base, per cui, in generale, fissata una lunghezza $l$ e una base $\mathcal{B}$ si ha che le quantità che possono essere rappresentate con $l$ cifre, non tutte uguali a $0$ è
\[\mathcal{B}^{l - 1} \leq n < \mathcal{B}^l\]
Traducendo tale risultato tramite il logaritmo in base $\mathcal{B}$, sfruttando la crescenza in senso stretto della funzione logaritmica, si ottiene, equivalentemente
\[l - 1 \leq \log_{\mathcal{B}}\left(n\right) < l\]
in cui, ovviamente,
\[\log_{\mathcal{B}}\left(n\right) < l \leq \log_{\mathcal{B}}\left(n\right) + 1\]
che si può scrivere che
\[\l_{\mathcal{B}}\left(n\right) \cong \log_{\mathcal{B}}\left(n\right)\]
Per cui l'\textbf{errore massimo} che si può commettere è di $1$ cifra in base $\mathcal{B}$, nel caso peggiore, ma sarà sempre un po' maggiore del $\log_{\mathcal{B}}\left(n\right)$, per cui il logaritmo è una \textbf{sottostima della lunghezza}. Tuttavia, nello spirito di Hardy, sarà utile anche scrivere che la lunghezza binaria di $n$ è circa uguale al logaritmo binario di $n$, ovvero
\[\log_\mathcal{B}(n) \cong \log_\mathcal{B}(n)\]
in cui si può interpetare il $\log_\mathcal{B}(n)$ come una \textbf{lunghezza analogica}, mentre $l_\mathcal{B}(n)$ è una \textbf{lunghezza digitale}, in quanto \textbf{intera}, con precisione alla cifra (senza nulla in mezzo): in molti casi sarà più utile la lunghezza analogica di quella digitale, in quanto molto più precisa.\\
Grazie a questa formula è possibile capire facilmente come si alterano le lunghezze quando si effettua un cambiamento di base. Per esempio, si può osservare che
\[\log_2(10) \cong 3.38\]
per cui la lunghezza in base $2$ è circa tre volte la lunghezza in base $10$.

\vspace{2em}
\noindent
\textbf{Esempio}: Per trasformare un numero da base $10$ in base $5$ si deve procedere per divisioni successive per $5$, considerando i resti (per questo si parla di \emph{divisione intera}). Per esempio si ha che
\[10 \div 3 = 3 \text{ con resto di } 1\]
in cui, ovviamente, il resto $r$ può essere
\[0 \leq r < D\]
con $D$ divisore. Convertendo $32$ da base $10$ a base $5$ ci si aspetta di ottenere un resto $0 \leq r \leq 4$.

\vspace{1em}
\noindent
\subsection{Architettura dei calcolatori}
Il calcolatore, naturalmente, si basa sulla logica binaria, ovvero opera impiegando la rappresentazione in base $2$.\\
Il metodo più utilizzato per rappresentare caratteri diversi da quelli binari, tramite una codifica binaria, è il metodo ASCII (dall'inglese, American Standard Code For Information Interchange). Naturalmente, siccome la codifica tramite ASCII fa uso di soli $7$ bit (sarebbero $8$, ma un bit è riservato alla parità, per la rilevazione degli errori), il numero di $n$-uple binarie che si possono ottenere è $2^7$, un numero certamente irrisorio per la rappresentazione di tutti i caratteri alfanumerici necessari per la comunicazione multilinugua.\\
In generale, fissata una lunghezza $n$, il numero di $n$-uple binarie distinte è, ovviamente, $2^n$, che rappresenta una crescita esponenziale, praticamente infinita, anche se, ovviamente, in teoria sono un numero ben limitato.\\

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri una macchina calcolatrice che considera delle istruzioni di $9$ bit come di seguito esposto:

\begin{table}[H]
  \centering
  \renewcommand\arraystretch{1.2}
  \begin{tabularx}{0.6 \textwidth}{|P|P|}
    \hline
    NOME ISTRUZIONE & ISTRUZIONE\\
    \hline
    ADD & $010\times\times\times\times\times\times$\\
    \hline
    PUNCH & $100\times\times\times\times\times\times$\\
    \hline
  \end{tabularx}
  \caption{Tabella di istruzioni operative per un calcolatore}
  \label{tab:tabella_istruzioni_calcolatore}
\end{table}

\noindent
Naturalmente, tale linguaggio è \textbf{Assembly}, ovvero un linguaggio molto simile al linguaggio macchina, che risulta particolarmente complesso da impiegare per lo sviluppo di software.

\vspace{1em}
\subsection{Diagramma di flusso}
Si consideri il seguente \textbf{flowchart}, o \textbf{diagramma di flusso}:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[node distance=2cm]
    % start
    \tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!30]
    % input/output
    \tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
    % process
    \tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=orange!30]
    % if
    \tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
    % arrow
    \tikzstyle{arrow} = [thick,->,>=stealth]

    \node (start) [startstop] {Start};
    \node (in1) [io, below of=start] {Input};
    \node (pro1) [process, below of=in1] {Processo 1};
    \node (dec1) [decision, below of=pro1] {Decisione 1};

    \draw [arrow] (start) -- (in1);
    \draw [arrow] (in1) -- (pro1);
    \draw [arrow] (pro1) -- (dec1);
  \end{tikzpicture}
  \caption{Diagramma di flusso}
  \label{fig:diagramma_flusso}
\end{figure}

\noindent
Tuttavia, tale tecnica di progettazione algoritmica è oramai superata, lasciando il posto allo \textbf{pseudocodice}, ossia un linguaggio di definizione delle istruzioni slegato da qualsiasi specifico linguaggio di programmazione di riferimento, che permette di esporre una serie di istruzioni esecutive molto simili a quelle di un programma vero e proprio.

\newpage
\section{Ordinamento (sorting)}
Si espongono, di seguito, i principi di algoritmica dei più importanti algoritmi di ordinamento.\\
Ciascuno di tali algoritmi prevede di effettuare l'ordinamento di $n$ numeri forniti come input, in modo debolmente crescente, in caso di uguaglianza.

\vspace{1em}
\subsection{Bubble-Sort}
Si espone di seguito l'algoritmo di ordinamento \textbf{bubble-sort} impiegando lo \emph{pseudocodice}:

\begin{algorithm}[H]
  \caption{Bubble-sort}\label{euclid}
  \begin{algorithmic}[1]
    \State \textbf{do} the following $n-1$ times
    \Indent
      \State \emph{point} to the $1^{\text{st}}$ element
      \State \textbf{do} the following $n-1$ times
      \Indent
      \State \emph{compare} with next
      \State \textbf{if} wrong order \emph{exchange}
      \State \emph{point} to the next
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
Naturalmente tale algoritmo è corretto e lo si può verificare immediatamente, considerando, per esempio, i seguenti $5$ elementi, così ordinati:
\[\boxed{2} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5}\]
Ovviamente il procedimento ci porta ad eseguire l'algoritmo $4$ volte. Nella prima iterazione si ottiene
\[\boxed{2} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5}\]
la seconda iterazione, invece, porta ad ottenere
\[\boxed{1} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5}\]
mentre le ultime due iterazioni sono superflue. Si capisce facilmente che tale algoritmo non risulta essere pienamente efficiente, in quante alcune iterazioni potrebbero essere evitate, tramite un \textbf{flag}, per esempio. Allo stato attuale, il numero delle iterazioni da eseguire è
\[\#\text{iterazioni} = \left(n-1\right)\cdot\left(n-1\right)\]
Se, invece, si facesse in modo di evitare alcune iterazioni si avrebbe un numero di iterazioni
\[\left(n-1\right) \leq \#\text{iterazioni} \leq \left(n-1\right)^2\]
considerando $n-1 \cong n$, e quindi $\left(n-1\right)^2 \cong n^2$ si può dire che la complessità del bubble-sort è \textbf{quadratica}, in quanto il numero delle iterazioni è $n^2$.

\newpage
\noindent
\begin{center}
  4 Marzo 2022
\end{center}
L'algoritmo bubble-sort non viene utilizzato, ad oggi, così come non si impiega lo pseudocodice in \emph{pseudo-english} (da leggere psude-inglish). Esso è funzionale, ma non efficiente, in quanto la sua complessità è $n^2$.\\
Ecco che per definire un algoritmo di ordinamento non è necessaria solamente la sua funzionalità, ma anche l'efficienza.

\vspace{1em}
\subsection{Insertion-sort}
L'insertion-sort è un algoritmo di ordinamento che prevede di considerare ciascuna quantità da ordinare ad una ad una e di effettuare un confronto solo quando ci sono dei cambiamenti.\\
La prima quantità è ovviamente già in ordine con se stessa. Se la seconda è più piccola della prima, si effettua uno scambio, per cui ora i primi due numeri sono ordinati. Si considera, ora, il terzo numero e se questo è più piccolo del secondo si effettua uno scambio e un nuovo confronto tra la seconda e la prima e così via.\\
Pertanto si effettuano tutti i confronti solamente quando si ha uno scambio delle quantità: questo comporta che il minimo numero di iterazioni è $n-1$, se $n$ è il numero delle quantità da ordinare. Se, invece, tutte le quantità sono in disordine si effettua un numeri di iterazioni pari
\[1 + 2 + ... + (n - 2) + (n - 1) = \frac{n \cdot (n + 1)}{2}\]
una formula molto semplice che Gauss determinò come segue, ovverosia scrivendo la somma dei numeri da $1$ a $k$ in ordine crescente e poi decrescente, come mostrato di seguito:

\begin{table}[H]
  \centering
  \rowcolors{1}{white}{white}
  \setlength{\tabcolsep}{5pt}
  \begin{tabular}{cccccccccccccccccccccc}
    $1$ & $+$ & $2$ & $+$ & $3$ & $+$ & $...$ & $+$ & $k-1$ & $+$ & $k$\\
    $k$ & $+$ & $k-1$ & $+$ & $k-2$ & $+$ & $...$ & $+$ & $2$ & $+$ & $1$
  \end{tabular}
\end{table}

\noindent
essendo $k$ numeri in ambedue le righe, sommando i due termini corrispondenti, uno sotto l'altro, si ottiene sempre $k+1$ che, sommato per $k$ volte produce $k \cdot (k+1)$. Tuttavia, dal momento che tale quantità è il doppio di quella richiesta si ottiene
\[\frac{k \cdot (k + 1)}{2}\]
Pertanto si ha che il numero di iterazioni dell'algoritmo \emph{insertion-sort} è
\[n \cong n - 1 \leq \#\text{iterazioni} \leq \frac{n \cdot (n - 1)}{2} \cong n^2\]
che, in maniera approssimata è
\[n \leq \#\text{iterazioni} \leq n^2\]
pertanto, nel caso migliore, la \textbf{complessità è lineare}, mentre nel caso peggiore, la \textbf{complessità è quadratica}.

\vspace{1em}
\noindent
\subsection{Pseudocodice}
Per la scrittura dello pseudocodice si devono impiegare delle notazioni e dei simboli ben specifici, che di seguito vengono riportati.

\vspace{1em}
\subsubsection{Assegnazione}
L'\textbf{assegnazione} viene indicata con il simbolo $=$ (oppure $:=$ o $\leftarrow$), anche se l'assegnazione non è un'uguaglianza. Per esempio, la notazione
\[A = 3\]
significa che nella cella di memoria $A$ viene inserito il valore $3$. Analogamente, se si scrive
\[A = A + 1\]
significa che il valore presente nella cella di memoria $A$ viene incrementato di $1$ unità.

\vspace{1em}
\subsubsection{Condizione}
La specifica della \textbf{condizione} avviene tramite l'istruzione \textbf{if}, secondo la notazione seguente:
\begin{center}
  \textbf{if} $C$ \textbf{then}\\
  \hspace{4em} istruzioni\\
  \hspace{-2.5em} \textbf{else}\\
  \hspace{4em} istruzioni\\
\end{center}

\vspace{1em}
\subsubsection{Ciclo for}
Il \textbf{ciclo for} è un'istruzione di ciclo in cui vengono indicate specificatamente le iterazioni che devono essere eseguite, secondo la notazione seguente
\begin{center}
  \textbf{for} $i = 0$ \textbf{to} $n$ \textbf{do}\\
\end{center}

\vspace{1em}
\subsubsection{Ciclo while}
Il \textbf{ciclo while} è un'istruzione di ciclo in cui si effettuano le istruzioni fintantoché la condizione specifcata è vera
\begin{center}
  \textbf{while} $C$ \textbf{do}\\
\end{center}

\vspace{1em}
\noindent
Si consideri lo pseudocodice dell'algoritmo \textbf{INSERTION-SORT(A)}, esposto di seguito, dove \textbf{A} sta ad indicare \textbf{array}, ovvero un record di $length[A] = n$ valori da ordinare, già forniti in input.\\
Di seguito si espone lo pseuodocodice, in cui si parte da $1$ come posizione iniziale dell'array:

\begin{algorithm}[H]
  \caption{Insertion-sort}\label{euclid}
  \begin{algorithmic}[1]
    \State \textbf{for} $j = 2$ \textbf{to} $length[A] = n$
    \Indent
      \State \textbf{do } $key = A[j]$
        \Indent
          \State ...
          \State $i = j - 1$
        \EndIndent
        \State \textbf{while} $i > 0 \wedge A[i] > key$
        \Indent
          \State \textbf{do } $A[i+1] = A[i]$
          \Indent
            \State $i = i - 1$
            \State $A[i+1] = key$
          \EndIndent
        \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
Tale codice é concluso e il suo funzionamento può essere facilmente verificato come segue, considerando l'array $A$ di lunghezza $length[A] = 6$:
\[\boxed{5} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{3}\]
Partendo con $j=2$ si fissa $key=A[j]=2$ e imponendo $i=1$, si entra all'interno del ciclo \emph{while}, in quanto $i>0$ e $A[i]>key$ e si effettua l'istruzione $A[i+1] = A[i]$ e $A[i+1] = key$, trovandosi nella configurazione seguente
\[\boxed{2} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{3}\]
Terminato il ciclo while e il ciclo for si procede con $j=3$ si fissa $key=A[j]=4$ e imponendo $i=2$, si entra all'interno del ciclo \emph{while}, in quanto $i>0$ e $A[i]>key$ e si effettua l'istruzione $A[i+1] = A[i]$ e $A[i+1] = key$, trovandosi nella configurazione seguente
\[\boxed{2} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{3}\]
Adesso, terminato il ciclo while e for, si considera $j=4$, specificando $key=A[j]=6$ e $i=3$. In questo caso, tuttavia, non si entra nel ciclo while, in quanto $i > 0$, ma $A[i] < key$. Si procede direttamene con $j=5$, $key=A[j]=1$ e $i=4$ e si entra nel ciclo while, compiendo tutte le iterazioni
\[\boxed{2} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{3}\]
\[\boxed{2} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{3}\]
\[\boxed{2} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{3}\]
\[\boxed{1} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{6} \hspace{0.5em} \boxed{3}\]
e così via fino ad arrivare all'ordinamento finale
\[\boxed{1} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{6}\]
Ecco che, come si può vedere, tale algoritmo ha una complessità che, nel caso migliore, è \textbf{lineare} ($n$) e nel caso peggiore è \textbf{quadratica} ($n^2$).\\
Mentre si chiama \textbf{complessità tipica} la \textbf{complessità media}, ovvero la complessità dell'algoritmo nel caso intermedio. In questo caso la complessità tipica è $n^2$, che può essere calcolata, in maniera non propriamente corretta, come segue
\[\frac{n + n^2}{2} = n^2\]
Esiste, infine, anche una \textbf{complessità empirica}, basata sull'uso pratico dell'algoritmo: in particolare, l'algoritmo insertion-sort funziona particolarmente bene quando il \textbf{numero degli elementi da ordinare è ridicolmente basso}, il che potrebbe essere un controsenso; tuttavia, potrebbe essere particolarmente utile ricorrere all'ordinamento di pochi numeri all'interno di una procedura particolarmente complessa: ecco, allora, che l'utilizzo di insertion-sort diviene conveniente (cosa che non accade per bubble-sort).

\vspace{1em}
\noindent
\textbf{Osservazione}: È importante osservare che l'algoritmo di insertion-sort è un \textbf{algoritmo di ordinamento in loco}, ovvero tale per cui non si impiega un altro array per l'ordinamento, ma tutte le operazioni si effettuano sullo stesso array di partenza.

\vspace{1em}
\noindent
\textbf{Osservazione}: Quando si parla di algoritmica, non è possibile parlare di \textbf{completezza} senza parlare di \textbf{complessità}.

\newpage
\section{Grafi}
Il \textbf{grafo} è una \textbf{struttura finita}. Gli elementi costituitvi di un grafo sono i \textbf{vertici} (o \textbf{nodi}) e gli \textbf{archi} (o \textbf{lati}) (dall'inglese \emph{arcs} o \emph{edges}). Per indicare i vertici si impiega la lettera $v$, mentre per indicare gli archi si usa la lettera $\xi$.\\
Un arco collega due vertici distinti che, per il momento, non è orientato, non rappresenta una freccia, in quanto si parla di \textbf{grafi semplici}, come illustrato di seguito:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$a$};
    \node[main node] (b) [below left of=a] {$b$};
    \node[main node] (c) [below right of=b] {$c$};
    \node[main node] (d) [below right of=a] {$d$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (b)
          edge [bend left, out=100, in=85, out looseness=2, in looseness=2] node[left] {} (c)
          % edge [loop above] node {0.1} (1)
      (b) edge node [right] {} (c)
          edge node {} (d)
          % edge [loop left] node {0.4} (2)
          % edge [bend right] node[left] {0.1} (3)
      (c) edge node [right] {} (d)
          % edge [bend right] node[right] {0.2} (4)
      (d) edge node [left] {} (a);
          % edge [loop right] node {0.6} (4)
          % edge [bend right] node[right] {0.2} (1);
  \end{tikzpicture}
  \caption{Esempio di grafo semplice}
  \label{fig:esempio_grafo_semplice}
\end{figure}

\vspace{1em}
\noindent
Il calcolo della copertura dei vertici prende il nome di \textbf{vertex cover} e prevede di definire il numero minimo di vertici essenziali per coprire tutti gli archi. L'ottimizzazione del grafo, in questo caso, prevede di determinare la copertura minima.\\
Si supponga di avere a disposizione $k$ vertici (che si indica come $\left \vert v \right \vert = k$, in cui $\left \vert v \right \vert$ rappresenta la \textbf{cardinalità} dell'insieme dei vertici). Naturalmente, la copertura minima pari a $0$ si ha quando i vertici del grafo sono tutti scollegati, ovvero non ci sono archi.\\
Il numero di archi in un grafo completo è pari a
\[\frac{\left \vert v \right \vert \cdot \left(\left \vert v \right \vert - 1\right)}{2}\]
per cui si potrebbe, in questo caso limite, affermare che la copertura minima sia $k$, ma se si elimina un nodo ancora la copertura sussiste, in quanto su ogni arco vi sarà sempre almeno un nodo coperto. Quindi si può affermare che la \emph{vertex cover}, nel caso generale è compresa tra $0$ e $k-1$, con $k$ numero dei vertici.

\newpage
\noindent
\begin{center}
  9 Marzo 2022
\end{center}
Il problema del \textbf{vertex cover}, ovvero di \quotes{ricoprimento dei vertici}, è un proplema che riguarda la teoria dei grafi.\\
Gli elementi costituitvi di un grafo sono i \textbf{vertici} (o \textbf{nodi}, molto più raramente chiamati \emph{punti}) e gli \textbf{archi} (dall'inglese \emph{edges}, traducibili in \textbf{spigoli} o, più impropriamente, in \emph{lati}), per il momento non orientati, che collegano due vertici, per il momento necessariamente distinti.\\
La notazione per indicare vertici e archi è la seguente
\begin{itemize}
  \item L'insieme dei vertici si denota con $v$
  \item L'insieme degli archi si denota con $\xi$
\end{itemize}
Naturalmente, sussiste la possibilità che in un grafo tutti i vertici siano sconnessi ed \textbf{isolati}, ovvero il numero degli archi sia nullo, per cui si ottiene che $\left \vert \xi \right \vert = 0$.\\
Analogamente, volendo collegare tutti i nodi con un arco (ottenendo un \textbf{grafo completo}), si procede come seugue: partendo da un primo vertice se ne collega un secondo, necessariamente distinto; ma non volendo considerare ogni arco due volte, si divide per due, ottenendo
\[\frac{\left \vert v \right \vert \cdot \left(\left \vert v \right \vert - 1\right)}{2} \cong \left \vert v \right \vert^2\]
da cui si evvince che il numero degli archi, in un grafo, è compreso tra
\[0 \leq \left \vert \xi \right \vert \leq \frac{\left \vert v \right \vert \cdot \left(\left \vert v \right \vert - 1\right)}{2}\]
considerando $\left \vert v \right \vert$ la cardinalità, ossia il numero dei vertici considerati che costituiscono il grafo.\\
Il problema di \textbf{vertex cover} è un problema di ottimizzazione: ridurre il numero minimo di vertici tale per cui nel grafo ad ogni arco deve essere collegato almeno un vertice coperto. Per esempio, in un grafo dove ogni nodo è isolato, il numero minimo dei vertici da coprire è $0$, in quanto non ci sono archi. Nel caso di un \textbf{grafo completo}, come quello esposto di seguito

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$a$};
    \node[main node] (b) [below left of=a] {$b$};
    \node[main node] (c) [below right of=b] {$c$};
    \node[main node] (d) [below right of=a] {$d$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (b)
          edge [bend left, out=100, in=85, out looseness=2, in looseness=2] node[left] {} (c)
          % edge [loop above] node {0.1} (1)
      (b) edge node [right] {} (c)
          edge node {} (d)
          % edge [loop left] node {0.4} (2)
          % edge [bend right] node[left] {0.1} (3)
      (c) edge node [right] {} (d)
          % edge [bend right] node[right] {0.2} (4)
      (d) edge node [left] {} (a);
          % edge [loop right] node {0.6} (4)
          % edge [bend right] node[right] {0.2} (1);
  \end{tikzpicture}
  \caption{Esempio di grafo semplice completo}
  \label{fig:esempio_grafo_semplice_completo}
\end{figure}

\vspace{1em}
\noindent
la copertura minima non è, come contrariamente si potrebbe pensare all'inizio, pari a $\left \vert v \right \vert$, in quanto eliminando uno qualsiasi dei nodi, ancora ad ogni arco sarà collegato almeno un nodo comperto. Pertanto si ha che il numero $\#nodi$ dei nodi che si dovranno coprire al fine di risolvere il problema del vertex cover in un grafo avente $\left \vert v \right \vert$ vertici sarà sempre compreso tra
\[0 \leq \#nodi \leq \left \vert v \right \vert - 1\]
Per la risoluzione meccanica del \textbf{vertex cover} vi sono due algoritmi, di cui solo il secondo realmente efficace. Si consideri, a titolo esemplificativo, il grafo seguete

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$a$};
    \node[main node] (b) [right of=a] {$b$};
    \node[main node] (c) [below of=a] {$c$};
    \node[main node] (d) [below of=b] {$d$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (b)
          edge node[left] {} (c)
          % edge [loop above] node {0.1} (1)
      (b) edge node {} (d)
          % edge [loop left] node {0.4} (2)
          % edge [bend right] node[left] {0.1} (3)
          % edge [bend right] node[right] {0.2} (4)
      (d) edge node [left] {} (a);
          % edge [loop right] node {0.6} (4)
          % edge [bend right] node[right] {0.2} (1);
  \end{tikzpicture}
  \caption{Esempio di grafo semplice}
  \label{fig:esempio_grafo_semplice_1}
\end{figure}

\vspace{1em}
\noindent
in cui, naturalmente, per coprire tutti gli archi sarà sufficiente considerare il vertice $a$ e il vertice $d$ (oppure il nodo $b$), ottenendo, come possibile copertura
\[\boxed{a} \hspace{0.5em} \boxed{b} \hspace{0.5em} \boxed{c} \hspace{0.5em} \boxed{d}\]
\[\boxed{1} \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{1}\]
in cui con $\boxed{1}$ si rappresenta la copertura del rispettivo vertice. Ecco che questa è una $k$-upla binaria di \textbf{peso} $w=1+1=2$, in cui $k=4$: usando tale notazione, quindi, un procedimento meccanico, atto a verificare la corretta copertura prevede di considerare tutti gli archi e verificare per ciascuno di essi che almeno ad un vertice collegato dall'arco in questione corrisponda un $1$; se un arco è collegato a due vertici cui corrisponde $0$, la copertura è scorretta, ovviamente.\\
Tuttavia, la $k$-upla $1001$ è anche una codifica binaria su $4$ bit del numero $(9)_{10}$, che suggerisce la procedura di controllo seguente, definita a partire da $k$ numero di vertici

\begin{algorithm}[H]
  \caption{Vertex-cover}\label{euclid}
  \begin{algorithmic}[1]
    \State \textbf{for} $i = 0$ \textbf{to} $2^k$
    \Indent
      \State $i \to $ \text{ binario}
      \State \emph{check}$(i)$
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
ove \emph{check(i)} è una procedura che svolge il compito precedentemente esposto: presa in ingresso una $k$-upla binaria, cui si fa corrispondere la copertura di rispettivi vertici, verifica per ciascun arco che ad esso sia collegato un vertice effettivamente coperto, ovvero cui corrisponde un $1$ nella $k$-upla binaria considerata; alla fine della procedura si ottiene la copertura di peso minore, ovvero quella con meno $1$ e quindi che prevede meno vertici coperti.\\
Tale algoritmo non risulta propriamente efficiente; pertanto, al fine di eliminare alcune iterazioni, si potrebbe pensare di partire con il peso $w$ ed effettuare il medesimo \emph{check} per tutte $n$-uple di peso $w$, come mostrato di seguito:

\begin{algorithm}[H]
  \caption{Vertex-cover}
  \begin{algorithmic}[1]
    \State \textbf{for} $w = 0$ \textbf{to} $k-1$
    \Indent
      \State \textbf{for} \text{ all}
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
che è un procedimento più razionale, in quanto si controlla progressivamente se sono sufficienti un numero sempre maggiore di vertici da coprire (basta $1$ vertice? Bastano $2$ e così via...) e si esce dal ciclo quando si trova il primo, in quanto quella copertura sarà certamente la minima.\\
Da notare che si cicla fino a $k-1$ e non fino a $k$, in quanto è noto dalla teoria che il numero di vertici che si andranno a coprire per risolvere un qualsiasi problema di vertex-cover è sempre compreso tra $0$ e $k-1$, con $k$ numero di vertici: ma l'unica $n$-upla con peso massimo è quella con tutti $1$, la quale costituisce una sola configurazione tra le $2^k$ possibili. Pertanto, nel caso peggiore, si potrebbe procedere ad effettuare un numero di iterazioni pari a $2^k - 1$, che non è molto dissimile dal caso precedente, in cui si facevano inevitabilmente $2^k$ iterazioni.\\
Questo algoritmo, pertanto, pur essendo corretto, è ispirato al meccanismo dell'\textbf{exaustive search} (dall'inglese, ricerca esauriente), che prevede di controllare tutti gli archi al fine di verificarne la corretta copertura.\\
Il motivo per cui tale algoritmo è inutilizzabile è che presenta un numero di \textbf{iterazioni esponenziale} e, conseguentemente, una \textbf{complessità esponenziale}, la quale è intollerabile, dal momento che il numero delle iterazioni cresce esponenzialmente al variare dell'imput.\\
Di seguito si espone, invece, un nuovo algoritmo che risolve il problema del \textbf{vertex cover}; alla base di tale algoritmo si pone la seguente idea: si considerino dapprima due nodi connessi da un arco e si eliminino tutti gli archi che incidono sul primo e sul secondo vertice e si proceda a considerare un nuovo arco che insiste su due nodi ancora non considerati e si eliminino tutti gli altri archi che incidono sui nodi stessi e così via, fino ad esaurire tutti gli archi a disposizione, tale che alla fine della procedura si ottiene un ricoprimento vero e proprio.\\
Come di consueto, nello pseudocodice esposto di seguito, l'input non viene specificato a priori, ma il numero delle iterazioni per specificare l'input è noto, ossia è pari a $\cong \left \vert v \right \vert + \left \vert \xi \right \vert$.\\
Lo pseudocodice è il seguente:

\begin{algorithm}[H]
  \caption{Vertex-cover}
  \begin{algorithmic}[1]
    \State $C \gets \varnothing$
    \State $E' \gets \xi(\mathcal{G})$
    \State \textbf{while } $E' \neq \varnothing$
    \Indent
      \State \textbf{do } \text{... } $(u,v)$ \text{ in } $E$
      \Indent
        \State $C \gets C \cup \left\{u,b\right\}$
        \State \emph{delete } incidienti
      \EndIndent
    \EndIndent
    \State \textbf{return } $C$
  \end{algorithmic}
\end{algorithm}

\noindent
In cui $C$ è un contenitore, inizialmente vuoto, all'interno del quale successivamente andranno inseriti i vertici necessari per la vertex cover. Invece, $E'$ è un contenitore, inizialmente pieno, in quanto contiene tutti gli archi che dovranno essere esaminati/scartati. Dopodiché, fintantoché non ci sono più archi da analizzare, si considera un primo arco, designato con la notazione $(u,v)$, i cui due estremi, appunto $u$ e $v$ andranno ad essere inseriti all'interno di $C$, mentre verrano eliminati da $E'$ tutti gli archi incidienti in $u$ o in $v$, finché non si avranno più archi a disposizione.\\
Il numero di iterazioni in cui tale algoritmo si impegna è, approssimativamente, pari a $\left \vert v \right \vert + \left \vert \xi \right \vert$, ovvero si ha una \textbf{complessità lineare}: questa è un'ottima notizia, in quanto significa che tale algoritmo è velocissimo; l'unico problema è che esso è scorretto.\\
Si consideri, a titolo di esempio, il seguente grafo semplice:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$a$};
    \node[main node] (b) [right of=a] {$b$};
    \node[main node] (c) [below of=a] {$c$};
    \node[main node] (d) [below of=b] {$d$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (b)
          edge node [left] {} (c)
      (b) edge node [left] {} (d)
      (d) edge node [left] {} (a);
  \end{tikzpicture}
  \caption{Esempio di scorrettezza dell'agoritmmo considerato}
  \label{fig:esempio_scorrettezza_algoritmo}
\end{figure}

\noindent
In questo caso, l'algoritmo, considerando per primo l'arco $a-b$, elimina tutti gli altri archi e produce un risultato corretto.\\
Tuttavia, se il primo arco considerato dall'algoritmo fosse $a-c$, l'algoritmo è costretto a considerare anche l'arco $b-d$, non producendo un risultato corretto.\\
Tale algoritmo prende il nome di \textbf{algoritmo approssimato}, in quanto non sempre produce un risultato corretto, ma mai disastroso.\\
Tuttavia, non esiste un algoritmo che sia corretto e anche accettabile dal punto di vista del numero delle iterazioni e quindi della complessità: pertanto ci si deve accontentare dell'algoritmo approssimato appena esposto.\\
Per capire il range di risposta di tal algoritmo, si assuma che il numero di nodi ottimali necessari alla copertura sia $\left \vert \mathcal{O} \right \vert$ e il numero dei nodi effettivamente ottenuti dalla procedura algoritmica sia $\left \vert \mathcal{P} \right \vert$, legati dalla seguente relazione, dimostrata con l'esempio precedente:
\[\left \vert \mathcal{O} \right \vert \leq \left \vert \mathcal{P} \right \vert\]
Naturalmente, per quanto visto con l'esempio precedente, tale disuguaglianza può anche essere stretta. Ovviamente, se ora si considerano gli archi privilegiati dall'algoritmo esposto, denotati con $\left \vert \mathcal{A} \right \vert$, ovverosia l'insieme degli archi che l'algoritmo ha via via considerato, esso, naturalmente, presenta la seguente cardinalità
\[\left \vert \mathcal{A} \right \vert = \frac{\left \vert \mathcal{P} \right \vert}{2}\]
in quanto su ogni arco vi sono due nodi ($u$ e $v$ nello pseudocodice), per cui basta considerarne la metà. Ora, il numero di nodi ottimale $\left \vert \mathcal{O} \right \vert$ deve essere necessariamente almeno uguale al numero di archi privilegiati $\left \vert \mathcal{A} \right \vert$, dal momento che ciascuno di tali archi deve insistere almeno su un vertice coperto. Pertanto $\left \vert \mathcal{o} \right \vert$ e $\left \vert \mathcal{A} \right \vert$ sono legati dalla seguente relazione
\[\left \vert \mathcal{O} \right \vert \geq \left \vert \mathcal{A} \right \vert\]
per cui si ottiene che
\[\frac{\left \vert \mathcal{P} \right \vert}{2} \leq \left \vert \mathcal{O} \right \vert \leq \left \vert \mathcal{P} \right \vert\]

\newpage
\section{Algoritmi aritmetici}
L'\textbf{algoritmo di Euclide} permette di calcolare il \textbf{Massimo Comune Divisore} (\textbf{M.C.D.}) tra due numeri, ma non procedendo alla fattorizzazione in fattori primi tra le due quantità considerate.\\
Infatti, normalmente, per determinare l'M.C.D. tra due quantità è necessario procedere alla scoposizione delle due in fattori primi, come mostrato di seguito per i numeri $12$ e $9$:
\begin{flalign*}
  12 & = 3 \cdot 2 \cdot 2\\
  9 & = 3 \cdot 3\\
\end{flalign*}
da cui si evince che
\[\text{MCD}(12,9)=(12,9)=3\]
Tuttavia, non è possibile procedere attraverso la fattorizzazione per la risoluzione di tale problema, in quanto gli algoritmi per la fattorizzazione sono estremamente lenti. L'algoritmo di Euclide, invece, risolve tale problematica in maniera corretta, veloce ed efficiente, basandosi sul meccanismo della \textbf{ricorsività} e delle divisioni intere. Infatti, com'è noto, la divisione può essere di due tipologie
\begin{enumerate}
  \item Divisione esatta: $10 \div 3 = 3,\overline{3}$
  \item Divisione intera: $10 \div 3 = 3$ con resto $1$
\end{enumerate}

\newpage
\noindent
\begin{center}
  10 Marzo 2022
\end{center}
Naturalmente, un algortimo non può essere applicato concretamente se ha una crescita esponenziale: esso è inutilizzabile, in quanto il tempo di risposta è troppo elevato per avere un impiego pratico.\\
L'esposto seguente, tratto da un lavoro di Gary \& Johnson, ne dà una fondamentale prova pratica, considerando un calcolatore le cui istruzioni durano $0,000001$ s per essere processate; naturalmente l'algoritmo considera input variabili, di lunghezza $10$, $20$, $30$, $40$, $50$ e $60$ e si suppone, per semplicità, che la lunghezza dell'input determini in modo quanto più preciso e linearmente dipendente il numero delle operazioni che devono essere eseguite: pertanto, se l'input ha lunghezza $60$ significa che sono necessarie $60$ operazione per terminare l'algoritmo e quindi $0,00006$ s sarà il tempo impiegato per l'esecuzione.\\
Se, invece, la complessità dell'algoritmo è quadratica, allora ciò significa che se l'input è di lunghezza $60$, il numero di operazioni diviene $60 \cdot 60 = 3600$ e quindi il numero di secondi diviene $0,0036$ s.\\
Se la complessità è cubica, allora con lunghezza dell'input di $60$ il numero di operazioni diviene $60 \cdot 60 \cdot 60$ e quindi il tempo impiegato è di $0,216$ s.\\
Con una complessità quintica, a $60$ di lunghezza corrispondono $60^5$ istruzioni e quindi $13$ minuti di esecuzione.\\
Passando ad una complessità esponenziale, come $2^n$, con lunghezza dell'imput pari a $60$ si hanno $2^{60}$ istruzioni e quindi un tempo esecutivo di $360$ secoli. Passando appena a $3^n$, con lunghezza dell'imput pari a $60$ si hanno $3^{60}$ istruzioni e quindi un tempo esecutivo di $1,3 \cdot 10^{13}$ secoli.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che considerare un calcolatore $1000$ volte più veloce può essere significativo se la complessità dell'algoritmo è polinomiale, ma è totalmente ininfluente se la complessità è esponenziale.

\vspace{1em}
\subsection{Algoritmo di Euclide}
Si consideri il seguente algoritmo, noto come \textbf{algoritmo di euclide}, estremamente fulmineo:

\begin{algorithm}[H]
  \caption{Euclide}\label{euclide}
  \begin{algorithmic}[1]
    \State \textbf{begin}
    \State $a,b = m,n$
    \State \textbf{while } $b \neq 0$ \textbf{ do } $a,b=b,a \text{ mod } b$
    \State \text{mcd} $=a$
    \State \textbf{end}
  \end{algorithmic}
\end{algorithm}

\noindent
In questo caso l'algoritmo considera come input \textbf{due numeri interi} $m$ e $n$ di cui ha senso determinare l'\textbf{m.c.d}: pertanto essi devono essere necessariamente interi per ipotesi, in quanto nell'algoritmo non è previsto un controllo sintattico a monte. Generalmente si considerano $m > n$, ma ciò è ininfluente, in quanto il programma provvede ad effettuare un cambiamento del loro ordine in automatico.\\
Nell'algoritmo vi sono delle assegnazioni composte, in cui
\[a,b = m,n\]
ovvero ad $a$ si assegna il valore $m$, mentre a $b$ si assegna il valore $n$. Così come in seguito si ha
\[a,b=b,a \text{ mod } b\]
ovvero ad $a$ si assegna il vecchio valore $b$, mentre a $b$ si assegna il resto della divisione intera tra $a$ e $b$. Alla fine del ciclo si ottiene che l'\textbf{m.c.d.} cercato è proprio $a$.\\
Si consideri, a tal proposito, il seguente esempio:
\[\underset{a}{\boxed{12}} \hspace{0.5em} \underset{b}{\boxed{9}}\]
Dopo il primo passo si ottiene
\[\underset{a}{\boxed{9}} \hspace{0.5em} \underset{b}{\boxed{3}}\]
ed infine
\[\underset{a}{\boxed{3}} \hspace{0.5em} \underset{b}{\boxed{0}}\]
ecco che nella prima cella si ha proprio l'm.c.d. cercato. Ora, tuttavia, bisogna verificare se tale algoritmo sia effettivamente corretto e che quella considerata non sia solo una combinazione; inoltre, per determinare la complessità dell'algoritmo è necessario considerare, essenzialmente, il numero di \textbf{iterazioni libere} del ciclo while, in quanto la complessitì dell'algoritmo dipende unicamente da tale fattore.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi, innanzitutto, che il ciclo while si chiude visto che in posizione $b$ sarà presente un resto, ovvero una quantità intera che progressivamente diminuisce fino a diventare $0$.\\
L'ultimo passaggio, naturalmente, è il più semplice, in quanto l'algoritmo, prima dell'ultima iterazione, si ritroverà sempre nella situazione desiderata:
\[\boxed{\alpha \cdot \text{m.c.d.}} \hspace{0.5em} \boxed{\text{m.c.d.}}\]
per cui nell'ultima iterazione, effettuando la divisione intera tra una quantità e un suo multiplo non si può che ottenere resto $0$ e, quindi, nella cella $a$ si ha proprio l'm.c.d. che si sta cercando, come mostrato di seguito:
\[\boxed{\text{m.c.d.}} \hspace{0.5em} \boxed{0}\]
Per la dimostrazione della correttezza dell'algoritmo, anche nelle fasi intermedie, si deve dimostrare essenzialmente che, a qualunque fase del processo esecutivo
\[\text{M.C.D.}(a,b) = \text{M.C.D.}(b,a \text{ mod } b)\]
ovvero che quello che si trova nelle celle $a-b$ nell'istante $t$ e ciò che si trova in $a-b$ all'istante successivo hanno entrambi lo stesso massimo comune divisore: bisogna, di fatto, dimostrare quella che in matematica prende il nome di \textbf{invariante}; in questo caso l'invariante è proprio l'm.c.d. Infatti, le quantità che si trovano progressivamente nelle due celle hanno sempre lo stesso massimo comune divisore, a qualunque istante, fino ad arrivare alla configurazione finale in cui nella prima cella è presente un multiplo del valore della seconda cella.\\
Per dimostrare, quindi, l'invarianza seguente:
\[\text{M.C.D.}(a,b) = \text{M.C.D.}(b,a \text{ mod } b)\]
si deve dimostrare che un diviore di $(a,b)$ è anche divisore di $(b,a \text{ mod } b)$ e viceversa, per cui hanno lo stesso massimo comune divisore. Si supponga, allora, che $\alpha$ divida sia $a$ che $b$, per cui si ha che $a = \alpha a'$ e $b = \alpha b'$; inoltre, considerando $r = a \text{ mod } b$ quale il resto della divisione tra $a$ e $b$, si può scrivere, per il teorema del quoziente e resto
\[a = q b + r\]
Pertanto, sapendo che $\alpha$ divide sia $a$ che $b$, bisogna verificare che $\alpha$ divida anche $r = a \text{ mod } b$ per concludere la dimostrazione dell'invarianza. Tuttavia, il teorema del quoziente e resto permette di ricavare $r$ come segue
in cui, ovviamente, si ha che
\[r = a - qb = \alpha \cdot (a' - q b')\]
in cui è evidente come $\alpha$ sia, effettivamente, un divisore anche di $r = a \text{ mod } b$. Procedendo, ora, al contrario, supponendo che $b = \alpha b'$ e $r = \alpha r'$, sempre per il teorema del quoziente e resto si può scrivere $a = q b + r = \alpha \cdot (qb' + r')$, per cui, ancora una volta, $\alpha$ divide anche $a$, come volevasi dimostrare.

\vspace{1em}
\noindent
\textbf{Osservazione}: Avendo dimostrato la correttezza dell'algoritmo di Euclide, bisogna ora procedere alla verifica della sua straordinara rapidità. Il problema della complessità dell'algoritmo di Euclide, il quale era un alessandrino, venne risolto brillantemente da un matematico francese di nome \textbf{Lamé}, nel $1844$, il quale, a tutti gli effetti, è da reputarsi il padre della \textbf{teoria della complessità}, quando \textbf{Reynaud}, nel $1811$ lo aveva già preceduto, ma si parla sempre della prima metà dell'$800$.\\
Per comprendere il meccanismo alla base dello studio della complessità, si consideri un numero rappresentato in base $10$, quale il seguente $n = (37855)_{10}$, il quale viene progressivammente ridotto di \textbf{almeno} $10$ volte ad ogni iterazione, portandolo progressivamente a $3785$, $378$, $37$, $3$ ed infine $0$.\\
Pertanto, al più, il numero di iterazioni necessarie per annientare questo valore e portarlo a $0$ è pari alla \textbf{lunghezza decimale} del numero $n$ (in questo caso pari a $5$) che, con una buona dose di approssimazione può essere considerata
\[l_{10}(n) \cong \log_{10}(n)\]
Se, ora, il numero $n$ considerato viene scritto in binario e ad ogni iterazione viene annientato della metà, al più serviranno un numero di iterazioni pari alla lunghezza binaria del numero $n$ considerato per annientarlo, ovvero
\[\#\text{iterazioni} \leq l_2(n) \cong \log_2(n)\]
Con questa premessa, considerando l'algoritmo di Euclide e procedendo con i calcoli di \textbf{Lamé}, si prendano ad esempio tre iterazioni successive del ciclo while, che producono i seguenti tre stati corrispondenti a tre istanti consecutivi dell'esecuzione:
\[\boxed{a} \hspace{0.5em} \boxed{b}\]
\[\boxed{b} \hspace{0.5em} \boxed{c}\]
\[\boxed{c} \hspace{0.5em} \boxed{d}\]
in cui, ovviamente, si ha che $a \geq b \geq c \geq d$; inoltre, per come sono stati ottenuti $a,b,c$ e $d$ appare evidente che
\[a = bq + c \hspace{0.5em} \text{e} \hspace{0.5em} b = q c + d\]
A parte il caso iniziale in cui i due numeri $a \leq b$, cui l'algoritmo provvede cambiandoli d'ordine, in generale si ha sempre che $a > b$ e, quindi, il \textbf{quoziente} della divisione tra i due numeri è sempre \textbf{almeno $\boldsymbol{1}$}, quindi, siccome $q \geq 1$ deve essere che
\[b = q c + d \geq c + d\]
ma non solo: è anche noto che il valore della prima posizione è sempre maggiore o uguale del valore nella seconda posizione, ovvero $c \geq d$, per cui si ha che
\[b = q c + d \geq c + d \geq 2d\]
Questo significa che in ogni passo doppio, ovvero ogni due iterazioni, il contenuto della seconda cella di memoria è dimezzato, o peggio, in quanto si ha che $b \geq 2d$. Volendo far sì che l'elemento nella seconda cella divenga zero, poiché ad ogni doppia iterazione il suo valore viene almeno dimezzato, il numero di passi doppi necessari è al più uguale al logaritmo in base $2$ di $n$, quindi
\[\# \text{passi doppi} \cong \log_2(n) \longrightarrow \# \text{passi singoli} < 2 \log_2(n)\]
pertanto, il numero di passi di cui si necessità per terminare il ciclo while è estremamente piccolo, anche nel caso in cui il numero $n$ considerato è enormemente grande.

\vspace{1em}
\noindent
\textbf{Osservazione}: Un altro modo per descrivere l'algoritmo di Euclide è quello che riguarda la \textbf{ricorsività}, come mostrato di seguito

\begin{algorithm}[H]
  \caption{Euclide}\label{euclide}
  \begin{algorithmic}[1]
    \State \textbf{Procedura } Euclid(a,b)
    \State \textbf{if } $b=0$ \textbf{ then}
      \Indent
        \State \textbf{return } $a$
      \EndIndent
    \State \textbf{else}
    \Indent
      \State \textbf{return } Euclid($b,a \text{ mod } b$)
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
\textbf{Esempio}: Per esempio, si considerino le iterazioni seguenti
\[\text{Euclid}(30,21)\longrightarrow\text{Euclid}(21,9)\longrightarrow\text{Euclid}(9,3)\longrightarrow\text{Euclid}(3,0)=3 \]

\vspace{1em}
\subsection{Notazione $\boldsymbol{O}$ grande}
Si considerno due funzione $f(x)$ e $g(x)$ infinite a $+ \infty$, ovvero
\[\lim_{x \to +\infty} f(x) = +\infty \hspace{1em} \text{e} \hspace{1em} \lim_{x \to +\infty} g(x) = +\infty\]
Allora le due funzioni si rassomiglieranno per $x \to +\infty$ quando hanno lo stesso ordine di infinito, ovvero
\[\lim_{x \to +\infty} \frac{f(x)}{g(x)} = \alpha \in \mathbb{R}^+ - \{0\}\]
Tuttavia, taluna è una generalizzazione molto spartana, ma la notazione $O$ grande lo è ancora di più. Si considerino, nuovamente, due funzioni $f(x)$ e $g(x)$, le quali non debbono più essere infinite a $+\infty$, ma è sufficiente che esse siano \textbf{definitivamente positive}, ovvero
\[\exists x_n : \forall x > x_n, f(x) > 0 \wedge g(x) > 0\]
In questo caso, pertanto, affermare che $f(x)$ \quotes{assomiglia} a $g(x)$ significa affermare che $f(x)$ appartiene alla stessa \textbf{classe di equivalenza} di $g(x)$ (ovvero la classe delle funzioni che rassomigliano a $g(x)$), che si indica come segue
\[f(x) \in \Theta \left(g(x)\right)\]
che, generalmente, verrà denotato con
\[f(x) = \Theta \left(g(x)\right)\]
nonostante sia più propriamente corretto impiegare un segno di appartenenza in luogo di uno di uguaglianza.

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{APPARTENENZA ALLA CLASSE DI EQUIVALENZA DI UNA FUNZIONE}}\\
    \parbox{\linewidth}{Si considerino due funzioni $f(x)$ e $g(x)$ \textbf{definitivamente positive}; si dirà che $f(x)$ appartiene alla stessa classe di equivalenza di $g(x)$ e si scriverà
    \[f(x) = \Theta \left(g(x)\right)\]
    se
    \[\exists c_1 > 0, c_2 > 0, \overline{x} : c_1 \cdot g(x) \leq f(x) \leq c_2 \cdot g(x), \hspace{1em} \forall x \geq \overline{x}\]
    ovvero si riesce a descrivere ipoteticamente, con la funzione $g(x)$, una guaina all'interno della quale racchiudere la funzione $f(x)$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\noindent
\textbf{Osservazione}: Dalla definizione appena fornita, appare evidente come questa sia a tutti gli effetti una \textbf{relazione di equivalenza}, in quanto è soddisfatta la proprietà \textbf{riflessiva}
\[g(x) = \Theta \left(g(x)\right)\]
in quanto basta considerare $c_1 = c_2 = 1$; inoltre è soddisfatta anche la proprietà \textbf{simmetrica}:
\[f(x) = \Theta \left(g(x)\right) \longrightarrow g(x) = \Theta \left(f(x)\right)\]
giacché se si ha che
\[c_1 \cdot g(x) \leq f(x) \leq c_2 \cdot g(x)\]
allora si può scrivere
\[\frac{1}{c_2} \cdot f(x) \leq g(x) \leq \frac{1}{c_1} \cdot f(x)\]
e infine quella \textbf{transitiva}
\[f(x) = \Theta \left(g(x)\right), g(x) = \Theta \left(h(x)\right) \longrightarrow f(x) = \Theta \left(h(x)\right)\]
in quanto se
\[c_1 \cdot g(x) \leq f(x) \leq c_2 \cdot g(x) \hspace{0.5em} \text{e} \hspace{0.5em} c_3 \cdot h(x) \leq g(x) \leq c_4 \cdot h(x)\]
è facile concludere che
\[c_1 \cdot c_3 \cdot h(x) \leq c_1 \cdot g(x) \leq f(x) \hspace{0.5em} \text{e} \hspace{0.5em} f(x) \leq c_2 \cdot g(x) \leq c_2 \cdot c_4 \cdot h(x)\]
per cui si ottiene che
\[c_1 \cdot c_3 \cdot h(x) \leq f(x) \leq c_2 \cdot c_4 \cdot h(x)\]

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la classe di equivalenza
\[\Theta \left(1\right)\]
Allora ad essa vi apparterranno tutte le costanti positive, così come le funzioni oscillanti che assumono valori positivi, come $f(x) = 2 + \sin(x)$, in quanto
\[2 \cdot 1 \leq f(x) \leq 3 \cdot 1, \hspace{1em} \text{con } g(x) = 1\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che se due funzioni hanno lo stesso ordine di infinito, allora appartengono alla stessa classe di equivalenza. Tuttavia non è vero il contrario.\\
Si considerino, infatti, due funzioni $f(x)$ e $g(x)$ tali che
\[\lim_{x \to +\infty} \frac{f(x)}{g(x)} = \alpha \in \mathbb{R}^+ - \{0\}\]
quindi definitivamente, per $x \geq \overline{x}$, si ha che
\[\alpha - \epsilon \leq \frac{f(x)}{g(x)} \leq \alpha + \epsilon\]
dal momento che dalla definizione di limite sia ha $\forall \epsilon > 0$, è possibile anche considerare $\epsilon = \frac{\alpha}{2}$, da cui
\[\frac{\alpha}{2} \leq \frac{f(x)}{g(x)} \leq \frac{3}{2} \alpha\]
ma allora, moltiplicando ambo i membri per $g(x)$, essendo per definizione necessariamente definitivamente positiva, la disuguaglianza si conserva e diviene
\[\frac{\alpha}{2} \cdot g(x) \leq f(x) \leq \frac{3}{2} \cdot \alpha \cdot g(x)\]
che corrisponde esattamente alla definizione di due funzioni che appartengono alla stessa classe di equivalenza, con
\[c_1 = \frac{\alpha}{2} \hspace{0.5em} \text{e} \hspace{0.5em} c_2 = \frac{3}{2} \alpha\]
Per cui si è dimostrato che avere lo stesso ordine di infinito significa anche appartenere alla stessa classe di equivalenza, ma non è vero il contrario.

\newpage
\noindent
\begin{center}
  11 Marzo 2022
\end{center}
\subsection{Funzioni di complessità}
Si consideri una funzione di complessità temporale $f(n)$ definita in funzione $n$ della lunghezza dell'input: è molto articolato, nonché scarsamente utile, provvedere al calcolo di valori precisi di una funzione di complessità temporale; presumibilmente, però, al crescere di $n$, ossia al crescere della lunghezza dell'input, aumenta anche il valore della funzione stessa.\\
Le funzioni $f(n)$ di complessità temporale oggetto di studio, comunque, sono molto generali e, comunemente, sono \textbf{definitivamente positive} e, generalmente, sono infinite per $x \to +\infty$ e vengono studiate per $x \geq 0$.\\
Com'é noto, è stata definita la classe di complessità $\Theta(f(n))$ come l'insieme di tutte le funzioni che rassomigliano alla funzione $f(n)$. In questo caso, in particolare, affermare che
\[g(n) = \Theta(f(n))\]
significa affermare che $\exists c_1 > 0, c_2 > 0, \overline{n}$ tali che
\[c_1 \cdot f(n) \leq g(n) \leq c_2 \cdot f(n), \hspace{1em} \forall n \geq \overline{n}\]
in cui $\Theta(f(n))$ è, a tutti gli effetti, una \textbf{classe di equivalenza}, in quanto gode delle proprietà di \textbf{riflessività}, \textbf{simmetria} e \textbf{transitività}.\\
Se si considera una lunghezza $l=5$ in base $10$, una lunghezza effettiva senza zeri in testa, allora il numero $n$ che si può descrivere con lunghezza $l=5$ è compreso tra
\[10000 \leq n \leq 100000 - 1 \longrightarrow 10^{l-1} \leq n < 10^l\]
e passando ai logaritmi in base $10$, ciò si traduce in
\[l_{10}(n) - 1 \leq \log_{10}(n) < l_{10}(n)\]
in cui $\log_{10}(n)$ viene considerata una \textbf{lunghezza continua}, decisamente più pratica nel suo utilizzo rispetto alla \textbf{lunghezza intera} $l_{10}(n)$, la quale è la lunghezza effettiva, ma inutilmente precisa. Naturalmente si può scrivere che
\[\frac{1}{2} \cdot l_{10}(n) \leq l_{10}(n) - 1 \leq \log_{10}(n) \leq l_{10}(n)\]
per cui si può osservare come $l_{10}(n)$ e $\log_{10}(n)$ appartengono alla stessa classe di equivalenza $\Theta(l_{10}(n))$ in quanto sono state usate le costanti $c_1 = \frac{1}{2}$ e $c_2 = 1$. Dal punto di vista della notazione $\Theta$, quindi, tali lunghezze sono perfettamente equivalenti.\\ Non solo, ma se si considerano le quantità $\log_{10}(n)$ e $\log_{2}(n)$, è noto che
\[\log_{2}(n) = \frac{\log_{10}(n)}{\log_{10}(2)}\]
in cui
\[\frac{1}{\log_{10}(n)}\]
è una costante certamente positiva, allora si evince come tali quantità siano perfettamente equivalenti secondo la notazione $\Theta$.\\
Quando si impiega la notazione $\Theta$, infatti, una funzione $f(n)$ e $\alpha \cdot f(n), \alpha \geq 0$ appartengono alla stessa classe $\Theta$ ed è per questo che normalmente viene omessa la base dei logaritmi, che comunque deve essere maggiore di $1$, la quale, di fatto, è ininfluente.\\
Inoltre si è osservato come due funzioni che hanno lo stesso ordine di infinito appartegono anche alla stessa classe $\Theta$, semplicemente applicando la definizioen di limite. Pertanto, le $3$ funzioni
\[3n^2 + 1 \hspace{1em} \frac{n^2}{1000} \hspace{1em} 8945 n^2 - n\]
appartengono alla stessa classe $\Theta(n^2)$, in quanto presentano lo stesso ordine di infinito, ovvero il limite del loro rapporto è una costante positiva.\\
Si consideri una funzione $f(n)$ infinita per $x \to +\infty$ e la funzione $g(n) = f(n) \cdot \left[\sin(n) + 2\right]$ anch'essa infinita per $x \to +\infty$ che appartengono alla stessa classe $\Theta$; tuttavia, si ha che
\[\lim_{n \to +\infty} \frac{g(n)}{f(n)} = \lim_{n \to +\infty} \sin(n) + 2 = \nexists\]
per cui due funzioni che appartengono alla stessa classe di equivalenza $\Theta$ non è detto che abbiano lo stesso ordine di infinito.

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{FUNZIONE O-GRANDE DI UN'ALTRA}}\\
    \parbox{\linewidth}{Si considerino due funzioni $f(x)$ e $g(x)$ \textbf{definitivamente positive}; allora indicare
    \[f(n) = O(g(n))\]
    significa affermare che definitivamente si ha che
    \[f(n) \leq g(n)\]
    ovvero che
    \[\exists \overline{n}, c \geq 0 : f(n) \leq c \cdot g(n), \hspace{1em} \forall n \geq \overline{n}\]
    per cui se $f(n)$ e $g(n)$ appartengono alla stessa classe di equivalenza $\Theta$, allora $f(n)$ è O-grande di $g(n)$ e $g(n)$ è O-grande $f(n)$ e viceversa: se $f(n)$ è O-grande di $g(n)$ e $g(n)$ è O-grande $f(n)$ allora $f(n)$ e $g(n)$ appartengono alla stessa classe di equivalenza $\Theta$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\noindent
\textbf{Esempio}: Si osservi che, ovviamente
\[2n^2 = O(n^2) \hspace{1em} \text{e} \hspace{1em} \frac{n}{17} = O(n^2)\]
o ancora, che quando
\[\lim_{n \to +\infty} \frac{f(n)}{g(n)} = +\infty\]
significa affermare che $g(n) = O(f(n))$.

\vspace{1em}
\noindent
\textbf{Ossevazione}: Si osservi, per quanto si è detto, che
\[
  \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{l}
      f(n) = O(g(n))\\
      g(n) = O(f(n))
    \end{array}
  \right.
\]
\textbf{se e solo se} appartengono alla stessa classe $\Theta$. Analogamente si ha che
\[
  \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{l}
      f(n) = O(g(n))\\
      g(n) = O(h(n))
    \end{array}
  \right.
\]
allora si ha che $f(n) = O(h(n))$.

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{FUNZIONE $\boldsymbol{\Omega}$-GRANDE DI UN'ALTRA}}\\
    \parbox{\linewidth}{Si considerino due funzioni $f(x)$ e $g(x)$ \textbf{definitivamente positive}; allora indicare
    \[f(n) = \Omega(g(n))\]
    significa affermare che definitivamente si ha che
    \[f(n) \geq g(n)\]
    ovvero che
    \[\exists \overline{n}, c \geq 0 : f(n) \geq c \cdot g(n), \hspace{1em} \forall n \geq \overline{n}\]
    per cui se $f(n)$ e $g(n)$ appartengono alla stessa classe di equivalenza $\Theta$, allora $f(n)$ è $\Omega$-grande di $g(n)$ e $g(n)$ è $\Omega$-grande $f(n)$ e viceversa: se $f(n)$ è $\Omega$-grande di $g(n)$ e $g(n)$ è $\Omega$-grande $f(n)$ allora $f(n)$ e $g(n)$ appartengono alla stessa classe di equivalenza $\Theta$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\noindent
\textbf{Esempio}: Si osservi che indicare
\[f(n) = \Omega(g(n))\]
\textbf{se e solo se}
\[g(n) = O(f(n))\]
Analogamente si ha che
\[
  \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{l}
      f(n) = O(g(n))\\
      f(n) = \Omega(g(n))
    \end{array}
  \right.
\]
\textbf{se e solo se} appartengono alla stessa classe $\Theta$.

\vspace{1em}
\subsection{Classi di complessità}
Le tre classi $\Theta$ che si incontreranno maggiormente saranno
\begin{enumerate}
  \item La classe \textbf{lineare} $\Theta(n)$;
  \item La classe \textbf{log-lineare} $\Theta(n \cdot \log(n))$;
  \item La classe \textbf{quadratica} $\Theta(n^2)$;
\end{enumerate}
in cui si ha che
\[n = O (n \cdot \log(n)) \hspace{1em} \text{e} \hspace{1em} n \cdot \log(n) = O(n^2)\]
ovvero la complessità log-lineare è comunque poco più complessa rispetto a quella lineare, in quanto la crescita all'infinito è molto lenta.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che la funzione
\[10^{10^{80}}n = \Theta(n)\]
anche se il suo coefficiente è estremamente elevato. Analogamentte, se si ha una funzione
\[2^{0,00000...1} = \Theta(2^n)\]
nonostante la sua crescita è estremamente lenta rispetto ad una funzione esponenziale normale. Tuttavia, tale risultato non costituisce un'anomalia nella notazione $\Theta$, in quanto tali tipologie di funzioni sono impossibili tra trovare nella vita reale, secondo una valenza empirica.

\vspace{1em}
\noindent
Si consideri il listato seguente:
\begin{algorithm}[H]
  \caption{Esempio listato 1}
  \begin{algorithmic}[1]
    \State \textbf{for} $i=1$ \textbf{to} $k$
    \Indent
      \State \textbf{for} $j=1$ \textbf{to} $k$
      \Indent
        \State $a_{i,j} = 0$
      \EndIndent
      \State \textbf{next} $i$
    \EndIndent
    \State \textbf{next} $j$
  \end{algorithmic}
\end{algorithm}

\noindent
Naturalmente il numero di iterazioni di tale algoritmo è $k \cdot k = k^2$, così come il numero delle assegnazioni. Ma se il listato fosse stato quello proposto di seguito:

\begin{algorithm}[H]
  \caption{Esempio listato 2}
  \begin{algorithmic}[1]
    \State $s=0$
    \State \textbf{for} $i=1$ \textbf{to} $k$
    \Indent
      \State \textbf{for} $j=1$ \textbf{to} $k$
      \Indent
        \State $s = s + a_{i,j}$
        \State $a_{i,j} = 0$
      \EndIndent
      \State \textbf{next} $i$
    \EndIndent
    \State \textbf{next} $j$
    \State $s = s*s$
  \end{algorithmic}
\end{algorithm}

\noindent
allora il numero della assegnazioni è significativamente aumentato, passando da $k^2$ a $2 + 2k^2$. Tuttavia, la notazione $\Theta$ fa sì che in ogni caso la complessità sia rimasta pari a $k^2$, in quanto il numero di iterazioni, e quindi l'ordine, è \textbf{inalterato}.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che nel caso dell'algoritmo \textbf{insertion-sort}, la complessità nel caso migliore è pari a $\Theta(n)$, in quanto il numero delle iterazioni è pari a $n-1$, mentre nel caso peggiore la complessità è $\Theta(n^2)$, in quanto le iterazioni sono $\frac{n \cdot (n-1)}{2}$. Per quanto concerne la complessità tipica, ovverosia la complessità media, dal punto di vista empirico si osserva che essa è dell'ordine $\Theta(n^2)$.\\
Per quanto concerne l'algoritmo di Euclide, il numero di iterazioni, nel caso peggiore era di
\[1 + 2 \cdot \log_2(\min(m,n))\]
per cui si ha che la complessità effettiva sia $O(\log(n))$, ovvero possibilmente inferiore ad una complessità logaritmica.

\vspace{1em}
\subsection{Merge-Sort}
L'algoritmo \textbf{merge-sort} è un algoritmo la cui idea alla base è molto più complessa dell'algoritmo \textbf{insertion-sort}.\\
L'input, come di consueto, è dato da un \textbf{array} $A$ di lunghezza $length(A)=n$, per cui si dovranno ordinare $n$ numeri. Inoltre, due altri input sono $p$ e $r$, i quali costituiscono delle posizioni dell'array (anche intermedie o coincidenti), tali che $1 \leq p \leq r \leq n$.\\
Di seguito si espone il listato:

\begin{algorithm}[H]
  \caption{Merge-sort}
  \begin{algorithmic}[1]
    \State \text{MERGE-SORT}$(A,p,r)$
    \State \textbf{if} $p<r$
    \Indent
      \State \textbf{then} $q = \left \lfloor {\dfrac{p+r}{2}} \right \rfloor$
      \Indent
        \State \text{MERGE-SORT}$(A,p,q)$
        \State \text{MERGE-SORT}$(A,q+1,r)$
        \State \text{MERGE}$(A,p,q,r)$
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
Dove
\[\left \lfloor {\dfrac{p+r}{2}} \right \rfloor\]
prende il nome di \emph{parte intera inferiore} della semisomma di $p$ e $r$, ovvero viene eliminata la parte dopo la virgola.\\
Inizialmente l'algoritmo opera partendo ponendo $p=1$ e $r=n$, ma successivamente le chiamate ricorsive fanno sì che $p$ e $r$ divengano delle posizioni intermedie, designate con $q$.\\
La procedura MERGE non è ricorsiva, ma presenta due cicli for in grado di ordinare due blocchi di numeri già ordinati. Tale procedura, infatti, inzia ad operare sue due insiemi di valori che sono già stati ordinati separatamente, con lo scopo di ricavare un unico insieme di valori ordinati, sfruttando l'ordinamento dell'insieme dei valori su cui si sta operando; per farlo si confrontano tutti i valori in testa agli insiemi ordinati, prendendo sempre il più piccolo tra i due che vengono confrontati, fino ad arrivare all'ultimo valore (necessariamente molto elevato, appositamente inserito come \quotes{fine-corsa}) che viene appositamente posto alla fine in modo da segnalare il termine dell'insieme di valori; in particolare si ha che $p < q < r$, in cui i valori che stanno tra $[p-q]$ e $[q-r]$ sono già stati ordinati.\\
La procedura viene esposta di seguito:

\begin{algorithm}[H]
  \caption{Merge}
  \begin{algorithmic}[1]
    \State \text{MERGE}$(A,p,q,r)$
    \State $n_1=q-p+1$
    \State $n_2=r-q$
    \State Create two new arrays $L$ and $R$
    \State \textbf{for} $i=1$ \textbf{to} $n_1$
    \Indent
      \State \textbf{do} $L[i]=A[p+i-1]$
      \Indent
        \State $R[j]=A[q+j]$
      \EndIndent
    \EndIndent
    \State ...
    \State $L[n_1+1]=+\infty, R[n_2+1]=+\infty$
    \State $i=1, j=1$
    \State \textbf{for} $k=p$ \textbf{to} $r$
    \Indent
      \State \textbf{do} \textbf{if} $L[i] \leq R[j]$
      \Indent
        \State \textbf{then} $A[k] = L[i]$
        \Indent
          \State $i = i + 1$
        \EndIndent
        \State \textbf{else} $A[k] = R[j]$
        \Indent
          \State $j = j + 1$
        \EndIndent
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
In cui le iterazioni coinvolte, naturalmente, fino alla riga $10$ sono
\[n_1 + n_2 = q - p + 1 + r - q = r - p + 1\]
ovverosia il numero delle posizioni che vanno da $p$ a $r$.\\
Mentre dalla riga $10$ in poi il numero di iterazioni è, ovviamente, ancora una volta $r - p + 1$; pertanto il numero di iterazioni complessive è pari al doppio di $r - p + 1$, quindi la complessità è dell'ordine
\[\Theta(r-p+1)\]

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri il seguente array
\[\boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{7} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{9}\]
in cui $n=8$. Per l'ordinamento di tale array, si divide lo stesso in due parti e poi successivamente ogni parte in ancora due parti e nuovamente in due parti fino ad ottenere una divisione dell'ordinamento riducendosi all'unità, in cui i singoli valori saranno automaticamente ordinati, come mostrato di seguito:
\[\boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{1} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{7} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{9}\]
e poi
\[\boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{1} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{7} \hspace{0.5em} \boxed{2} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{9}\]
ed infine
\[\boxed{4} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{5} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{3} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{1} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{7} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{2} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{0} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{9}\]
Dal momento che i blocchi da $1$ sono già ordinati si procede a ad utilizzare la procedura \emph{Merge} per ordinare i blocchi da $2$, ottenenendo
\[\boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{3} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{7} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{9}\]
e poi procedendo con i blocchi da $4$, applicando la procedura \emph{Merge} si ottiene
\[\boxed{1} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \left\vert\right. \hspace{0.5em} \boxed{0} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{7} \hspace{0.5em} \boxed{9}\]
e infine si ordina il blocco completo, applicando la procedura \emph{Merge} si ottiene
\[\boxed{0} \hspace{0.5em} \boxed{1} \hspace{0.5em} \boxed{2} \hspace{0.5em} \boxed{3} \hspace{0.5em} \boxed{4} \hspace{0.5em} \boxed{5} \hspace{0.5em} \boxed{7} \hspace{0.5em} \boxed{9}\]
Questa, naturalmente, è una \textbf{esecuzione parallela}, mentre quella dell'algoritmo è \textbf{lineare}, in quanto va ad analizzare progressiamente ogni ramo di un albero binario, risalendolo progresivamente.

\vspace{1em}
\noindent
\textbf{Osservazione}: Tale algoritmo è \textbf{profondamente rigido}, in quanto il numero delle iterazioni che si devono eseguire è sempre lo stesso, indipendente dal caso migliore, peggiore e generale.\\
Questa non è una buona caratteristica, la quale rende ragionevole l'utilizzo di tale algoritmo quando i numeri sono inzialmente fortemente disordinati; se i numeri iniziali sono parzialmente ordinati è più conveniente impiegare l'algoritmo di insertion-sort.\\
La complessità di tale algoritmo, tuttavia, nel caso peggiore è \textbf{log-lineare} ed è estremamente ottima in quanto un algortimo di ordinamento generale, come il merg-sort (che non richiede una specifica struttura dei dati in ingresso) non è in grado di battere una complessità log-lineare.\\
Il principio di costruzione di tale algoritmo, nonché la tecnica impiegata per la risoluzione di tale problema, prende il nome di \emph{Divide et impera} (o \emph{Divide and conquer}).

\newpage
\noindent
\begin{center}
  16 Marzo 2022
\end{center}
Una componente fondamentale dello studio algoritmico è la \textbf{relazione di ricorrenza lineare}, alla base degli algoritmi che si fondano sulla \textbf{ricorsività}.\\
È noto che la sezione aurea si basa sulla suddivisione di un segmento in maniera armoniosa:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \draw (0,0) node[circ]{} -- ++(2,0) node[circ]{} -- ++(1,0) node[circ]{};
    \draw (1,0) node[above]{$a$} (2.5,0) node[above]{$b$};
    \draw (1.5,-0.1) node[below]{$L=a+b$};
  \end{tikzpicture}
  \caption{Sezione aurea}
  \label{fig:sezione_aurea}
\end{figure}

\noindent
Rispettando la proporzione seguente
\[a + b : a = a : b\]
e ponendo $b=1$ si ottiene che
\[a + 1 : a = a : 1\]
che dà vita all'equazione di secondo grado seguente (e ricordando che il prodotto dei medi è uguale al prodotto degli estremi):
\[a^2 - a - 1 = 0\]
che produce due soluzioni
\[a_{1,2} = \frac{1 \pm \sqrt{5}}{2}\]
in cui la soluzione più importante è quella naturalmente positiva, ovvero:
\[\phi = \frac{1 + \sqrt{5}}{2} \cong 1.62 \hspace{1em} \text{e} \hspace{1em} \psi = \frac{1 - \sqrt{5}}{2}\]

\vspace{1em}
\noindent
\textbf{Esempio}: Alla base dello studio della ricorsività vi é il \emph{Liber Abaci} di Fibonacci, il quale è anche l'ideatore della sequenza di Fibonacci e delle straordinarie proprietà ad essa collegate; all'interno del Liber Abaci si espone anche il problema della crescita demografica dei conigli, alla base del quale vi è proprio una relazione di ricorrenza.\\
Per la visualizzazione del problema si parte dalla condizione iniziale che riguarda la popolazione dei conigli seguente:
\[F_1 = F_2 = 1\]
e si espone anche una legge che permette di calcolare l'evoluzione demografica dei conigli nel tempo, ossia la cosiddetta legge di Fibonacci, ovvero una \textbf{relazione di ricorrenza lineare}:
\[\boxed{F_n = F_{n-2} + F_{n-1}}\]
Naturalmente, nella risoluzione del problema, ciò che era maggiormente importante era determinare una formula chiusa, la quale è la seguente:
\[F_n = \frac{\phi^n - \psi^n}{\sqrt{5}}\]
ma siccome $\psi < 0$, implode a $0$, per cui con ottima approssimazione si può affermare che
\[F_n = \frac{\phi^n - \psi^n}{\sqrt{5}} \cong \frac{\phi^n}{\sqrt{5}}\]
che sottolinea il carattere esponenziale della crescita dei conigli; tale risultato, inoltre, permette di ottenere anche l'evidenza seguente:
\[\lim_{n \to +\infty} \frac{F_{n+1}}{F_n} = \phi\]
Pertanto la relazione di ricorrenza lineare è accompagnata da una formula chiusa che, in questo caso, è una formula esatta (un risultato impressionante visto che, partendo da una relazione di ricorrenza, ottenere una formula ciusa non è banale).\\
In generale, infatti, un approccio sistematico ad una relazione di ricorrenza lineare permette di ottenere una formula chiusa, la quale, però, è tutt'altro che precisa, in quanto fornita in termini della notazione O-grande.

\vspace{1em}
\subsection{Master Theorem}
Il \textbf{teorema maestro}, o il \textbf{teorema principale delle relazioni di ricorrenza}, si può applicare solamente in casi ben determinati (e particolarmente fortunati). In ogni caso, tale teorema inizia ad operare partendo dall'\textbf{espressione della condizione iniziale}
\[T(1) = \Theta(1)\]
ovvero $T(1)$ appartiene alla classe di equivalenza $\Theta(1)$, la quale è una definizione decisamente vaga (esistono due costanti positive tra cui $T(1)$ è compreso, ma null'altro), per cui non vale la pena di scriverla, giacché non interviene in nessuna maniera nella soluzione (benché sia fondamentale che vi sia, per decretare l'applicazione del teorema o meno).\\
La relazione, invece, alla base del teorema è la seguente
\[\boxed{T(n) = a_1 \cdot T \left(\frac{n}{\lfloor b \rfloor}\right) + a_2 \cdot T \left(\frac{n}{\lceil b \rceil}\right) + f(n)}\]
in cui $f(n)$ è una funzione nota. Tuttavia, la parte intera inferiore o superiore è ininfluente, per cui, posto $a=a_1+a_2$, si preferisce la formula abbreviata
\[\boxed{T(n) = a \cdot T \left(\frac{n}{b}\right) + f(n)}\]
imponendo che $a \geq 1$ e $b > 1$ (altrimenti non è possibile applicare il teorema considerato, cercando di ricorrere ad altri metodi).\\
Supponendo che le condizioni di cui sopra siano verificati, è possibile applicare il teorema: l'applicazione del teorema maestro prevede $3$ differenti casi, a seconda della \textbf{natura della funzione $\boldsymbol{f(n)}$}:
\begin{itemize}
  \item la funzione $f(n)$ è molto piccola
  \item la funzione $f(n)$ è giusta
  \item la funzione $f(n)$ è molto grande
\end{itemize}
Naturalmente, per confrontare tale funzione $f(n)$ se ne definisca una seconda, chiamata \textbf{funzione di confronto}:
\[\phi(n) = n^{\log_b(a)}\]
che assicura che la funzione sia crescente grazie alla base $b>1$ del logaritmo. Pertanto si ha che il \emph{master theorem} può essere applicato in questi tre differenti casi:
\begin{itemize}
  \item La funzione $f(n)$ è \textbf{molto} più piccola della funzione $\phi(n)$, tuttavia \textbf{non è sufficiente} scrivere
  \[f(n) = O(\phi(n))\]
  per cui di fatto, rimane una sorta di \quotes{buco} in cui il teorema non si può applicare, ossia quando la funzione $f(n)$ è troppo piccola per rientrare nella seconda casistica, ma troppo grande per entrare nel primo caso.
  \item La funzione $f(n)$ rassomiglia alla funzione $\phi(n)$, ovvero
  \[f(n) = \Theta(\phi(n))\]
  \item La funzione $f(n)$ è \textbf{molto} più grande della funzione $\phi(n)$, che è una condizione che in algoritmica serve a poco, in quanto non basta affermare che
  \[f(n) = \Omega(\phi(n))\]
  per cui, ancora una volta, rimane un buco, determinato da funzioni che sono troppo grandi per rientrare nella seconda casistica, ma troppo piccole per per entrare nel terzo caso, per cui il teorema non si può impiegare.
\end{itemize}
Tuttavia, alla terza e ultima casistica si deve aggiungere anche un'ulteriore condizione che la funzione $f(n)$ deve soddisfare affinché si possa applicare il teorema; il problema, però, che si viene a determinare, è che la condizione in questione non è stobile rispetto alla classe di equivalenza $\Theta$ di appartenenza della funzione $f(n)$: è possibile che due funzioni $f(n)$ e $g(n)$ appartenenti alla stessa classe di equivalenza possano l'una soddisfare la condizione, la l'altra no, permettendo nel primo caso di applicare il teorema, nel secondo di non applicarlo.\\
Dal momento che in algoritmica la funzione $f(n)$ non si conosce bene giacché è nota solamente la classe $\Theta$ a cui appartiene, non è sempre possibile sapere se si può applicare il teorema o meno nel terzo caso.\\
Pertanto in algoritmica i casi più importanti sono il primo e il secondo; tuttavia, il caso che viene trattato nel dettaglio, però, è solo il secondo. Nel secondo caso è noto che $a \geq 1$, $b > 1$ e le due funzioni $f(n)$ e la funzione di confronto $\phi(n)$ appartengono alla stessa classe; da notare che vi è \textbf{transitività}, per cui il fatto di non conoscere $f(n)$ diviene irrilevante quando è nota la classe di equivalenza $\Theta$ di appartenenza della funzione $f(n)$: se $f(n)$ è equivalente a $\phi(n)$, qualunque funzione equivalente a $f(n)$ è equivalente a $\phi(n)$ per transitività.\\
Tramite procedimenti matematici omessi, si giunge alla soluzione matematica seguente
\[\boxed{T(n) = \Theta \left(f(n) \cdot \log(n)\right)}\]
in cui la base del logaritmo è ininfluente dal punto di vista della notazione $\Theta$.

\vspace{1em}
\noindent
\textbf{Osservazione}: Nell'algoritmo \textbf{merge-sort}, la procedura \textbf{merge} permette di ordinare dei valori partendo da due insiemi di valori già ordinati, ottenendo un ultimo insieme perfettamente ordinato. La procedura \emph{merge} non è una procedura ricorsiva, mentre l'algoritmo \textit{merge-sort} si basa sul meccanismo della ricorsività, andando a scorrere un albero binario da sinistra verso destra, risalendolo progressivammente (con una esecuzione lineare e sequenziale, per nulla parallela).\\
L'algoritmo è assolutamente corretto, dal punto di vista esecutivo, è ciò è evidente, anche solo dal punto di vista empirico; tuttavia, ciò che non appare evidente è la complessità dell'algoritmo considerato.\\
Si ripropone di seguito, a tal proposito, il listato dell'algoritmo \textbf{merge-sort}:

\begin{algorithm}[H]
  \caption{Merge-sort}
  \begin{algorithmic}[1]
    \State \text{MERGE-SORT}$(A,p,r)$
    \State \textbf{if} $p<r$
    \Indent
      \State \textbf{then} $q = \left \lfloor {\dfrac{p+r}{2}} \right \rfloor$
      \Indent
        \State \text{MERGE-SORT}$(A,p,q)$
        \State \text{MERGE-SORT}$(A,q+1,r)$
        \State \text{MERGE}$(A,p,q,r)$
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
Lo sforzo per ordinare $n$ numeri, naturalmente, è dato dalla seguente semplice relazione
\[T(n) = T \left(\frac{n}{2}\right) + T \left(\frac{n}{2}\right) + f(n)\]
ovvero la somma dello sforzo per ordinare ciascuna metà più l'operazione di \emph{merge} delle due metà, denotata dalla funzione $f(n)$, che, in forma più compatta, può essere scritta come segue
\[T(n) = 2 \cdot T \left(\frac{n}{2}\right) + f(n)\]
ed applicando tale formula a qualunque passo si ottiene, per esempio nel caso di $\left(\frac{n}{2}\right)$
\[T\left(\frac{n}{2}\right) = 2 \cdot T \left(\frac{n}{4}\right) + f\left(\frac{n}{2}\right)\]
mentre nel caso di $\left(\frac{n}{4}\right)$ si ha
\[T\left(\frac{n}{4}\right) = 2 \cdot T \left(\frac{n}{8}\right) + f\left(\frac{n}{4}\right)\]
e via dicendo, fintantoché non si arriva a $1$. Inoltre, per quanto concerne la procedura \emph{merge}, partendo da un array iniziale $A$, si creavano ulteriori due array, uno di destra $R$, di dimensione $n_1$, e uno di sinistra $L$, di dimensione $n_2$, e per farlo si necessitava di $n = n_1 + n_2$ iterazioni (ovviamente pari alla somma del numero di valori che compongono il primo e il secondo array di lavoro). Per l'ordinamento di tutti i valori, ancora una volta, si necessitava di $n = n_1 + n_2$ (sempre pari al numero totale di valori da confrontare) iterazioni, per un totale di $2n$ iterazioni: pertanto la complessità della procedura \emph{merge} è dell'ordine $\Theta(n)$ (in quanto il coefficiente $2$ è totalmente ininfluente per la notazione $\Theta$).\\
Pertanto, il problema da risolvere tramite il \emph{teorema maestro} è il seguente
\[T(n) = 2 \cdot T \left(\frac{n}{2}\right) + \Theta(n)\]
in cui, evidente, se confrontata con la formula generale del \emph{teorema maestro}, si evince che $a=b=2$; inoltre si osservi che impiegare $\Theta(n)$ in luogo di $f(n)$ è una scorrettezza, giustificata dal fatto che non é nota la funzione precisa appartenente alla classe $\Theta(n)$. Per l'applicazione del \emph{teorema maestro} è necessario considerare una funzione di confrono, come quella precedentemente introdotta
\[\phi = n^{\log_b(a)}\]
ma essendo $a=b=2$ si ottiene che
\[\phi = n^{\log_2(2)} = n^1 = n\]
Ecco che allora la funzine $f(n)$ che non è nota, ma appartiene alla classe $\Theta(n)$ rassomiglia alla funzione di confronto; pertanto si può applicare il secondo caso del \emph{teorema maestro} e anche la soluzione ad esso associata, ovvero
\[\boxed{T(n) = \Theta \left(f(n) \cdot \log(n)\right)}\]
che nel caso qui considerato, essendo $f(n) = \Theta(n)$ si traduce in
\[T(n) = \Theta(n \cdot \log(n))\]
ovvero la \textbf{complessità di merge-sort} è log-lineare, la quale è una complessità generale e \textbf{profondamente rigida}, in quanto le operazioni che vengono svolte sono sempre le stesso; pertanto tale complessità si mantiene costante sia nel caso migliore, sia nel caso peggiore, sia un quello medio.\\
Una complessità log-lineare è leggermente peggiore rispetto ad una complessità lineare, ma è comunque una complessità ottima: infatti, esiste un teorema che afferma che, asintoticamente, un \textbf{algoritmo di ordinamento generale} (ovvero un algoritmo che si propone di ordinare dei dati indipendemente da come essi vengono strutturati) è destinato ad avere una complessità nel caso peggiore \textbf{almeno log-lineare} e ad avere una complessità nel caso medio \textbf{almeno log-lineare}, che sono esattamente le prestazioni che merge-sort garantisce.\\
Particolarmente infruttuosa è la complessità di merge-sort nel caso migliore, la quale è sempre log-lineare, mentre nel caso dell'algoritmo insertion-sort è lineare. Questo permette di affermare che l'algoritmo merge-sort risulta essere particolarmente utile nel suo utilizzo quando i valori da ordinare non sono già ordinati, ma totalmente in disordine; nel caso fossero parzialmente ordinati, sarebbe più conveniente impiegare insertion-sort.

\vspace{1em}
\subsection{Heap-sort}
La struttura di grafo più comunque è quella di \textbf{albero}; in un grafo semplice è sempre possibile costruire un percorso chiuso, in cui partendo da un primo vertice, utilizzando almeno un arco, solamente archi in un solo senso (ovvero un arco serve per andare solamente da $a$ a $b$ e non viceversa) e senza archi multipli (ovvero non è possibile che vi siano più archi che collegano $a$ e $b$) si ritorna al vertice di partenza, come mostrato di seguito:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$a$};
    \node[main node] (b) [below left of=a] {$b$};
    \node[main node] (c) [below right of=a] {$c$};
    \node[main node] (d) [below of=c] {$d$};
    \node[main node] (e) [below of=b] {$e$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (b)
          edge node [left] {} (c)
      (c) edge node [right] {} (d)
      (d) edge node [right] {} (e)
      (e) edge node [right] {} (b);
  \end{tikzpicture}
  \caption{Esempio di grafo con percorso chiuso}
  \label{fig:esempio_grafo_percorso_chiuso}
\end{figure}

\vspace{1em}
\noindent
La definizione di albero segue dall'assenza di percorso chiusi all'interno di un grafo semplice, come esposto nella definizione che segue:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{ALBERO}}\\
    \parbox{\linewidth}{Un grafo semplice \textbf{privo percorsi chiusi} prende il nome di \textbf{albero}. \vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\noindent
Tuttavia, tale definizione è eccessivamente generica, in quanto molto spesso si preferisce operare con \textbf{alberi radicati} (rooted-tree), ovvero degli alberi in cui è stata opportunamente specificata la \textbf{radice}, ossia un vertice specifico.\\
Un esempio di \textbf{albero radicato}, con \textbf{radice} (root) il vertice $a$, che verrà considerato viene raffigurato nel seguito:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$a$};
    \node[main node] (b) [below left of=a] {$b$};
    \node[main node] (c) [below right of=a] {$c$};
    \node[main node] (d) [xshift=2em, below left of=b] {$d$};
    \node[main node] (e) [xshift=-2em, below right of=b] {$e$};
    \node[main node] (f) [xshift=2em, below left of=c] {\large$f$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (b)
          edge node [left] {} (c)
      (b) edge node [right] {} (d)
      (b) edge node [right] {} (e)
      (c) edge node [right] {} (f);
  \end{tikzpicture}
  \caption{Esempio di albero radicato di radice $a$ e profondità $2$}
  \label{fig:esempio_albero_radicato_radice_a_profondita_due}
\end{figure}

\vspace{1em}
\noindent
Dove per \textbf{profondità} è da intendersi il livello più basso raggiunto dai vertici dell'albero. I vertici che non presentano figli prendono, invece, il nome di \textbf{foglie} (in questo albero le foglie sono i vertici $d$,$e$,$f$).\\
L'albero radicato che maggiormente verrà consideranto per la progettazione algoritmica è l'\textbf{albero binario}, in cui i figli di ogni vertice sono \textbf{al più $\boldsymbol{2}$}. Un albero binario ben strutturato prevede che vi siano $n$ vertici e che l'albero sia \textbf{completo}, ovvero vengono saturati tutti i livelli di diversa profondità dell'albero prima di raggiungere al livello successivo, come mostrato di seguito:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$1$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$2$};
    \node[main node] (3) [xshift=2em, below right of=1] {$3$};
    \node[main node] (4) [xshift=2em, below left of=2] {$4$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$5$};
    \node[main node] (6) [xshift=2em, below left of=3] {$6$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$7$};
    \node[main node] (8) [xshift=4em, below left of=4] {$8$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$9$};
    \node[main node] (10) [xshift=4em, below left of=5] {\large$10$};
    \node[main node] (11) [xshift=-4em, below right of=5] {\large$11$};
    \node[main node] (12) [xshift=4em, below left of=6] {\large$12$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9)
      (5) edge node [right] {} (10)
      (5) edge node [right] {} (11)
      (6) edge node [right] {} (12);
  \end{tikzpicture}
  \caption{Esempio di albero binario completo}
  \label{fig:esempio_albero_binario_completo}
\end{figure}

\vspace{1em}
\noindent
In cui la regola di completamento prevede di procedere da sinistra verso destra, completando ciascun livello prima di procedere in quello successivo, eccezion fatta per l'ultimo che, per ragioni di numero, non può essere completato. In un albero binario completo è facile capire la profondità dello stesso, semplicemente osservando che
\[\boxed{2^h \leq n < 2^{h+1}}\]
in cui $h$ è la profondità dell'albero, mentre $n$ è il numero di vertici dell'albero. Pertanto si ottiene che
\[\boxed{h \leq \log_2(n) < h+1}\]
ovvero ciò significa che $h$ e $\log_2(n)$ appartengono alla stessa classe $\Theta$.\\
Dire che un albero è \textbf{super completo} significa che anche il livello delle foglie è pieno, e quindi $n$ deve essere una potenza di $2$ meno uno, ovvero $n=2^h-1$, con $h$ l'altezza dello stesso. Inoltre è facile osservare come il numero di nodi in un albero \textbf{super completo} sia
\[2^0 + 2^1 + 2^2 + 2^3 + ... + 2^{h-1} + 2^h = n\]
in cui $h$ è la profondità dell'albero.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi la progessione geometrica seguente
\[1 + q + q^2 + q^3 + ... + q^k = \sum_{i=0}^k q^i\]
alla quale può essere associata una formula chiusa, come mostrato di seguito
\[(1 + q + q^2 + q^3 + ... + q^k) \cdot (q-1) = (q + q^2 + ... + q^{k+1} - 1 - q - q^2 - ... - q^k) = (q^{k+1} - 1)\]
e dividendo ambo i membri per $(q-1)$ si ottiene
\[(1 + q + q^2 + q^3 + ... + q^k) \cdot (q-1) = (q^{k+1} - 1) \longrightarrow \sum_{i=0}^k q^i = \frac{q^{k+1} - 1}{q - 1}\]
In cui, nel caso in cui la ragione $q=2$, si ottiene che
\[\sum_{i=0}^k 2^i = 2^{k+1} - 1\]
Per cui è facile osservare che in un albero binario super completo, di altezza $h=k+1$, la somma di tutti i vertici di ogni livello è pari a $2^h-1$, come già dimostrato. Inoltre, nel livello delle foglie vi sarà sempre un numero di vertici, pari a $2^h$, superiore a tutto il resto dell'albero, in cui il numero di vertici è $2^h-1$, ovvero
\[\# \text{numero vertici} \cong 2 \cdot \# \text{numero foglie}\]
ovvero, dal punto di vista della notazione $\Theta$, il numero di foglie è confondibile con il numero di vertici.

\vspace{1em}
\noindent
\textbf{Osservazione}: Il numero di archi in un albero di $k$ vertici è esattamente pari a $k-1$ (mentre nel caso di un grafo semplice, i nodi possono essere tutti isolati, con numero di archi pari a $0$, oppure possono essere tutti connessi, con un numero di archi dell'ordine $\Theta(n^2)$), per cui dal punto di vista della notazione $\Theta$ il numero di vertici e il numero degli archi sono confondibili.\\

\vspace{1em}
\noindent
L'algoritmo \textbf{heap-sort} è un algoritmo di ordinamento per cataste, ove per catasta è da intendersi un albero binario completo, non necessariamente super completo, arricchito di ulteriori proprietà; tale proprietà all'interno della logica dell'\textbf{heap-sort}, è la designazione dei vertici, assegnando un valore numerico a ciascun vertice, facendo attenzione a far sì che il numero di ogni padre sia maggiore o uguale di ogni suo figlio (e di conseguenza, per ovvie ragioni di costruzione, di tutti quelli nei livelli sottostanti), ottenendo una \textbf{catasta} (heap), come mostrato di seguito:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$7$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$5$};
    \node[main node] (3) [xshift=2em, below right of=1] {$7$};
    \node[main node] (4) [xshift=2em, below left of=2] {$4$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$3$};
    \node[main node] (6) [xshift=2em, below left of=3] {$2$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$1$};
    \node[main node] (8) [xshift=4em, below left of=4] {$3$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$2$};
    \node[main node] (10) [xshift=4em, below left of=5] {$1$};
    \node[main node] (11) [xshift=-4em, below right of=5] {$0$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9)
      (5) edge node [right] {} (10)
      (5) edge node [right] {} (11);
  \end{tikzpicture}
  \caption{Esempio di catasta (heap)}
  \label{fig:esempio_catasta_heap}
\end{figure}

\vspace{1em}
\noindent
All'interno della logica dell'\textbf{heap-sort} è possibile distinguere tra \emph{maximum-heap} e \emph{minimum-heap}: nel primo caso è il padre ad avere il valore numerico maggiore rispetto ai figli, nel secondo il genitore deve essere dominato da entrambi i figli.\\
Il primo algoritmo che si andrà a considerare prende il nome di \textbf{build-heap}, ovvero un algoritmo che è capace di creare una catasta che rispetta il vincolo del \emph{maximum-heap} partendo da un albero in cui i nodi sono già pesati.

\newpage
\noindent
\begin{center}
  17 Marzo 2022
\end{center}
Si consideri un array, ovverosia una struttura dati assolutamente fondamentale in informatica, la quale presenta un numero di locazioni di memoria consecutive pari alla sua lunghezza $length(A)=n$.\\
Pertanto, scrivere
\[A[3]\]
signfica considerare il contenuto della $3$ cella di memoria consecutiva memorizzata nell'array $A$; inoltre, si ha che lo sforzo per accedere ad una precisa locazione di memoria è dell'ordine $\Theta(1)$ (pari ad una sola operazione macchina), ovvero viene eseguito un \textbf{accesso diretto} (o \textbf{random access}, accesso casuale, o \textbf{accesso arbitrario}) alla cella di memoria richiesta, indipendemente dalla lunghezza dell'array.\\
Un array è, quindi, una struttura a random-access, ovvero ad accesso lineare, designato con una lettera maiuscola (come $A$) e di una lunghezza $n$. Tuttavia, la sua stuttura lineare si può interpretare come un albero binario (bidimensionale). Per esempio, l'array seguente
\[A = \underset{1}{\boxed{7}} \hspace{0.5em} \underset{2}{\boxed{8}} \hspace{0.5em} \underset{3}{\boxed{2}} \hspace{0.5em} \underset{4}{\boxed{9}} \hspace{0.5em} \underset{5}{\boxed{4}} \hspace{0.5em} \underset{6}{\boxed{1}} \hspace{0.5em} \underset{7}{\boxed{0}} \hspace{0.5em} \underset{8}{\boxed{5}} \hspace{0.5em} \underset{9}{\boxed{6}}\]
viene rappresentato tramite il seguente albero binario

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$7$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$8$};
    \node[main node] (3) [xshift=2em, below right of=1] {$2$};
    \node[main node] (4) [xshift=2em, below left of=2] {$9$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$4$};
    \node[main node] (6) [xshift=2em, below left of=3] {$1$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$0$};
    \node[main node] (8) [xshift=4em, below left of=4] {$5$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$6$};

    \draw (1) ++(-1,0) node[]{Pos $1$};
    \draw (2) ++(-1,0) node[]{Pos $2$};
    \draw (4) ++(-1,0) node[]{Pos $4$};
    \draw (8) ++(-1,0) node[]{Pos $8$};
    \draw (5) ++(-1,0) node[]{Pos $5$};
    \draw (9) ++(1,0) node[]{Pos $9$};
    \draw (3) ++(1,0) node[]{Pos $3$};
    \draw (6) ++(1,0) node[]{Pos $6$};
    \draw (7) ++(1,0) node[]{Pos $7$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9);
  \end{tikzpicture}
  \caption{Esempio di catasta (heap)}
  \label{fig:esempio_catasta_heap}
\end{figure}

\vspace{1em}
\noindent
La possibilità di considerare una struttura monodimensionale come un albero bidimensionale avviene tramite due algoritmi fondamentali (\textbf{heapify} e \textbf{buil-heap}), che adoperano le seguenti tre procedure:

\begin{algorithm}[H]
  \caption{Parent}
  \begin{algorithmic}[1]
    \State PARENT($i$)
    \Indent
      \State \textbf{return} $\left \lfloor \frac{i}{2} \right \rfloor$
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Left}
  \begin{algorithmic}[1]
    \State LEFT($i$)
    \Indent
      \State \textbf{return} $2i$
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Right}
  \begin{algorithmic}[1]
    \State RIGHT($i$)
    \Indent
      \State \textbf{return} $2i+1$
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
In cui sono stati omessi degli opportuni controlli di validità dell'input $i$ (come se $i=1$ e si richiamasse la procedura Parent($1$), la quale restituirebbe il valore $0$ che è ovviamente corretto, o se non esiste un nodo sinistro/destro e si richiama la procedura Left/Right); questo in quanto si ha piena fiducia nell'utilizzo affidabile e consono di tali procedure; tramite esse, il calcolatore riesce ad interpretare una struttura monodimensionale come un albero bidimensionale.\\
La prima procedura che verrà impiegata nell'heap-sort (ordinamento per cataste) prende il nome di \textbf{heapify}, un sotto-algoritmo che verrà impiegato frequentemente dall'algoritmo \textbf{heap-sort}: una catasta, naturalmente, è una struttura ad albero in cui nei diversi nodi si trovano i numero che debbono essere ordinati (un ordinamento che poi si rifletterà nell'array di partenza).\\
Heapify opera su un sottoalbero dell'albero principale, la cui radice è uno dei nodi dell'albero primario individuato: ogni nodo, quindi, andrà ad individuare un sotto-albero (nel caso di una foglia, naturalmente il sottoalbero non esiste).\\
L'algoritmo \textbf{heapify}, tuttavia, viene utilizzato solamente quando il sottoalbero è noto a priori essere una catasta che rispetta il vincolo \textit{maximum-heap}, eccezion fatta per la sottoradice del sottoalbero, in cui non è detto che venga rispettato tale vincolo.\\
Per comprendere il funzionamento di \textbf{heapify} si consideri il listato esposto di seguito, in cui si prende in considerazione il vertice di numero d'ordine $i$ e il sottoalbero di $A$ che presenta come sottoradice il nodo $i$:

\begin{algorithm}[H]
  \caption{Heapify}
  \begin{algorithmic}[1]
    \State HEAPIFY($A,i$)
    \State $l=$ LEFT($i$), $r=$ RIGHT(i)
    \State \textbf{if} $l\leq$ heap-size[$A$] $\wedge$ $A[l]>A[i]$
    \Indent
      \State \textbf{then}
      \Indent
        \State largest $=l$
      \EndIndent
      \State \textbf{else}
      \Indent
        \State largest $=i$
      \EndIndent
    \EndIndent
    \State \textbf{if} $r\leq$ heap-size[$A$] $\wedge$ $A[r]>A[$largest$]$
    \Indent
      \State \textbf{then}
      \Indent
        \State largest $=r$
      \EndIndent
    \EndIndent
    \State \textbf{if} largest $\neq i$
    \Indent
      \State \textbf{then}
      \Indent
        \State Exchange: $A[i] \leftrightarrow A[$largest$]$
        \State HEAPIFY($A$,largest)
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\vspace{1em}
\noindent
in cui il termine \quotes{heap-size} fa riferimento alla dimensione, o altezza, del sottoalbero (ovvero al numero di nodi che costituiscono il sottoalbero).\\
La procedura esecutiva di heapify prevede dei controlli sequenziali: se esiste il figlio sinistro ($l\leq$ heap-size[$A$]) e si verifica che il valore del figlio sinistro è maggiore del nodo radice ($A[l]>A[i]$), ponendo il valore maggiore all'interno di una posizione di lavoro denominata \emph{largest}.\\
Dopodiché si deve procedere alla valutazione anche del figlio destro, se esiste ($r\leq$ heap-size[$A$]), e si deve controllare se esso ha un valore maggiore sia del padre che del figlio sinistro ($A[r]>A[largest]$): se questo è il caso, allora all'interno di \emph{largest} viene posta la posizione del figlio destro, ossia $r$.\\
Pertanto, ora, all'interno della posizione di lavoro \emph{largest} vi è la posizione del padre, del figlio sinistro o del figlio destro, a seconda di chi abbia il valore massimo.\\
Naturalmente, ora, si procede con una modifica dell'albero solamente se all'interno di \emph{largest} non vi è $i$, ossia la posizione del padre; se, infatti, vi fosse una posizione diversa da $i$ significherebbe che uno dei due figli presenta un valore maggiore del padre e, quindi, il vincolo \emph{maximum-heap} non sarebbe rispettato.\\
Quando si effettua uno scambio tra radice e figlio, ovviamente, è necessario ripetere la procedura \textbf{heapify} sul sottoalbero che presenta come sottoradice il figlio che aveva il valore maggiore, in quanto non è detto che taluno sia ancora una catasta.\\
Si consideri un sottoalbero come il seguente, in cui è possibile impiegare \textbf{heapify}, ovvero in cui la radice non è detto che domini i figli, ma il resto dell'albero rispetta il vincolo, come mostrato di seguito:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$2$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$8$};
    \node[main node] (3) [xshift=2em, below right of=1] {$5$};
    \node[main node] (4) [xshift=2em, below left of=2] {$4$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$1$};
    \node[main node] (6) [xshift=2em, below left of=3] {$2$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$2$};
    \node[main node] (8) [xshift=4em, below left of=4] {$4$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$4$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9);
  \end{tikzpicture}
  \caption{Esempio di sottoalbero al quale applicare heapify}
  \label{fig:esempio_sottoalbero_heapify}
\end{figure}

\vspace{1em}
\noindent
Supponendo che il nodo radice di tale sottoalbero abbia il numero d'ordine $i=3$, applicando l'algoritmo \textbf{heapify}, si considerano tre posizioni
\[\underset{l}{\boxed{6}} \hspace{0.5em} \underset{r}{\boxed{7}} \hspace{0.5em} \underset{\text{largest}}{\boxed{6 \text{ o } 3 \text{ o } 7}}\]
Naturalmente $l=6$ in quanto la procedura LEFT($i$) restituisce $2i$, mentre $r=7$ in quanto la procedura RIGHT($i$) restituisce $2i+1$.\\
Operando secondo la procedura heapify, se il figlio sinistro esiste e il suo valore è maggiore di quello della sua radice, allora all'interno di \emph{largest} andrà posto il valore $l=6$, altrimenti il valore $i=3$; Analogamente, se il valore del figlio di destra è maggiore di quello di sinistra e della radice, allora in \textit{largest} andrà inserito il valore $r=7$.\\
Nella posizione finale \quotes{largest}, pertanto, vi può essere la posizione del padre, del figlio sinistro o del figlio destro; se nella posizione \quotes{largest} vi è un valore più grande della radice, avviene l'\textbf{exchange}, come in questo caso, ottenendo

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$8$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$2$};
    \node[main node] (3) [xshift=2em, below right of=1] {$5$};
    \node[main node] (4) [xshift=2em, below left of=2] {$4$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$1$};
    \node[main node] (6) [xshift=2em, below left of=3] {$2$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$4$};
    \node[main node] (8) [xshift=4em, below left of=4] {$2$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$4$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9);
  \end{tikzpicture}
  \caption{Esempio di sottoalbero al quale applicare heapify dopo un primo passo}
  \label{fig:esempio_sottoalbero_heapify_primo_passo}
\end{figure}

\vspace{1em}
\noindent
Avendo operato tale sostituzione sul figlio di sinistra, è opportuno verificare il rispetto del vincolo del sottoalbero di sinistra (mentre il sottoalbero di destra non presenta anomalie, in quanto se prima rispettava il vincolo, lo rispetta anche ora).
Si applica, quindi, ancora una volta heapify considerando come sottoradice il nodo avente numero d'ordine $6$:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$8$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$4$};
    \node[main node] (3) [xshift=2em, below right of=1] {$5$};
    \node[main node] (4) [xshift=2em, below left of=2] {$2$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$1$};
    \node[main node] (6) [xshift=2em, below left of=3] {$2$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$4$};
    \node[main node] (8) [xshift=4em, below left of=4] {$2$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$4$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9);
  \end{tikzpicture}
  \caption{Esempio di sottoalbero al quale applicare heapify dopo un secondo passo}
  \label{fig:esempio_sottoalbero_heapify_secondo_passo}
\end{figure}

\vspace{1em}
\noindent
Ancora una volta, avendo scambiato la radice con uno dei figli, è necessario applicare heapify, questa volta considerando il sottoalbero avente sottoradice il nodo con numero d'ordine $12$:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$8$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$4$};
    \node[main node] (3) [xshift=2em, below right of=1] {$5$};
    \node[main node] (4) [xshift=2em, below left of=2] {$4$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$1$};
    \node[main node] (6) [xshift=2em, below left of=3] {$2$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$4$};
    \node[main node] (8) [xshift=4em, below left of=4] {$2$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$2$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9);
  \end{tikzpicture}
  \caption{Esempio di sottoalbero al quale applicare heapify dopo un secondo passo}
  \label{fig:esempio_sottoalbero_heapify_secondo_passo}
\end{figure}

\vspace{1em}
\noindent
Avendo sostituito la radice con il figlio destro, è necessario applicare heapify ancora una volta, sul sottoalbero di destra; tuttavia, in questo caso, heapify non esegue alcuna operazione, giacché il sottoalbero di destra non esiste.\\
Ecco che l'albero inziale, ora, rispetta il vincolo di \emph{maximum heap}.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che non sono presenti cicli in tale algoritmo, in quanto la complessità è data dalla ricorsività; in particolare, il numero delle iterazioni è
\[\#\text{iterazioni}\leq h_i+1\]
in cui $h_i$ è l'altezza del sottoalbero (infatti, nel caso peggiore, si deve scandagliare un albero per ogni livello, più una chiamata finale per constatare che non vi è un sottoalbero del nodo figlio); pertanto la complessità è, operando una forte maggiorazione, dell'ordine $O(\log(n))$, in cui $n$ è il numero dei nodi dell'albero intero (avendo interpretato il sottoalbero come l'albero intero); infatti è noto che la profondità di un albero completo avente $n$ nodi è circa $\log(n)$, per cui si ha che $h$ e $\log(n)$ appartengono alla stessa classe $\Theta$.\\
Si osservi che, nel caso in cui si consideri l'albero intero di partenza, con $H$ la sua altezza complessiva, allora si ha che
\[\#\text{iterazioni}\leq H+1\]
Tuttavia, gli algoritmi più importanti da dover considerare sono altri due: \textbf{heap-sort} e \textbf{build-heap}, l'ultimo del quale serve proprio per costruire una catasta partendo da un albero non catastizzato, come mostrato di seguito:

\begin{algorithm}[H]
  \caption{Build-Heap}
  \begin{algorithmic}[1]
    \State BUILD-HEAP($A$)
    \State heap-size$=length[A]$
    \State \textbf{for} $i = \left \lfloor \frac{n}{2} \right \rfloor$ \textbf{down to} $1$
    \Indent
      \State \textbf{do} HEAPIFY($A$,i)
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
In cui
\[i = \left \lfloor \frac{n}{2} \right \rfloor\]
è il primo nodo con figli (in quanto è noto che il livello delle foglie, in un albero binario super completo, contiene più nodi di tutto il resto dell'albero) e che, quindi, potrebbe non rispettare il vingolo di \emph{maximum-heap} (i nodi succcessivi, naturalmente, non possono avere di questo problema, visto che non presentano figli). Si consideri il caso seguente:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$3$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$2$};
    \node[main node] (3) [xshift=2em, below right of=1] {$4$};
    \node[main node] (4) [xshift=2em, below left of=2] {$5$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$1$};
    \node[main node] (6) [xshift=2em, below left of=3] {$6$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$2$};
    \node[main node] (8) [xshift=4em, below left of=4] {$8$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$4$};
    \node[main node] (10) [xshift=4em, below left of=5] {$3$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9)
      (5) edge node [right] {} (10);
  \end{tikzpicture}
  \caption{Esempio di albero da catastizzare}
  \label{fig:esempio_albero_catastizzare}
\end{figure}

\vspace{1em}
\noindent
Quest'ultimo è un albero binario completo composto da $10$ nodi. Pertanto il primo nodo che potrebbe avere figli è il noto $5$, mentre le foglie non hanno alcun problema.
A partire dal nodo avente numero d'ordine $5$ si procede risalendo l'albero e applicando heapify, come mostrato di seguito, scambiando il nodo $5$ con il nodo $10$:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$3$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$2$};
    \node[main node] (3) [xshift=2em, below right of=1] {$4$};
    \node[main node] (4) [xshift=2em, below left of=2] {$5$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$3$};
    \node[main node] (6) [xshift=2em, below left of=3] {$6$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$2$};
    \node[main node] (8) [xshift=4em, below left of=4] {$8$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$4$};
    \node[main node] (10) [xshift=4em, below left of=5] {$1$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9)
      (5) edge node [right] {} (10);
  \end{tikzpicture}
  \caption{Esempio di albero da catastizzare dopo un primo passo}
  \label{fig:esempio_albero_catastizzare_primo_passo}
\end{figure}

\vspace{1em}
\noindent
Risalendo progressivamente si considera il nodo precedente, ossia il $4$ e si applica heapify, scambiando il nodo $4$ con il nodo $8$:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$3$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$2$};
    \node[main node] (3) [xshift=2em, below right of=1] {$4$};
    \node[main node] (4) [xshift=2em, below left of=2] {$8$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$3$};
    \node[main node] (6) [xshift=2em, below left of=3] {$6$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$2$};
    \node[main node] (8) [xshift=4em, below left of=4] {$5$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$4$};
    \node[main node] (10) [xshift=4em, below left of=5] {$1$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9)
      (5) edge node [right] {} (10);
  \end{tikzpicture}
  \caption{Esempio di albero da catastizzare dopo un secondo passo}
  \label{fig:esempio_albero_catastizzare_secondo_passo}
\end{figure}

\vspace{1em}
\noindent
Procedendo a ritroso si deve applicare nuovamente heapify, considerando il nodo precedente, ossia il $3$ ed effettuando lo scambio tra il nodo $3$ e il nodo $6$:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$3$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$2$};
    \node[main node] (3) [xshift=2em, below right of=1] {$6$};
    \node[main node] (4) [xshift=2em, below left of=2] {$8$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$3$};
    \node[main node] (6) [xshift=2em, below left of=3] {$4$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$2$};
    \node[main node] (8) [xshift=4em, below left of=4] {$5$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$4$};
    \node[main node] (10) [xshift=4em, below left of=5] {$1$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9)
      (5) edge node [right] {} (10);
  \end{tikzpicture}
  \caption{Esempio di albero da catastizzare dopo un terzo passo}
  \label{fig:esempio_albero_catastizzare_terzo_passo}
\end{figure}

\vspace{1em}
\noindent
Considerando ancora il nodo $2$ si applica heapify e si scambia il nodo $2$ con il nodo $4$; tuttavia, heapify è ricorsivo e deve procedere operando sul sottoalbero avente sottoradice il nodo $4$, effettuando uno scambio tra il nodo $4$ e il nodo $8$:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$3$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$8$};
    \node[main node] (3) [xshift=2em, below right of=1] {$6$};
    \node[main node] (4) [xshift=2em, below left of=2] {$5$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$3$};
    \node[main node] (6) [xshift=2em, below left of=3] {$4$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$2$};
    \node[main node] (8) [xshift=4em, below left of=4] {$2$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$4$};
    \node[main node] (10) [xshift=4em, below left of=5] {$1$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9)
      (5) edge node [right] {} (10);
  \end{tikzpicture}
  \caption{Esempio di albero da catastizzare dopo un quarto passo}
  \label{fig:esempio_albero_catastizzare_quarto_passo}
\end{figure}

\vspace{1em}
\noindent
Ed infine si applica heapify sul nodo $1$, scambiandolo con il nodo $2$; ancora una volta heapify deve scendere verso il basso, facendo rispettare il vincolo a tutti i sottoalberi sottostanti, scambiando il nodo $2$ con il nodo $4$ ed il nodo $4$ con il nodo $9$:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$8$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$5$};
    \node[main node] (3) [xshift=2em, below right of=1] {$6$};
    \node[main node] (4) [xshift=2em, below left of=2] {$4$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$3$};
    \node[main node] (6) [xshift=2em, below left of=3] {$4$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$2$};
    \node[main node] (8) [xshift=4em, below left of=4] {$2$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$3$};
    \node[main node] (10) [xshift=4em, below left of=5] {$1$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9)
      (5) edge node [right] {} (10);
  \end{tikzpicture}
  \caption{Esempio di albero da catastizzare dopo un quinto passo}
  \label{fig:esempio_albero_catastizzare_quinto_passo}
\end{figure}

\vspace{1em}
\noindent
Procedendo in questo modo, si procede alla creazione di una catasta, con un numero di iterazioni che dipende dalle chiamate ricorsive della procedura heapify; in particolare, si può osservare che il numero di nodi considerati da build-sort è approssimabile a
\[\frac{n}{2}\]
in quanto l'ultima metà è certamente esclusa, essendo i nodi delle foglie. Pertanto il ciclo for dell'algortimo ha un nuumero di iterazioni pari a $\frac{n}{2}$; tuttavia, in ognuna di tali iterazioni viene richiamata la procedura heapify, la quale è ricorsiva e presenta una complessità di $O(\log(n))$; da ciò si evince che la complessità dell'algoritmo \textbf{build-heap} è pari a
\[\boxed{O \left(\frac{n}{2} \cdot \log(n)\right) \cong O(n \cdot \log(n))}\]
Inoltre è immediato capire che siccome il numero dei nodi che bisogna visitare in \textbf{build-heap} è pari a $\frac{n}{2}$, indipendemente che heapify sia operativa o mena, si capisce che la complessità di \textbf{build-sort} è almeno lineare, ovvero
\[\boxed{\Omega \left(\frac{n}{2}\right) \cong \Omega(n)}\]
Quindi la complessità di \textbf{buil-heap} è almeno \textbf{lineare} e al più \textbf{log-lineare}; tuttavia, è sufficiente osservare che i nodi che la procedura heapify considera sono prossimi alle foglie (sempre perché considera la metà dei nodi in su ed è noto che se un albero binario è super completo, in livello delle foglie contiene più nodi di tutto il resto dell'albero), quindi i sottoalberi che vengono analizzati hanno tutti una lunghezza molto ridotta, che può abbassare la limitazione superiore assunta. Infatti, si può dimostrare che la complessità di calcolo di \textbf{build-heap} è lineare
\[\boxed{\Theta(n)}\]
e questo risultato, dal punto di vista dell'ordinamento, è totalmente irrilevante, anzi deve essere così affinché la complessità generale di altri algoritmi che ne fanno uso sia quantomeno accettabile.

\vspace{1em}
\noindent
Per conoscere come opera \textbf{heap-sort} si consideri l'albero binario seguente:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$2$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$7$};
    \node[main node] (3) [xshift=2em, below right of=1] {$3$};
    \node[main node] (4) [xshift=2em, below left of=2] {$1$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$5$};
    \node[main node] (6) [xshift=2em, below left of=3] {$9$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$8$};
    \node[main node] (8) [xshift=4em, below left of=4] {$6$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$1$};
    \node[main node] (10) [xshift=4em, below left of=5] {$5$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9)
      (5) edge node [right] {} (10);
  \end{tikzpicture}
  \caption{Esempio di albero da ordinare con heap-sort}
  \label{fig:esempio_albero_ordinare_heap_sort}
\end{figure}

\noindent
\textbf{Lavorando in loco}, la prima operazione da eseguire su tale albero è quello di catastizzarlo, ottenendo una struttura completamente opposta rispetto a quella che si necesssita: sul nodo radice è presente il valore maggiore, e sull'ultima posizione il valore più piccolo, com'è evidente nel seguito:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$9$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$8$};
    \node[main node] (3) [xshift=2em, below right of=1] {$7$};
    \node[main node] (4) [xshift=2em, below left of=2] {$6$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$5$};
    \node[main node] (6) [xshift=2em, below left of=3] {$5$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$4$};
    \node[main node] (8) [xshift=4em, below left of=4] {$3$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$2$};
    \node[main node] (10) [xshift=4em, below left of=5] {$1$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9)
      (5) edge node [right] {} (10);
  \end{tikzpicture}
  \caption{Esempio di albero catastizzato}
  \label{fig:esempio_albero_catastizzato}
\end{figure}

\noindent
Allora, secondo la logica heap-sort, si procede a sostituire il primo nodo con l'ultimo (effettuando un primo ordinamento); dal momento che l'ultimo nodo è un nodo foglia, non vi è problema che rispetti il vincolo \emph{maximum-heap}, quindi si opera una cancellazione dell'ultimo nodo e dell'arco che lo congiunge con il nodo padre (heap-size[$A$] = heap-size[$A$] - 1).

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$1$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$8$};
    \node[main node] (3) [xshift=2em, below right of=1] {$7$};
    \node[main node] (4) [xshift=2em, below left of=2] {$6$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$5$};
    \node[main node] (6) [xshift=2em, below left of=3] {$5$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$4$};
    \node[main node] (8) [xshift=4em, below left of=4] {$3$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$2$};
    \node[main node] (10) [xshift=4em, below left of=5] {$9$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9);
  \end{tikzpicture}
  \caption{Esempio di albero da ordinare dopo un primo heap-sort}
  \label{fig:esempio_albero_ordinare_primo_heap_sort}
\end{figure}

\noindent
Ora si procede nuovamente a catastizzare, applicando heapify all'albero intero: da notare che è possibile applicare tale algoritmo in quanto l'albero intero è una catasta, essendolo già da prima; l'unico problema nel rispetto del vincolo, che giustifica l'applicazione di heapify, sta proprio nel nodo radice dell'albero intero:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$8$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$7$};
    \node[main node] (3) [xshift=2em, below right of=1] {$6$};
    \node[main node] (4) [xshift=2em, below left of=2] {$5$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$5$};
    \node[main node] (6) [xshift=2em, below left of=3] {$4$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$3$};
    \node[main node] (8) [xshift=4em, below left of=4] {$2$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$1$};
    \node[main node] (10) [xshift=4em, below left of=5] {$9$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8)
      (4) edge node [right] {} (9);
  \end{tikzpicture}
  \caption{Esempio di albero catastizzato dopo un primo heap-sort}
  \label{fig:esempio_albero_catastizzato_primo_heap_sort}
\end{figure}

\noindent
Ancora una volta si è ottenuta una catasta nella quale, dei valori rimasti, il maggiore si trova in prima posizione e il minore in ultima. Si procede, allora, a sostituire il primo nodo con l'ultimo, il quale viene eliminato insieme al suo collegamento con il nodo padre.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$1$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$7$};
    \node[main node] (3) [xshift=2em, below right of=1] {$6$};
    \node[main node] (4) [xshift=2em, below left of=2] {$5$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$5$};
    \node[main node] (6) [xshift=2em, below left of=3] {$4$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$3$};
    \node[main node] (8) [xshift=4em, below left of=4] {$2$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$8$};
    \node[main node] (10) [xshift=4em, below left of=5] {$9$};

    \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (4)
      (2) edge node [left] {} (5)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (7)
      (4) edge node [right] {} (8);
  \end{tikzpicture}
  \caption{Esempio di albero da ordinare dopo un secondo heap-sort}
  \label{fig:esempio_albero_ordinare_secondo_heap_sort}
\end{figure}

\vspace{1em}
\noindent
Ecco che allora il penultimo nodo si trova perfettamente ordinato con gli altri; procedendo con un'operazione di heapify sull'albero intero, si ottiene ancora una volta una catasta che rispetta il vincolo e si ricomincia il procedimento daccapo, fino ad ottenere l'array perfettamente ordinato, come mostrato di seguito:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (1) {$1$};
    \node[main node] (2) [xshift=-2em, below left of=1] {$2$};
    \node[main node] (3) [xshift=2em, below right of=1] {$3$};
    \node[main node] (4) [xshift=2em, below left of=2] {$4$};
    \node[main node] (5) [xshift=-2em, below right of=2] {$5$};
    \node[main node] (6) [xshift=2em, below left of=3] {$5$};
    \node[main node] (7) [xshift=-2em, below right of=3] {$6$};
    \node[main node] (8) [xshift=4em, below left of=4] {$7$};
    \node[main node] (9) [xshift=-4em, below right of=4] {$8$};
    \node[main node] (10) [xshift=4em, below left of=5] {$9$};
  \end{tikzpicture}
  \caption{Esempio di albero ordinato dopo $n-1$ heap-sort}
  \label{fig:esempio_albero_ordinato_heap_sort}
\end{figure}

\noindent
Di seguito si propone il listato dell'algoritmo \textbf{merge-sort}:

\begin{algorithm}[H]
  \caption{Heap-Sort}
  \begin{algorithmic}[1]
    \State HEAP-SORT($A$)
    \State BUILD-HEAP(A)
    \State \textbf{for} $i=length[A]$ \textbf{down to} $2$
    \Indent
      \State \textbf{do} Exchange: $A[1] \leftrightarrow A[i]$
      \Indent
        \State heap-size[$A$] = heap-size[$A$] - 1
        \State HEAPIFY($A,1$)
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
In questo caso il numero di iterazioni di ciclo for sono $n-1$ e sono tutte operazioni in serie. Per valutare la complessità di tale algoritmo si deve tenere presente la seguente regola:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{COMPLESSITÀ COMPLESSIVA DI UN ALGORITMO MULTI-BLOCCO}}\\
    \parbox{\linewidth}{Quando un algoritmo è costituito da diversi blocchi di istruzioni, ognuno avente una propria distinta complessità esecutiva, prevale sempre la \textbf{complessità maggiore}.\vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\noindent
In particolare, in questo caso, la complessità di build-heap è $\Theta(n)$, mentre la complesità nel secondo blocco è $O(n \cdot \log(n))$, essendo la complessità di heapify $O(\log(n))$ e dal momento che tale procedura si richiama $n-1$ volte, applicandola su tutto l'albero e non su sottoalberi.\\
Pertanto si evince che la complessità dell'algoritmo \textbf{heap-sort} è al più log-lineare (ossia nel caso peggiore), ovvero
\[\boxed{O(n \cdot \log(n))}\]
mentre la complessità nel caso migliore è lineare, ovvero $\Theta(n)$. Richiamando il \textbf{teorema sugli algoritmi di ordinamento generale}, il quale afferma che un algoritmo di ordinamento di tipo generale, come \textbf{heap-sort}, è condannato ad avere una complessità nel caso peggiore almeno log-lineare e una complessità nel caso medio almeno log-lineare: questo significa che, siccome la complessità di \textbf{heap-sort} è al più log-lineare e tale risultato conferma che è almeno log-lineare, quindi, alla fine, la complessità dell'algoritmo \textbf{heap-sort} (nel caso peggiore e nel caso medio) è proprio \textbf{log-lineare}.

\newpage
\noindent
\begin{center}
  18 Marzo 2022
\end{center}
Si osservi che le operazioni che vengono svolte dall'algoritmo \textbf{heap-sort} sono \textbf{in serie}, pertanto devono essere valutate in maniera attenta per comprender la complesità dell'algortimo stesso.\\
In particolare, è noto che, dal punto di vista della notazione $\Theta$, se vi sono due blocchi di istruzioni che presentano complessità diverse, ciò che determina la complessità finale dell'algoritmo è la complessità peggiore e più pesante fra le due date. Ciò si può dimostrare facilmente, considerando, a tal proposito, due funzioni $f(n)$ e $g(n)$, allora
\[f(n) + g(n) \hspace{0.5em} \text{e} \hspace{0.5em} f(n) \vee g(n)\]
in cui con $f(n) \vee g(n)$ è da intendersi il massimo tra $f(n)$ e $g(n)$,
appartengono alla stessa classe $\Theta$, in quanto, logicamente, si ha che
\[f(n) \vee g(n) \leq f(n) + g(n) \leq 2 \cdot \left[ f(n) \vee g(n) \right]\]
In particolare, si osserva che l'algoritmo \textbf{heap-sort} procede dapprima realizzando una catasta tramite il sotto-algoritmo \textbf{build-heap}, la cui complessità è lineare; dopo questa procedura si ottiene un albero che è ordinato al contrario (con i valori maggiori sui primi nodi e i valori minori sugli ultimi nodi); pertanto si procede a sostituire il primo nodo con l'ultimo dei rimanenti ed eliminando progressivamente l'ultimo nodo e il collegamento con i rimanenti (tramite l'istruzione heap-size = heapsize - 1), procedendo ad ordinare l'albero intero (a partire dalla radice, a differenza della procedura \emph{build-heap}, in cui \emph{heapify} veninva applicata a sottoalberi vicini alle foglie), tramite la procedura \emph{heapify}, ad ogni passaggio.\\
Pertanto la complessità dell'algoritmo \textbf{heap-sort} è \textbf{log-lineare}, che \textbf{asintoticamente} è ottima, come avvalorato dal teorema sulla complessità di un algoritmo di ordinamento generale.

\vspace{1em}
\subsection{Quick-sort}
L'algoritmo di ordinamento \textbf{quick-sort} è un algoritmo che, come merge-sort, lavora su posizioni intermedie da $1$ a $n$, impiegando gli indici $p$ e $r$, e opera in maniera ricorsiva.\\
Tuttavia, mentre in merge-sort, ad ogni chiamata, l'insieme dei valori da ordinare si dimezzava fino a considerare un solo valore, in quick-sort tale divisione esatta in due parti non si verifica, in quanto le divisioni sono irregolari.\\
Il primo sotto-algoritmo adoperato da quick-sort prende il nome \textbf{partition} (partizione), il quale considera come argomenti l'array $A$ di lunghezza $n$ e due posizioni intermedie $p$ e $r$, con il vincolo che $p<r$ (vincolo imposto da quick-sort), in cui inzialmente $p=1$ e $r=n$, come mostrato di seguito:
\[\overset{p}{\boxed{2}}\boxed{8}\boxed{7}\boxed{2}\boxed{3}\boxed{5}\boxed{6}\overset{r}{\boxed{4}}\]
Tale array verrà diviso in $4$ aree, in una delle quali (l'ultima, generalmente) è presente un \textbf{perno} (o \textbf{pivot}): nella prima area saranno contenuti valori $\leq$ del perno, nella seconda area saranno contenuti valori $>$ del perno, mentre nella terza area saranno contenuti valori che ancora con sono stati esaminati (e che quindi non è noto se siano maggiori o minori del perno): è chiaro che progressivamente le prime due aree andranno incrementandosi, mentre la terza area andrà via via riducendosi, fino a sparire con la fine dell'algoritmo. Pertanto, vi sono i confini delle due prime aree che sono mobili, chiamati $i$ e $j$: il primo, $i$, che divide la prima e la seconda area, si muove in maniera irregolare; il secondo, $j$, che aumenta di un passo alla volta in modo meccanico.\\
Si osservi l'esecuzione di tale algoritmo: posto $p=1$ e $r=n$, si ha $i=p-1$ e $j=p$, come mostrato di seguito:
\[\underset{i}{\left \vert \right.} \hspace{0.25em} \underset{j}{\overset{p}{\boxed{2}}}\boxed{8}\boxed{7}\boxed{2}\boxed{3}\boxed{5}\boxed{6}\overset{r}{\boxed{4}}\]
e si procede a verificare se il primo elemento è più piccolo o più grande dell'ultimo elemento (ossia del perno), come nel caso seguente $2 \leq 4$; se questo è il caso, allora $i$ si incrementa di $1$ e si cambia $A[i]\leftrightarrow A[j]$ e si incrementa $j$ di $1$:
\[\overset{p}{\boxed{2}} \hspace{0.25em} \underset{i}{\left \vert \right.} \hspace{0.25em} \underset{j}{\boxed{8}} \boxed{7}\boxed{2}\boxed{3}\boxed{5}\boxed{6}\overset{r}{\boxed{4}}\]
Naturalmente in tale prima fase, lo scambio $A[i]\leftrightarrow A[j]$ è totalmente inutile, in quanto $i=j=1$, ma avrà una validità significativa nel seguito.\\
Ora si confronta il $j$-esimo elemento con il pivot e si ha che $8 > 4$ e si incrementa $j$ di uno, e così anche per l'iterazione successiva, in quanto $7 > 4$, giungendo alla configurazione seguente:
\[\overset{p}{\boxed{2}} \hspace{0.25em} \underset{i}{\left \vert \right.} \hspace{0.25em} \boxed{8}\boxed{7} \underset{j}{\boxed{2}}\boxed{3}\boxed{5}\boxed{6}\overset{r}{\boxed{4}}\]
Tuttavia, quando $j=4$, si ha che $2 \leq 4$, per cui si incrementa $i$ di $1$: tuttavia, adesso l'indice $i$ punta ad una zona errata ($i=2$ e quindi $A[i]=8$), per cui si deve scambiare $A[i]\leftrightarrow A[j]$ ed incrementare $j$, ottenendo:
\[\overset{p}{\boxed{2}}\boxed{2} \hspace{0.25em} \underset{i}{\left \vert \right.} \hspace{0.25em} \boxed{7}\boxed{8} \underset{j}{\boxed{3}}\boxed{5}\boxed{6}\overset{r}{\boxed{4}}\]
Confrontando il $j$-esimo elemento, ancora una volta $3 \leq 4$, per cui si incrementa $i$ di $1$ e si deve scambiare $A[i]\leftrightarrow A[j]$ ed incrementare $j$:
\[\overset{p}{\boxed{2}}\boxed{2}\boxed{3} \hspace{0.25em} \underset{i}{\left \vert \right.} \hspace{0.25em} \boxed{8} \boxed{7} \underset{j}{\boxed{5}}\boxed{6}\overset{r}{\boxed{4}}\]
Le due ultime due iterazioni prevedono solo di incrementare $j$ essendo $A[j]$ tutti valori maggiori del perno:
\[\overset{p}{\boxed{2}}\boxed{2}\boxed{3} \hspace{0.25em} \underset{i}{\left \vert \right.} \hspace{0.25em} \boxed{8} \boxed{7} \boxed{5}\boxed{6} \hspace{0.25em} \underset{j}{\left \vert \right.} \hspace{0.25em} \overset{r}{\boxed{4}}\]
Ora da $p$ a $i$ vi sono valori più piccoli del pivot, mentre da $i+1$ a $j$ vi sono valori maggiori del perno. Ora è sufficiente scambiare l'ultima posizione con la posizione $i+1$ per porre il perno nella sua posizione corretta:
\[\overset{p}{\boxed{2}}\boxed{2}\boxed{3} \hspace{0.25em} \underset{i}{\left \vert \right.} \hspace{0.25em} \boxed{4} \boxed{7} \boxed{5}\boxed{6} \hspace{0.25em} \underset{j}{\left \vert \right.} \hspace{0.25em} \overset{r}{\boxed{8}}\]
Ecco che il \textbf{perno} è stato collocato proprio dove deve stare, all'interno dell'array: la sua posizione è definitiva e non verrà più cambiata, in quanto è preceduto da valori inferiori ed è seguito da valori maggiori: questa la chiave di funzione dell'algoritmo \textbf{quick-sort}.\\
Pertanto, ora, \emph{partition} dovrà essere applicato da $p$ ad $i$ e da $i+2$ a $r$, andando a collocare nella loro posizione esatta e definitiva tutti i perni rimamenti; ora che l'array è stato partizionato, di seguito si espone il listato dell'algoritmo \textbf{partition}:

\begin{algorithm}[H]
  \caption{Partition}
  \begin{algorithmic}[1]
    \State PARTITION($A$,$p$,$r$)
    \State $x \gets A[r]$
    \State $i=p-1$
    \State \textbf{for} $j=p$ \textbf{to} $r-1$
    \Indent
      \State \textbf{do} \textbf{if} $A[j] \leq x$
      \Indent
        \State \textbf{then} $i=i+1$
        \State Exchange: $A[i] \leftrightarrow A[j]$
      \EndIndent
    \EndIndent
    \State Exchange: $A[i+1] \leftrightarrow A[r]$
    \State \textbf{return} $i+1$
  \end{algorithmic}
\end{algorithm}

\noindent
In cui, ovviaente $i+1$ è la posizione definitiva del perno che viene restituita dal sotto-algoritmo \emph{partition}.\\
Mentre di seguito si espone il listato dell'algoritmo \textbf{quick-sort}:

\begin{algorithm}[H]
  \caption{Quick-Sort}
  \begin{algorithmic}[1]
    \State QUICK-SORT($A$,$p$,$r$)
    \State \textbf{if} $p<r$
    \Indent
      \State \textbf{then} $q=$ PARTITION($A$,$p$,$r$)
      \State QUICK-SORT($A$,$p$,$q-1$)
      \State QUICK-SORT($A$,$q+1$,$r$)
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
In cui in $q$ viene memorizzata la posizione definitiva del perno. Naturalmente ciò che permette di uscire dalla chiamata ricorsiva sono i tratti di lunghezza unitaria, per cui $p=r$.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri l'array seguente:
\[\boxed{3}\boxed{4}\boxed{1}\boxed{2}\]
Allora, nell'esecuzione dell'algoritmo \textbf{quick-sort}, si procede eseguendo dapprima la procedura \emph{partition}, con $p=1$ e $r=n$ e fissando come perno $A[r]=2$.\\
Naturalmente, nelle prime due iterazioni, essendo $3>2$ e $4>2$, \emph{partition} non opera alcun cambiamento all'array $A$, ed effettua esclusivamente l'incremento di $j$. Quando $j=3$, allora si osserva che $1 \leq 2$, per cui si deve incrementare $i$ di $1$, scambiare $A[i] \leftrightarrow A[j]$ e incrementare $j$, ottenendo:
\[\boxed{1}\boxed{4}\boxed{3}\boxed{2}\]
Arrivati all'ultima iterazione, bisogna eseguire lo scambio $A[i+1] \leftrightarrow A[r]$, ottenendo:
\[\boxed{1}\boxed{2}\boxed{3}\boxed{4}\]
ecco che il perno $2$ si trova nella sua posizione $q=2$ definitiva, dalla quale non verrà più spostato.\\
Ora bisogna applicare quick-sort da $p=1$ a $q-1=1$ e da $q+1=3$ a $r=4$; naturalmente, nel primo caso, quick-sort non effettua alcuna operazione, visto che $p=r=1$; nel secondo caso si dovrà applicare \emph{partition} con $p=3$ e $r=4$, pertanto, essendo $3 \leq 4$, \emph{partition} dovrà incrementare $i$ di $1$, eseguire lo scambio $A[i] \leftrightarrow A[j]$ (ma essendo $i=j$, tale scambio è superfluo) ed incrementare $j$ di $1$. Terminata l'unica iterazione del ciclo, \emph{partition} eseguirà lo scambio $A[i+1] \leftrightarrow A[r]$ (ma essendo $i+1=4$ e $r=4$, ancora una volta tale scambio è superfluo) per cui l'array diverrà
\[\boxed{1}\boxed{2}\boxed{3}\boxed{4}\]
e \emph{partition} terminerà, restituendo $q=4$. Ancora una volta si dovrà applicare quick-sort, dalla posizione $p=3$ e $r=3$ e $p=5$ e $r=4$; naturalmente, in nessuno di tali casi $p<r$, per cui le chiamate ricorsive cessano, così come l'algoritmo quick-sort.

\vspace{1em}
\noindent
\textbf{Osservazione}: Per comprendere il numero di iterazioni che comporta l'utilizzo di quick-sort, non è possibile ragionare come con merge-sort, in cui venivano eseguite delle operazioni di divisione netta in due parti uguali ad ogni iterazione, in quanto con quick-sort non si ha la certezza che le partizioni su cui si andrà a lavorare siano proprio delle metà esatte.\\
Si consideri, a tal proposito, il caso di un ordinamento di $n$ numeri che sono, in realtà, già ordinati:
\[\boxed{1}\boxed{2}\boxed{3}\boxed{4}\boxed{\textcolor{white}{0} \cdot \cdot \cdot \textcolor{white}{0}}\boxed{n-1} \hspace{0.25em} \vert \hspace{0.25em} \boxed{\textcolor{white}{0} n \textcolor{white}{0}}\]
Naturalmente, in questo, caso, il ciclo for della procedura \emph{partition} eseguirà un numero di iterazioni pari a $n-1$, ossia da $1$ a $r-1=n-1$: il perno si trovava in ultima posizione all'inizio del ciclo e taluna sarà la sua posizione definitiva; dopo il primo quick-sort, si dovranno applicare due quick-sort, uno da $p=1$ a $r=q-1=n-1$ e un secondo da $p=q+1=n+1$ a $r=n$: è ovvio che dei due lavorerà solamente il primo, mentre il secondo prevederà solo un controllo iniziale; tuttavia, la prima \emph{partition} prevede un ciclo for di $n-2$ iterazioni, ossia da $1$ a $r-1=n-2$, e dopoiché si dovranno eseguire due nuovi quick-sort, il primo da $p=1$ a $r=q-1=n-2$ e il secondo da $p=q+1=n$ fino a $r=n-1$; ancora una volta il primo prevede $n-3$ iterazioni, mentre il secondo un solo controllo, e così via, fino ad arrivare ad un quick-sort da $1$ iterazione e $1$ controllo.\\
Naturalmente, la complessità data dal numero di controlli che devono essere eseguiti nei secondi quick-sort è $\Theta(n)$, mentre la somma di tutte le iterazioni dovute all'esecuzione dei primi quick-sort è data da
\[n-1 + n-2 + n-3 + .... + 1 = \frac{n \cdot (n+1)}{2} \cong n^2\]
Ecco che allora si evince come la complessità di tale algoritmo, nel caso \emph{peggiore}, è, appunto \textbf{quadratica}, ovvero $\Theta(n^2)$.\\
Tuttavia, la complessità dell'algoritmo nel caso generale (o caso medio) è \textbf{log-lineare}, che quindi rende tale algoritmo \textbf{asintoticamente} ottimo. Inoltre anche la \textbf{complessità empirica} è log-lineare, in quanto l'utilizzo di tale codice regolarmente conferma come la complessità quadratica si riscontri solamente in casi particolari, quando i dati in input sono strutturati in un certo modo.

\vspace{1em}
\subsection{Randomize Quick-Sort}
Per comprendere la complessità dell'algoritmo \textbf{quick-sort}, nel caso generale, bisogna considerare una variante aleatoria di tale algoritmo, la quale prende il nome di \textbf{randomize quick-sort}.\\
La probabilità di un evento può essere definita come il grado di fiducia che il senso comune attribuisce al verificarsi dell'evento, valutato su una scala $[0,1]$: tuttavia, la casualità dell'evento riguarda non tanto l'evento in sé, ma l'osservatore che, non potendo conoscere il risultato dell'evento stesso, lo reputa casuale.\\
Il concetto di probabilità, in senso teorico, può essere suddivisa in \textbf{tre macroaree di interesse}:
\begin{itemize}
  \item probabilità combinatoria;
  \item probabilità empirica;
  \item probabilità soggettiva o neo-bayesiana.
\end{itemize}
La probabilità, infatti, è strettamente connessa al calcolo combinatorio: pertanto, la probabilità di un evento è definibile come il rapporto fra il numero di casi favorevoli e il numero di casi possibili (ossia gli eventi elementari), sempre nell'ipotesi che gli eventi elementari siano tutti equiprobabili (che è un'ipotesi decisamente condizionante per fornire significato alla formula seguente):
\[\boxed{P(E) = \frac{\#\text{casi favorevoli}}{\#\text{casi possibili}}}\]
per cui il grado di fiducia è proporzionale al numero di casi favorevoli.\\
Dal punto di vista empirico (o statistico o oggettivo, in quanto fondata sull'esperimento), invece, si ha che
\[\boxed{P(E) = \lim_{n \to +\infty} \frac{\#\text{successi}}{n}}\]
ovvero il limite metafisico quando il numero degli esperimenti diverge all'infinito del rapporto tra il numero dei successi e il numero degli esperimenti (è ovvio che sia un limite metafisico, perché affinché la mera valutazione statistica fornita da un numero finito di esperimenti si trasformi in probabilità, il numero degli esperimenti deve divenire infinito, cosa che è materialmente impraticabile). È chiaro che anche tale definizione è fortemente limitante, in quanto affinché essia sia affidabile, dovrebbe richiedere che tutti gli esperimenti che vengono eseguiti siano una copia dello stesso esperimento astratto, ossia che si ripetano sempre nelle stesse condizioni (\textbf{decidendo in maniera} soggettiva quali fattori siano rilevanti e, pertanto, debbano essere mantenuti costanti nel corso della sperimentazione): eccco, allora, che tale elemento soggettivo eguaglia quello della definizione combinatoria, nella quale si richiedeva l'equiprobabilità degli eventi elementari.\\
La probabilità soggettiva o neo-bayesiana (o definettiana) deve la sua concezione, naturalmente, al reverendo \textbf{Bayes}, mentre la sua diffusione e formulazione a \textbf{Bruno de Finetti}, il quale è riuscito ad analizzare gli \emph{odds} tramite delle fondamenta teoriche precise, definendo la probabilità come la misura di quanto l'uomo è propenso a scommettere.\\
Si possono, pertanto, interpretare i tre differenti concetti di probabilità in \textbf{modalità telescopica}:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node[draw,circle,minimum size=3cm](a) at (0,0){};
    \node[draw,circle,minimum size=6cm](b) at (0,0){};
    \node[draw,circle,minimum size=9cm](c) at (0,0){};
    \draw (0,0) node[]{Combinatoria};
    \draw (0,2.2) node[]{Empirica};
    \draw (0,3.7) node[]{Neo-Bayesiana};
  \end{tikzpicture}
  \caption{Interpretazione telescopica della probabilità}
  \label{fig:interpretazione_telescopica_probabilità}
\end{figure}

\noindent
In cui è evidente come la probabilità combinatoria è contenuta nella probabilità statistica (o empirica) che sono contenute nella probabilità bayesiana, in quanto il suo campo di applicazione è enorme, notevolmente più esteso dei precedenti.

\newpage
\noindent
\begin{center}
  23 Marzo 2022
\end{center}
La definizione di probabilità più consona per una valutazione sperimentale è di tipo frequentista: la probabilità di un evento rappresenta il limite, per il numero degli esperimenti che tende all'infinito, del rapporto tra il numero di volte in cui tale evento si è verificato e il numero degli esperimenti effettuati (sempre nell'ipotesi che tali esperimenti ripetuti si siano svolti nelle medesime condizioni):
\[\boxed{P(E) = \lim_{n \to +\infty} \frac{\#\text{successi}}{n}}\]
in cui, ovviamente, essendo il numero degli esperimenti teoricamente infinito, la frequenza di successo dell'evento aleatorio $E$ tende ad assestarsi su un valore che viene considerato la probabilità di $E$. Naturalmente, quello esposto è un limite metafisico in quando un numero di esperimenti infinito è materialmente irrealizzabile.\\
È ovvio che, quindi, la probabilità che si verifichi l'evento $E$ è
\[p(E) = \frac{\#\text{successi}}{n}\]
mentre la probabilità che non si verifichi $E$ è
\[p(\overline{E}) = \frac{n-\#\text{successi}}{n}\]
Da ciò appare evidente come
\[\boxed{P(\overline{E}) = 1 - P(E)}\]
Considerando, ora, due eventi $E$ e $F$, è nota che la probabilità dell'unione è proprio
\[\boxed{P(E \vee F) = \frac{n_E + n_F - n_{E \wedge F}}{n} = p(E) + p(F) - p(E \wedge F)}\]
Naturalmente, nel caso in cui $E$ e $F$ siano \textbf{incompatibili}, la \textbf{probabilità} diviene \textbf{additiva}, in quanto la probabilità che si verifichino entrambi i due eventi è nulla, allora
\[\boxed{P(E \vee F) = p(E) + p(F)}\]
Per quando concerne la \textbf{probabilità condizionata}, dati due eventi $E$ e $F$, la probabilità che si verifichi $E$ (\textbf{evento condizionato}) dopo che si è verificato $F$ (\textbf{evento condizionante}), cosa che può essere vera o può essere una mera supposizione, si ottiene che:
\[\boxed{P(E \vert F) = \frac{n_{E \wedge F}}{n_F} \cdot \frac{n}{n} = \frac{P(E \wedge F)}{p(F)}}\]
ovvero il rapporto del numero degli esperimenti in cui si è verificato $E$ e anche $F$ e il numero di esperimenti in cui si è verificato $F$ (altrimenti tale probabilità non sarebbe condizionata); tale risultato ha significato matematico solamente nell'ipotesi in cui l'evento condizionante abbia una probabilità strettamente positiva.\\
Nell'ipotesi in cui $F$ ed $E$ siano \textbf{indipendenti} l'uno rispetto all'altro (ovvero $F$ non condiziona $E$ e viceversa), è ovvio che
\[p(E \vert F) = P(E) \hspace{1em} \text{e} \hspace{1em} p(F \vert E) = P(F)\]
Ma ricordando la formula per il calcolo della probabilità di eventi condizionanti, si ottiene:
\[p(E \vert F) = P(E)= \frac{P(E \wedge F)}{p(F)} \hspace{1em} \text{e} \hspace{1em} p(F \vert E) = P(F) = \frac{P(E \wedge F)}{p(E)}\]
ed ecco che si è ottenuta la \textbf{relazione di indipendenza tra due eventi} $E$ e $F$:
\[\boxed{P(E \wedge F) = P(E) \cdot P(F)}\]
per cui $E$ non condiziona (o influenza) $F$ \textbf{se e solo se} $F$ non condiziona (o influenza) $F$.

\newpage
\noindent
Si consideri, ora, $\mathcal{X}$ come \textbf{variabile} (o \textbf{quantità} o \textbf{qualità}) \textbf{aleatoria}, formata da uno spazio campionario (in questo caso finito) $\{x_1,...,x_n\}$ di \textbf{eventi (o determinazioni) lementari} i quali devono essere \textbf{incompatibili} fra di loro ed \textbf{esausitivi}: effettuando un esperimento, si deve sempre verificare uno e uno solo degli eventi elementari, in quanto l'esaustività prevede tutti gli eventi elementari possibili. Ciascuno di tali eventi elementari, inoltre, presenta una probabilità $p_i$, ottenendo l'insieme delle probabilità elementari $\{p_1,...,p_n\}$.\\
Naturalmente, siccome gli eventi elementari sono incompatibili fra di loro, la probabilità di un evento composto (o aggregato) è semplicemente la somma delle probabilità degli eventi elementari che lo compongono.\\
Nell'ipotesi in cui $\mathcal{X}$ sia un \textbf{numero aleatorio}, ovvero $\mathcal{X} \in \Omega_x \subset \mathbb{R}$, allora ha senso calcolare il suo \textbf{valore medio} (o \textbf{valore atteso}), ovvero $E(\mathcal{X})$ (dall'inglese \quotes{expected value}, o speranza matematica).\\
Data una variabile aleatoria $\mathcal{X}$ formata dagli eventi elementari seguenti:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}|P|@{}|P|@{}|P|@{}}
    \hline
    \multicolumn{4}{|>{\hsize=\dimexpr4\hsize+6.2\tabcolsep\centering}P|}{\parbox{\linewidth}{\textbf{VARIABILE ALEATORIA}}}\\
    \hline
    \parbox{\linewidth}{\text{Eventi elementari}} & $x_1$ & $x_2$ & $x_3$\\
    \hline
    \parbox{\linewidth}{\text{Prob. elementari}} & $p_1$ & $p_2$ & $p_3$\\
    \hline
\end{tabularx}
\vspace{1em}

\noindent
Pertanto, se $x_1$ figura $n_1$ volte, $x_2$ figura $n_2$ volte e $x_3$ figura $n_3$ volte, si ha che il valore medio di $\mathcal{X}$ è
\[E(\mathcal{X}) = \frac{n_1 \cdot x_1 + n_2 \cdot x_2 + n_3 \cdot x_3}{n}\]
Allora è ovvio che
\[\boxed{E(\mathcal{X}) = \sum_{i}^n p_i \cdot x_i}\]
che risulta esssere una definizione perfettamente consona della speranza matematica.

\vspace{1em}
\subsection{Teorema del trasferimento}
Si consideri un numero aleatorio $\mathcal{Y}$ (che deve essere un numero aleatorio, altrimenti non ha senso parlare della sua speranza matematica) che è funzione deterministica di una quantità (o qualità) aleatoria $\mathcal{X}$ (la quale non deve essere necessariamente un numero aleatorio), ovvero
\[\boxed{\mathcal{Y} = f(\mathcal{X})}\]
tale per cui $f(a)=f(b)=f(d)=0$, mentre $f(c)=1$, in cui $a,b,c,d$ sono valori di $\mathcal{X}$, aventi probabilità $p_a,p_b,p_c,p_d$. Naturalmente $\mathcal{Y}$ è un \textbf{numero aleatorio di tipo binario}, di cui si vuole calcolare la speranza matematica; non impiegando il teorema del trasferimento, si deve procedere al \textbf{calcolo delle distribuzioni}, determinando le probabilità dei valori di $\mathcal{Y}$, cosa che è resa possibile in questo caso in quanto si opera su un insieme finito di valori, mentre dovrebbe essere eseguito mediante un espediente differente se si operasse sul continuo.\\
Allora appare evidente come
\[\boxed{E(\mathcal{Y}) = \sum_{y} q_y \cdot y}\]
in cui, per quello che si è supposto, si ha $q_0=p_a+p_b+p_d$ e $q_1=p_c$, ottenendo
\[q_0 \cdot 0 + q_1 \cdot 1 = \left(p_a+p_b+p_d\right) \cdot 0 + p_c \cdot 1 = p_a \cdot f(a) + p_b \cdot f(b) + p_d \cdot f(d) + p_c \cdot f(c)\]
che corrisponde esattamente a
\[\boxed{E(\mathcal{Y}) = \sum_{x} p_x \cdot f(x)}\]
che riscritta in modo alternativo, permette di ottenere la formula del \textbf{teorema del trasferimento}:
\[\boxed{E \left[f(\mathcal{X}) \right] = \sum_{x} p_x \cdot f(x)}\]
in cui non si richiede che $\mathcal{X}$ sia di natura numerica, ma che soprattutto permette di conoscere $E \left[f(\mathcal{X}) \right]$ senza dover procedere al calcolo delle distribuzioni di $f(\mathcal{X})$, ma lavorando unicamente con gli elementi di $\mathcal{X}$.\\
Applicando il teorema del trasferimento a due variabili aleatorie $\mathcal{X}$ e $\mathcal{Y}$, si proceda al calcolo del valore atteso di $\mathcal{Z} = f(\mathcal{X},\mathcal{Y}) = \mathcal{X} + \mathcal{Y}$, in cui $\mathcal{Z}$ è una funzione deterministica della coppia $\mathcal{X}$ e $\mathcal{Y}$. Si ottiene, quindi:
\[E \left[\mathcal{Z}\right] = E \left[\mathcal{X} + \mathcal{Y}\right] = \sum_x \sum_y p_{xy} \cdot f(x,y)\]
Ma siccome $f(\mathcal{X},\mathcal{Y}) = \mathcal{X} + \mathcal{Y}$ per ipotesi, si ha che
\[\sum_x \sum_y p_{xy} \cdot f(x,y) = \sum_x \sum_y p_{xy} \cdot (x + y)\]
ma sfruttando la proprietà distributiva e commutativa (per cui l'ordine delle sommatorie può essere invertito) si ottiene
\[E \left[\mathcal{X} + \mathcal{Y}\right] = \sum_x \sum_y p_{xy} \cdot (x + y) = \sum_x \sum_y p_{xy} \cdot x + \sum_y \sum_x p_{xy} \cdot y\]
Ma siccome il primo termine non dipende dalla sommatoria in $y$ e il secondo termine non dipende dalla sommatoria in $x$, si può scrivere:
\[\sum_x \sum_y p_{xy} \cdot x + \sum_y \sum_x p_{xy} \cdot y = \sum_x x \sum_y p_{xy} + \sum_y y \sum_x p_{xy}\]
Ma siccome
\[\sum_y p_{xy}\]
è la somma delle probabilità in cui $x$ è fisso, mentre $y$ è arbitrario, allora taluna è la probabilità che $\mathcal{X}$ valga $x$ e $\mathcal{Y}$ sia arbitrario, ovvero la \textbf{probabilità marginale} di avere $x$, ossia $p_x$ (in quanto il valore di $y$ è totalmente ininfluente). Analogamente
\[\sum_x p_{xy}\]
è la somma delle probabilità in cui $y$ è fisso, mentre $x$ è arbitrario, allora taluna è la probabilità che $\mathcal{Y}$ valga $y$ e $\mathcal{X}$ sia arbitrario, ovvero la \textbf{probabilità marginale} di avere $y$, ossia $p_y$ (in quanto il valore di $x$ è totalmente ininfluente). Pertanto la sommatoria di cui sopra corrisponde esattamente a
\[\sum_x x \sum_y p_{xy} + \sum_y y \sum_x p_{xy} = \sum_x x \cdot p_x + \sum_y y \cdot p_y = E(\mathcal{X}) + E(\mathcal{Y})\]
che permette di concludere che la \textbf{speranza matematica è additiva}: un risultato fondamentale, che è reso ancora più significativo in quanto valido in qualsiasi ipotesi di interdipendenza tra $\mathcal{X}$ e $\mathcal{Y}$.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri un \textbf{indicatore di successo} $I \in \{0,1\}$, ossia un \textbf{numero aleatorio binario}, in cui $p_0=1-p$ e $p_1=p$ (per la formula del calcolo della probabilità dell'evento complementare). Allora è evidente come
\[E(I) = p_0 \cdot 0 + p_1 \cdot 1 = (1-p) \cdot 0 + p \cdot 1 = p\]
Nell'ipotesi in cui $\mathcal{Z}$ sia una funzione deterministica di un numero $n$ di indicatori di successo, allora il numero di successi è dato da:
\[\mathcal{Z} = I_1+I_2+...+I_n\]
Anche se le modalità sperimentali garantiscono l'indipendenza tra gli indicatori di successo, in realtà non ve ne è bisogno, in quanto la speranza matematica è data dalla somma della speranza matematica di ciascun indicatore di successo, ed avendo visto che $E(I) = p_0 \cdot 0 + p_1 \cdot 1 = (1-p) \cdot 0 + p \cdot 1 = p$ e che il numero di indicatori è $n$, è facile concludere che
\[\boxed{E(\mathcal{Z}) = n \cdot p}\]
Allora, senza impiegare il teorema del trasferimento, si può descrivere $\mathcal{Z}$ tramite il calcolo delle distribuzioni, andando a determinare il numero di valori che $\mathcal{Z}$ può assumere ($0$ successi, $1$ successo, ..., $n-1$ successi, $n$ successi), come mostrato di seguito:
\[\mathcal{Z}=\sum_{i=0}^n \text{Prob}\left\{\mathcal{Z}=i\right\} \cdot i = \sum_{i} \binom{n}{i} p^i \cdot (1-p)^{n-i} \cdot i = n \cdot p\]
Ovvero si è ottenuto lo stesso risultato ottenuto in precedenza, solamente che il processo escutivo si basato sul calcolo della probabilità che $\mathcal{Z}$ assuma il valore $i$-esimo, tenuto conto che $\mathcal{Z}$ è una funzione deterministica di $I_1,I_2,...,I_n$, ossia di $n$ indicatori di successo: è, in altre parole, un oggetto $n$-dimensionale.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si consideri una stringa binaria di lunghezza $n=5$ e di peso $i=3$, come mostrato di seguito
\[\boxed{0}\boxed{1}\boxed{1}\boxed{0}\boxed{1}\]
e se ne consideri una seconda
\[\boxed{1}\boxed{1}\boxed{0}\boxed{1}\boxed{0}\]
che naturalmente è ottenuta permutando la stringa binaria precedente. Tali stringhe binarie sono da reputarsi come la sequenza degli esiti di $n$ esperimenti binari, il cui risultato può essere o $0$ o $1$. Allora, in ciascuno di tali casi, la probabilità che si presenti una qualisasi stringa di peso $i=3$ e lunghezza $n=5$ è pari
\[p^i \cdot (1-p)^{n-i}\]
essendo gli esperimenti indipendenti e la probabilità moltiplicativa. Tale risultato è stato ottenuto in quanto si è assunto che la probabilità che, in ogni esperimento, figuri $1$ è $p$, mentre la probabilità che figuri $0$ è $1-p$, per cui la probabilità di ottenere la stringa binaria richiesta è $p$ elevato al numero dei successi $i$, moltiplicato per $1-p$, elevato al numero degli insuccessi $n-p$.\\
Da ciò si evince che la probabilità di ottenere $i=3$ successi, con qualunque stringa come quelle considerate, è data dalla somma delle probabilità di ottenere ciascuna stringa, essendo esse incompatibili (il risultato dei $5$ esperimenti o è uno o è l'altro): pertanto, la probabilità che $\mathcal{Z}=i$ è proprio il prodotto tra il numero di stringhe aventi peso $i=3$ e lunghezza $n=5$ e la probabilità $p^i \cdot (1-p)^{n-i}$, ossia
\[P \{\mathcal{Z}=i\} = \#\text{stringhe aventi peso i=3 e lunghezza n=3} \cdot p^i \cdot (1-p)^{n-i}\]
in cui $\#\text{stringhe aventi peso i=3 e lunghezza n=3}$ rappresenta il numero di stringhe di lunghezza $n=5$ avente peso $i=5$ che è, ovviamente:
\[\#\text{successi} = \binom{n}{i} = \frac{n!}{i! \cdot (n-i)!}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: È noto che la speranza matematica binomiale che si verifichi la variabile aleatoria $\mathcal{Z}$ è $n$ volte la probabilità di successo, ovvero
\[E(\mathcal{Z}) = np = \mu_\mathcal{Z}\]
Tuttavia, per valutare propriamente un numero aleatorio, oltre alla speranza matematico necessario conoscere anche la \textbf{dispersione attorno al valore atteso} della variabile $\mathcal{X}$, ossia la \textbf{varianza}, che si potrebbe calcolare come
\[\text{var}(\mathcal{X}) = E \left[\left \vert \mathcal{X}-\mu \right \vert \right]\]
essendo la risposta più ovvia a tale problema. Tuttavia, volendo eliminare il valore assoluto si può procedere valutando il quadrato della differenza tra $\mathcal{X}$ e la sua speranza matematica, ossia:
\[\boxed{\text{var}(\mathcal{X}) = E \left[(\mathcal{X}-\mu)^2\right]}\]
per cui la varianza della variabile aleatoria $\mathcal{X}$ si definisce come \textbf{il valore atteso del quadrato della differenza tra $\boldsymbol{\mathcal{X}}$ e $\mu$}. Al fine di ottenere, tuttavia, una valutazione lineare e non quadratica, molto spesso al posto della varianza si consiedera lo \textbf{scarto quadratico medio}
\[\text{sqm}(\mathcal{X}) = \sqrt{\text{var}(\mathcal{X})}\]
Naturalmente, però, dal teorema del trasferimento, è noto che la varianza si può scrivere come
\[\text{var}(\mathcal{X}) = \sum_{x} \left(x - \mu \right)^2 \cdot p_x\]
essendo $f(\mathcal{X})=(\mathcal{X}-\mu)^2$ una funzione deterministica di $\mathcal{X}$; sviluppando il quadrato descritto, tale risultato si può scrivere come
\[\sum_{x} x^2 \cdot p_x - 2 \mu \cdot \sum x \cdot p_x + \mu^2 \cdot \sum p_x\]
ma siccome la somma di tutte le probabilità elementari è
\[\sum p_x=1\]
e la speranza $\mu$ si può ravvisare nel calcolo intermedio
\[\sum x p_x = \mu\]
si ottiene che
\[\sum_{x} x^2 \cdot p_x - 2 \mu \cdot \sum x \cdot p_x + \mu^2 \cdot \sum p_x = \sum_{x} x^2 \cdot p_x - \mu^2\]
Invocando ancora una volta il teorema del trasferimento, essendo $f(\mathcal{X})=\mathcal{X}^2$ una funzione deterministica di $\mathcal{X}$, si ottiene che:
\[\boxed{\text{var}(\mathcal{X}) = E\left[\mathcal{X}^2 \right] - \left[E(\mathcal{X})\right]^2}\]
Volendo calcolare la varianza della somma di due variabili aleatorie, si può procedere come segue:
\[\text{var}(\mathcal{X}+\mathcal{Y})=E\left[(\mathcal{X}+\mathcal{Y})^2 \right] - \left[E(\mathcal{X} + \mathcal{Y})\right]^2\]
tuttavia, in tale caso, è fondamentale considerare l'ipotesi di indipendenza seguente $p_{xy}=p_x \cdot q_y$, da cui si evince che la \textbf{varianza è additiva}:
\[\boxed{\text{var}\left[\mathcal{X} + \mathcal{Y}\right] = \text{var}\left(\mathcal{X}\right) + \text{var}\left(\mathcal{Y}\right)}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che, in questo caso, la varianza non è, in generale, additiva, come la speranza matematica: la varianza è additiva solamente nell'ipotesi idi indipendenza tra $\mathcal{X}$ e $\mathcal{Y}$.\\
L'indipendenza è una condizione sufficiente, ma non necessaria, per l'additività: ci sono situazioni che prendono il nome di \textbf{non correlazione}, in cui vi è additività anche senza indipendenza.

\vspace{1em}
\noindent
\textbf{Esempio}: Considerando, ora, un indicatore di successo $I$, volendo calcolarne la varianza, applicando le formule note, si perviene al risultato seguente:
\[\text{var}(I) = E[I^2]-E[I]^2 = E[I^2]-p^2\]
siccome è noto che $E[I]=(1-p) \cdot 0 + p \cdot 1=p$. Tuttavia, siccome $I^2=I$, in quanto $0^2=0$ e $1^2=1$, allora si ha che $I^2 \in \{0,1\}$, in cui, ancora una volta $p_0=1-p$ e $p_1=p$. Si ottiene, quindi, che
\[\text{var}(I) = E[I^2] - p^2 = E[I] - p^2 = p - p^2 = p(1 - p) = p \cdot q\]
essendo $q=1-p$ la probabilità di insuccesso. Pertanto, se si considera la variabile aleatoria $\mathcal{Z}$, la quale prevede di eseguire $n$ esperimenti a ciascuno dei quali si associa un indicatore di successo $I_i$ indipendente rispetto agli altri, allora la \textbf{varianza binomiale} è additiva (in quanto vi è indipendenza), per cui è la somma delle $n$ varianze degli indicatori di successo (pari a $p \cdot q$), per cui si ottiene
\[\boxed{\text{var}(\mathcal{Z})=n \cdot p \cdot q}\]
mentre il suo \textbf{valore atteso} è
\[\boxed{E(\mathcal{Z})=n \cdot p}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si consideri una variabile aleatoria $\mathcal{X}$ e uno spazio campionario $A$ tale che $\vert A \vert = k$, in cui la probabilità è uniforme, ovvero ciascuno degli eventi elementari presenta probabilità
\[P(E) = \frac{1}{\vert A \vert} = \frac{1}{k}, \hspace{1em} \forall E \in A\]
Considerando un sottoinsieme $B \subset A$, tale che $\vert B \vert = h$, volendo calcolare la probabilità condizionata
\[P(X \vert B) = \text{Prob}\{\mathcal{X}=x \vert \mathcal{X} \in B\}\]
che ha significato, naturalmente, solamente quando $x \in B$. Alla luce di cio si ha che
\[P(\mathcal{X}\vert B) = \frac{P(x \cap B)}{P(B)}\]
ma è chiaro che se $x \in B$, allora $P(x \cap B) = P(x)$, per cui si ottiene che
\[P(\mathcal{X}\vert B) = \frac{P(x \cap B)}{P(B)} = \frac{P(x)}{P(B)} = \dfrac{\dfrac{1}{k}}{h \cdot \dfrac{1}{k}} = \frac{1}{h}\]
in quanto la probabilità degli eventi in $B$ si mantiene uniforme: tutti gli eventi di $B$ hanno la stessa probabilità, uno rispetto all'altro.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si consideri una variante aleatoria dell'algoritmo quick-sort, nella quale si deve procedere alla definizione della procedura RANDOM($p$,$r$), in cui si deve imporre che $p \leq r$, essendo essi due numeri interi, procedura capace di generare dei numeri che, tuttavia, saranno solamente pseudocasuali; naturalmente i numeri generati da $p$ a $r$ (estremi inclusi) sono $r-p+1$, in cui ciascuno di tali valori presenta la probabilità
\[P(\text{num})=\frac{1}{r-p+1}\]
di essere generato ed estratto, nell'ipotesi in cui la \textbf{probabilità sia uniforme}, ossia identica per ciascun valore che significa che si ha totale casualitò.

\newpage
\noindent
\begin{center}
  24 Marzo 2022
\end{center}
La serie armonica è così definita
\[\sum_{k=1}^\infty \frac{1}{k}\]
Tale serie, naturalmente, essendo a \textbf{termini positivi}, può divergere o convergere: è chiaro che una condizione necessaria affinché tale serie converga è che il termine generico della serie sia infinitesimo, ma non è una condizione sufficiente. Si consideri la \textbf{somma parziale} seguente:
\[\sum_{k=1}^{18}\frac{1}{k}=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}+\frac{1}{7}+\frac{1}{8}+\frac{1}{9}+\frac{1}{10}+\frac{1}{11}+\frac{1}{12}+\frac{1}{13}+\frac{1}{14}+\frac{1}{15}+\frac{1}{16}+\frac{1}{17}+\frac{1}{18}\]
Volendo ottenere una limitazione inferiore di tale somma, si può procedere al raggruppamento seguente
\[1+\underbrace{\frac{1}{2}}_{\frac{1}{2}}+\underbrace{\frac{1}{3}+\frac{1}{4}}_{> 2 \cdot \frac{1}{4}}+\underbrace{\frac{1}{5}+\frac{1}{6}+\frac{1}{7}+\frac{1}{8}}_{> 4 \cdot \frac{1}{8}}+\underbrace{\frac{1}{9}+\frac{1}{10}+\frac{1}{11}+\frac{1}{12}+\frac{1}{13}+\frac{1}{14}+\frac{1}{15}+\frac{1}{16}}_{> 8 \cdot \frac{1}{16}}+\frac{1}{17}+\frac{1}{18}\]
in cui sono state via via ottenute delle minorazioni sostituendo all'interno di ogni ragguppamento ogni frazione con il reciproco di una opportuna potenza di due, come mostrato di seguito:
\[\frac{1}{3}+\frac{1}{4} > \frac{1}{4}+\frac{1}{4} = 2^1 \cdot \frac{1}{4}\]
Il numero di addendi da sommare è dato proprio da potenze crescenti di $2$. Il tratto interrotto e non raggruppato viene eliminato, dal momento che si sta cercando una limitazione inferiore, che è facile capire essere la seguente:
\[\sum_{k=1}^{18}\frac{1}{k} > 1 + \frac{1}{2} \cdot \left \lfloor \log_2(n) \right \rfloor\]
In cui è ovvio che il numero di $\frac{1}{2}$ che sono stati sommati è proprio pari alla parte intera inferiore del logaritmo in base $2$ del numero totale $n$ di rapporti sommati: nel caso seguente si ha
\[\left \lfloor \log_2(18) \right \rfloor = 4\]
in cui si ha disuguaglianza stetta a meno che $n \leq 2$. Ciò è sufficiente a dimostrare che la serie armonica diverge (nonostante il termine generico sia infinitesimo), dal momento che la limitazione inferiore contiente $\log_2(n)$ che in infinito, seppur sottoreale, per $n \to +\infty$.\\
Per voler ottenere una limitazione superiore, ora, si esegue il seguente raggruppamento:
\[1+\underbrace{\frac{1}{2}+\frac{1}{3}}_{< 2 \cdot \frac{1}{2}}+\underbrace{\frac{1}{4}+\frac{1}{5}+\frac{1}{6}+\frac{1}{7}}_{< 4 \cdot \frac{1}{4}}+\underbrace{\frac{1}{8}+\frac{1}{9}+\frac{1}{10}+\frac{1}{11}+\frac{1}{12}+\frac{1}{13}+\frac{1}{14}+\frac{1}{15}}_{< 8 \cdot \frac{1}{8}}+\underbrace{\frac{1}{16}+\frac{1}{17}+\frac{1}{18}}_{< 16 \cdot \frac{1}{16}}\]
in cui sono state via via ottenute delle maggiorazioni sostituendo all'interno di ogni ragguppamento ogni frazione con il reciproco di una opportuna potenza di due, come mostrato di seguito:
\[\frac{1}{2}+\frac{1}{3} < \frac{1}{2}+\frac{1}{2} = 2^1 \cdot \frac{1}{2} = 1\]
Ancora una volta, il numero di addendi da sommare è dato proprio da potenze crescenti di $2$. Il tratto interrotto e viene maggiorato fino al reciproco della potenza di due successiva, dal momento che si sta cercando una limitazione superiore, che è facile capire essere la seguente:
\[\sum_{k=1}^{18}\frac{1}{k} < 1 + \left \lfloor \log_2(n) \right \rfloor\]
Analogamente a quanto visto in precedenza, il numero di $1$ che sono stati sommati è proprio pari alla parte intera inferiore del logaritmo in base $2$ numero totale $n$ di rapporti sommati: nel caso seguente si ha
\[\left \lfloor \log_2(18) \right \rfloor = 4\]
in cui si ha disuguaglianza stretta a meno che $n=1$. Pertanto la serie armonica risulta così limitata:
\[1 + \frac{1}{2} \cdot \left \lfloor \log_2(n) \right \rfloor < \sum_{k=1}^n \frac{1}{k} < 1 + \left \lfloor \log_2(n) \right \rfloor\]
In cui le disuaguaglianze sono strette am meno che, nel primo caso $n = 1 \vee n = 2$ e nel secondo caso $n = 1$. Volendo allora migliorare le limitazioni inferiori e superiori, si può scrivere:
\[1 + \frac{1}{2} \cdot \left \lfloor \log_2(n) \right \rfloor > 1 + \frac{1}{2} \cdot \left(\log_2(n) - 1\right) \geq \frac{1}{2} \cdot \log_2{n}\]
inoltre si ha che
\[1 + \left \lfloor \log_2(n) \right \rfloor \leq 1 + \log_2(n) \leq \log_2(n) + \log_2(n) = 2 \cdot \log_2(n)\]
ed ecco quindi che si è ottenuta una nuova limitazione:
\[\frac{1}{2} \cdot \log_2(n) \leq \sum_{k=1}^n \frac{1}{k} \leq 2 \cdot \log_2(n)\]
che permette di concludere che
\[\sum_{k=1}^n \frac{1}{k} = \Theta(\log_2(n))\]
anche se la base del logaritmo è totalmente irrilevante per la notazione $\Theta$.

\vspace{1em}
\noindent
\textbf{Osservazione}: La procedura \textbf{partition} veniva richiamata ricorsivamente dall'algoritmo \textbf{quick-sort} e permetteva di collocare progressivamente ogni pivot nella loro posizione definitiva (in quanto seguiti da valori maggiori e preceduti da valori minori); tuttavia, nel caso peggiore, è già noto che la complessità di quick-sort è quadratica. All'interno di partition vi è un ciclo for che presenta un numero di iterazioni pari a $r-1-p+1=r-p$; tuttavia, partition viene richiamato molteplici volte all'interno di quick-sort, per cui al fine di determinare la complessità dell'algoritmo è necessario sommare tutte le iterazioni che vengono effettivamente eseguite.\\
Si può osservare, però, che \textbf{il numero di iterazioni che viene eseguito da partition è pari al $\boldsymbol{\#}$confronti con il \textbf{pivot}}, in quanto in ogni iterazione vi è, per ogni valore, uno e un solo confronto con esso: \textbf{contare il numero totale delle iterazioni for all'interno di quick-sort equivale a contare il numero totale di confronti con il pivot che vengono effettuati nel corso di tutto l'algoritmo nell'uso di ogni partition}.\\
Oltre a ciò, tuttavia, è necessario considerare un'ulteriore componente di complessità; a tal proposito, si osservi il listato dell'algoritmo quick-sort:

\begin{algorithm}[H]
  \caption{Quick-Sort}
  \begin{algorithmic}[1]
    \State QUICK-SORT($A$,$p$,$r$)
    \State \textbf{if} $p<r$
    \Indent
      \State \textbf{then} $q=$ PARTITION($A$,$p$,$r$)
      \State QUICK-SORT($A$,$p$,$q-1$)
      \State QUICK-SORT($A$,$q+1$,$r$)
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
Come si vede, naturalmente, oltre ai confronti col pivot all'interno di partitition, si devono considerare anche i confronti tra $p$ e $r$ che \quotes{vanno a vuoto}, ossia quando $p=r$ e quando $p=r+1$; il primo caso si ha quando il perno viene collocato in penultima posizione al termine di partition e quindi si applica nuovamente partition tra $p=r$ e $r=r$, mentre il secondo caso si ha quando il perno viene collocato in ultima posizione (in quanto era il maggiore tra tutti i valori fin dall'inizio) e quindi applicando nuovamente partition si ha che $p=r+1$ e $r=r$. In ambedue i casi si deve fare uno sforzo $\Theta(1)$ per avere la certezza di aver finito la procedura: il numero di confronti negativi che si verificano quando $p=r$ può essere al massimo $\leq n$, mentre nel secondo caso il numero dei confronti negativi che si possono verificare quando $p=q+1$ è al più $\leq n+1$, per cui il numero di negativi che si devono valutare è sicuramente al più $\leq 2n+1$ (ossia è \textbf{linearmente controllato}). Pertanto, si ha che il numero di iterazioni di quick-sort (ossia la sua complessità) è dato da
\[\#\text{confronti col pivot} + \#\text{confronti negativi}\]
ma è noto che, dal punto di vista della notazione $\Theta$, tra le due quantità prevale la peggiore, che è naturalmente la prima, in quanto è ovvio che anche solo per scrivere i dati lo sforzo è almeno lineare, per cui il numero di iterazioni di quick-sort è valutabile semplicemente tramite il conteggio del numero di confronti con il pivot.\\
È noto che nel caso peggiore (come quando i numeri sono già ordinati), quick-sort presenta una complessità quadratica, mentre negli altri casi è importante valutare la scelta della posizione di collocamento del pivot: a tal proposito si deve introdurre una variante aleatoria della procedura partition, che prende il nome di \textbf{Randomized-Partition}, nella quale il perno viene scelto arbitrariamente tra uno dei valori compresi tra $p$ e $r$ (estremi inclusi):

\begin{algorithm}[H]
  \caption{Randomized-Partition}
  \begin{algorithmic}[1]
    \State RANDOMIZED-PARTITION($A$,$p$,$r$)
    \State $i=$ RANDOM($p$,$r$)
    \State $A[i] \leftrightarrow A[r]$
    \State PARTITION($p$,$r$)
  \end{algorithmic}
\end{algorithm}

\noindent
In cui, naturalmente, la probabilità di estrarre un numero tra $p$ e $r$ è casuale (e quindi uniforme)
\[P(\text{num}) = \frac{1}{r-p+1}\]
Naturalmente l'istruzione $A[i] \leftrightarrow A[r]$ garantisce che il perno si trovi in una posizione perfettamente casuale, che quindi slega l'algoritmo dal vincolo di porre il pivot nell'ultima posizione.\\
La variante aleatoria di quick-sort è presto conclusa:

\begin{algorithm}[H]
  \caption{Randomized-Quick-Sort}
  \begin{algorithmic}[1]
    \State RANDOMIZED-QUICK-SORT($A$,$p$,$r$)
    \State \textbf{if} $p<r$
    \Indent
      \State \textbf{then} $q=$ RANDOMIZED-PARTITION($A$,$p$,$r$)
      \State RANDOMIZED-QUICK-SORT($A$,$p$,$q-1$)
      \State RANDOMIZED-QUICK-SORT($A$,$q+1$,$r$)
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
Ecco che allora, in questo caso, il numero di confronti con il pivot non è determinabile a priori, ma costituisce una \textbf{variabile aleatoria} $\mathcal{X}$, di cui si può formulare una previsione del suo valore, determinandone la speranza matematica.\\
Per effettuare tale valutazione, per semplicità si supponga, senza perdita di generalità, che gli $n$ numeri da ordinare siano tutti diversi fra di loro tale che, una volta operato l'ordinamento, si ottiene
\[z_1 < z_2 < z_3 < ... < z_n\]
Alla luce di ciò si considerino due valori arbitrari $z_i$ e $z_j$, con $i \neq j$: è ovvio che è possibile che tra di essi non avvenga alcun confronto (come nel caso in cui $z_i \leq \text{ perno } \leq z_j$), oppure che vengano confrontati una e una sola volta, in quanto se tale confronto avviene, ciò signfica che uno dei due è un perno e dopo che il perno è stato collocato nella sua posizione definitiva, esso non viene più sottoposto ad alcuna operazione di confronto.\\
Allora si ha che
\[\boxed{\mathcal{X}=\sum_{i=1}^{n-1} \sum_{j=i+1}^n \mathcal{X}_{i,j}}\]
in cui si è supposto $i<j$, mentre $\mathcal{X}_{i,j}$ è un \textbf{numero aleatorio} che quantifica la probabilità che $z_i$ e $z_j$ vengono confrontati nel corso dell'algoritmo: in altre parole $\mathcal{X}_{i,j}$ rappresenta un indicatore di successo (o un \textbf{bit aleatorio}, che assume valore $1$ quando avviene il confronto, $0$ quando non avviene), che valuta il confronto di due termini $z_i$ e $z_j$.\\
Volendo conoscere la speranza matematica di $\mathcal{X}$, si può sfruttare il fatto che la speranza matematica sia additiva: la speranza matematica di un numero finito di addendi è la somma delle speranze matematiche; ciò permette di concludere che
\[\boxed{E(\mathcal{X}) = E \left(\sum_{i=1}^{n-1} \sum_{j=i+1}^n \mathcal{X}_{i,j}\right) = \sum_{i=1}^{n-1} \sum_{j=i+1}^n E( \mathcal{X}_{i,j})}\]
pertanto è sufficiente calcolare la speranza matematica di $\mathcal{X}_{i,j}$ ed effettuarne la somma. Inoltre, è ovvio capire che la speranza matematica del bit aleatorio $\mathcal{X}_{i,j}$ è proprio la probabilità che $z_i$ e $z_j$ vengano confrontati, quindi
\[E(\mathcal{X}_{i,j}) = 0 \cdot \text{Prob}(\text{insuccesso}) + 1 \cdot \text{Prob}(\text{successo}) = \text{Prob}\{z_i \text{ cfr } z_j\}\]
Nell'ipotesi in cui né $z_i$ né $z_j$ siano un perno e che il perno scelto sia $z_i \leq \text{ perno } \leq z_j$, allora $z_i$ e $z_j$ non verranno mai confrontati fra di loro, naturalmente, in quanto il perno, alla fine della procedura partition, si posizionerà tra $z_i$ e $z_j$ e partition si applicherà nei due tratti distinti. Se ne deduce che l'unica possibilità di un confronto tra $z_i$ e $z_j$ è che o l'uno o l'altro siano il primo pivot scelto nell'intervallo $[z_i,z_j]$; la probabilità che ne deriva è data dalla somma di due probabilità, come mostrato di seguito:
\[\text{Prob}\{z_i \text{ cfr } z_j\}=\text{Prob}\{z_i \text{ o } z_j \text{ è il primo perno estratto da } Z_{i,j}\}\]
in cui $Z_{i,j}$ è l'insieme di tutti i numeri compresi tra $z_i$ e $z_j$ (estremi inclusi): ma ciò fa capire come taluna sia una \textbf{probabilità condizionata} dal fatto che il valore estratto appartenga all'insieme $Z_{i,j}$, ovvero:
\[\text{Prob}\{z_i \text{ cfr } z_j \hspace{0.25em} \vert \hspace{0.25em} Z_{i,j}\}\]
Ricordando che la probabilità uniforme su insieme di valori si mantiene uniforme quando si condiziona su un sottoinsieme dell'insieme di partenza (come mostrato in precedenza sull'insieme $B$ dell'esempio precedente). Ecco che allora la probabilità cercata è
\[\boxed{E(\mathcal{X}_{i,j})=\text{Prob}\{z_i \text{ cfr } z_j\}=\frac{2}{j-i+1}}\]
in quando due sono gli eventi da sommare (ossia $z_i$ e $z_j$, che hanno probabilità uniforme) e $\vert Z_{i,j} \vert = j-i+1$. Pertanto si ottiene che
\[E \left[\mathcal{X}\right] = \sum_{i=1}^{n-1} \sum_{j=i+1}^n \frac{2}{j-i+1}\]
Volendo semplificare l'espressione, si opera una sostituzione con $k=j-1$, sicché se $j$ partiva da $i+1$ e arrivava a $n$, $k$ parte da $1$ e giunge fino a $n-i$, allora si può ottenere:
\[E \left[\mathcal{X}\right] = \sum_{i=1}^{n-1} \sum_{k=1}^{n-i} \frac{2}{k+1}\]
Volendo ottenere una maggiorazione di tale aspettativa matematica, si può fare in modo che la seconda sommatoria giunga fino a $n$ e l'$1$ del denominatore venga eliminato, ottenendo:
\[E \left[\mathcal{X}\right] < 2 \cdot \sum_{i=1}^{n-1} \sum_{k=1}^{n} \frac{1}{k}\]
in cui la seconda sommatoria è proprio la serie armonica precedentemente calcolata e di cui si era visto che
\[\sum_{k=1}^n \frac{1}{k} = \Theta(\log_2(n))\]
Per cui si può scrivere:
\[E \left[\mathcal{X}\right] < 2 \cdot \sum_{i=1}^{n-1} \sum_{k=1}^{n} \frac{1}{k} < 2 \cdot \sum_{i=1}^{n} \sum_{k=1}^{n} \frac{1}{k} = 2 \cdot \sum_{i=1}^{n} \Theta(\log_2(n)) = 2 \Theta(n \cdot \log_2(n)) = \Theta(n \cdot \log_2(n))\]
Naturalmente, trattandosi di una maggiorazione, si ottiene che:
\[E(\mathcal{X}) = O(n \cdot \log(n))\]
e ricordando il teorema sulla complessità di un algoritmo di ordinamento generale, si ha anche che $E(\mathcal{X}) = \Omega(n \cdot \log(n))$, per cui si ottiene che:
\[\boxed{E(\mathcal{X})= \Theta(n \cdot \log(n))}\]
Per cui si ottiene che la complessità di randomized-quick-sort \textbf{tipicamente} è \textbf{log-lineare}: più specificatamente è noto che nel caso peggiore la complessità è quadratica, mentre nel caso medio (o tipico) e nel caso migliore, la complessità è \textbf{log-lineare} (quindi si ha che tipicamente l'algoritmo opera molto più vicino al caso migliore).

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che gli algoritmi fino ad adesso esposti non alterano la loro esecuzione a seconda della natura o grandezza dei numeri da ordinare.\\
Pertanto, gli algoritmi di ordinamento generale (per \textbf{confronti}) presentano una complessità nel caso peggiore è almeno log-lineare:
\[\boxed{T_{\text{worst}}(n)=\Omega(n \cdot \log(n))}\]
mentre nel caso medio si ha una complessità, ancora una volta, almeno log-lineare:
\[\boxed{T_{\text{avarage}}(n)=\Omega(n \cdot \log(n))}\]
È chiaro che sapendo il secondo risultato, naturalmente si ottiene anche il primo.

\vspace{1em}
\noindent
\textbf{Osservazione}: Per dimostrare che un algoritmo di ordinamento per confronti presenta una complessità almeno log-lineare, si osservi che un algoritmo per confronti, al fine di procedere all'ordinamento di $n$ valori distinti, procede alla costruzione di un albero binario avente $n!$ foglie, in cui si trovano proprio gli ordinamenti possibili di $n$ valori distinti.

\newpage
\noindent
\begin{center}
  25 Marzo 2022
\end{center}
In un algoritmo di ordinamento generale l'elemento fondamentale è dato dai confronti, i quali vanno a costituire un albero binario, sulle cui foglie sono contenuti tutti i possibili ordinamenti dei valori di partenza: in particolare, ogni algoritmo di ordinamento di tipo generale presenta una complessità che è dettata unicamente dai confronti.\\
È ovvio che, presi in considerazione $n$ numeri distinti, il numero di ordinamenti possibili è dato da $n!$: in un albero binario, qualunque sia l'algoritmo di ordinamento generale considerato (efficiente o meno), tali ordinamenti sarebbero tutti collocati sulle foglie: ecco che il modo per osservare la \textbf{complessità nel caso peggiore} è quello di determinare l'\textbf{altezza dell'albero} (da intendersi come altezza massima, ovvero la distanza tra la radice e la foglia più lontana); inoltre \textbf{la complessità media} è data dalla \textbf{media delle distanze} delle foglie dalla radice (ovvero la somma delle distanze dalla radice di tutte le foglie diviso per $n!$, con $n!$ numero di foglie).\\
La soluzione più auspicabile al fine di ottimizzare sia la complessità media che la complessità massima, evitando che vi sia una differenza di distanza superiore a $1$ è quella di andare a costruire (tramite un algoritmo opportunamente definito) un \textbf{albero binario completo}, riempiendo completamente tutti i livelli: taluna è la situazione più favorevole, in quanto la differenza tra l'altezza massima e quella minima è proprio $1$ (per cui dal punto di vista della notazione $\Theta$ non cambia nulla).\\
Pertanto, sapendo che l'altezza di un albero binario completo è data dal logaritmo del numero delle foglie, si deduce che la comlessità nel caso peggiore di un algoritmo di ordinamento che è in grado di produrre dei confronti che si diramano su un albero binario completo è
\[T_{\text{worse}}(n)=\Omega(\log_2(n!))\]
Questo, naturalmente, quando vi è un algoritmo capace di costruire un albero binario completo, altrimenti la situazione (così come la complessità), non può che peggiorare (e tale complessità è la stessa anche nel caso medio).

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che nel caso dell'algoritmo quick-sort, presa una qualsiasi permutazione di $n$ numeri in ingresso, essa presenterà la stessa probabilità di presentarsi rispetto alle altre nell'ordinamento, la quale è pari a
\[\frac{1}{n!}\]
Naturalmente, l'esecuzione dell'algoritmo quick-sort presenta un elemento di casualità, in quando dipende dall'estrazione di una possibile permutazione delle $n!$ $n$-uple in ingresso. Tuttavia, dal punto di vista di quick-sort, estrarre casualmente una permutazione di $n$ numeri o estrarre di volta in volta in maniera casuale un perno, rappresentano la stessa cosa (cosa che si può facilmente dimostrare) in quanto non si sta privilegiando una configurazione rispetto ad un'altra: la complessità media di quick-sort e di randomized-quick-sort permane sempre log-lineare.\\
È noto che la complessità nel caso medio di un algoritmo di ordinamento basato sui confronti è almeno $\log_2(n!)$ (con $n$ numero di oggetti da ordinare); per capire l'entità del fattoriale di un numero $n$ è opportuno richiamare la \textbf{formula di Stirling}:
\[\boxed{n! \cong \sqrt{2 \pi n} \cdot \left(\frac{n}{e}\right)^n = \text{St}(n)}\]
in cui $\text{St}(n)$ è un'abbreviazione di formula di Stirling, la quale approssima il fattoriale di un numero $n$. Tale approssimazione non controlla certamente l'errore assoluto, ma controlla l'errore relativo (o percentuale), in quanto
\[\boxed{\lim_{n \to +\infty} \frac{n!}{\text{St}(n)}=1}\]
In particolare si ha che il rapporto di cui sopra è pari alla somma di $1$ e di una funzione infinitesima, come mostrato di seguito
\[\frac{n!}{\text{St}(n)}=1+\phi(n)\]
in cui $\phi(n)$ è una funzione infinitesima per $n \to \infty$, come quella mostrate di seguito:
\[1 < \frac{n!}{\text{St}(n)} < 1 + \frac{1}{11n}\]
in cui si può osservare come, in generale, $\text{St}(n)<n!$. Per questo si preferisce scrivere, dal punto di vista formale:
\[n! = \text{St}(n) \cdot \left[1 + O\left(\frac{1}{n}\right)\right]\]
In cui dal punto di vista della notazione O-grande, ciò significa che
\[1+O\left(\frac{1}{n}\right) := \exists c \geq 0 : 1+c\cdot \frac{1}{n} \leq 1+\frac{1}{n}\]
in cui è noto, come mostrato nel precedente esempio, che $c=\frac{1}{11}$.\\
Tuttavia, è noto che il rapporto tra $n!$ e $\text{St}(n)$ per $n \to +\infty$ è uguale a $1$, quindi hanno lo stesso ordine di infinito e quindi appartengono alla stessa classe $\Theta$, per cui
\[\Theta(n!)=\Theta(\text{St}(n))\]
Ma è facile dimostrare che se due infiniti stanno nella stessa classe $\Theta$, anche i loro logaritmi appartengono alla stessa classe $\Theta$. Infatti, se è noto che
\[\exists \overline{x}, c_1 \geq 0, c_2 \geq 0 : c_1 \cdot g(x) \leq f(x) \leq c_2 \cdot g(x), \hspace{1em} \forall x \geq \overline{x}\]
Ma giacché per le proprietà dei logaritmi si può scrivere
\[g(x) = a^{\log_a(g(x))}\]
essendo ambedue le funzioni definitivamente positive, allora si ha che:
\[\exists \overline{x}, c_1 \geq 0, c_2 \geq 0 : a^{\log_a(c_1)} \cdot a^{\log_a(g(x))} \leq a^{\log_a(f(x))} \leq a^{\log_a(c_2)} \cdot a^{\log_a(g(x))}, \hspace{1em} \forall x \geq \overline{x}\]
E potendo ora trasferire la disuguaglianza agli esponenti, essendo tutte potenze con la stessa base si ottiene
\[\exists \overline{x}, c_1 \geq 0, c_2 \geq 0 : \log_a(c_1) + \log_a(g(x)) \leq \log_a(f(x)) \leq \log_a(c_2) + \log_a(g(x)), \hspace{1em} \forall x \geq \overline{x}\]
Volendo ottenere una minorazione nel primo caso e una maggiorazione nel secondo è possibile sostituire $\log_a(c_1)$ con $- \frac{1}{2} \cdot \log_a(g(x))$ (essendo una quantità negativa) e $\log_a(c_2)$ con $\log_a(g(x))$ (essendo una funzione infinita), ottenendo
\[\exists \overline{x} : \frac{1}{2} \cdot \log_a(g(x)) \leq \log_a(f(x)) \leq 2 \cdot \log_a(g(x)), \hspace{1em} \forall x \geq \overline{x}\]
Da cui si evince che:
\[\Theta(\log(n!))=\Theta(\log(\text{St}(n)))\]
In cui si può calcolare che
\[\log(\text{St}(n)) = \log(\sqrt{2\pi n}) + n \cdot \log(n) - n \cdot \log(e)\]
Al fine di ottenere una maggiorazione, si può eliminare il termine negativo e, siccome, $n \cdot \log(n) \geq \log(\sqrt{2\pi n})$ (nel senso che il primo è un infinito di ordine superiore al secondo), si può scrivere
\[\log(\text{St}(n)) \leq n \cdot \log(n) + n \cdot \log(n) + 0 = 2n \cdot \log(n)\]
Al fine di ottenere una minorazione, invece, si elimina il termine $\log(\sqrt{2\pi n})$ e si sottrae al rimanente $\frac{1}{2} n \cdot \log(n) $, ossia un infinito log-lineare, in luogo di $n \cdot \log(e)$ che è solo un infinito lineare, ottenendo
\[\log(\text{St}(n)) \geq n \cdot \log(n) - n \cdot \log(e) \geq n \cdot \log(n) - \frac{1}{2} n \cdot \log(n) = \frac{1}{2} n \cdot \log(n)\]
Da cui si evince che l'approssimazione di Stirling appartiene ad una classe log-lineare, ed appartenendo anche alla classe del logaritmo del fattoriale di $n$ se ne deduce che
\[\boxed{\Theta(\log(n!)) = \Theta(n \cdot \log(n))}\]
Pertanto, si ha che la complessità di un algoritmo di ordinamento generale (ossia per confronti), nel caso medio e nel caso peggiore è $\Omega(n \cdot \log(n))$.

\newpage
\noindent
\section{Ordinamento di strutture dati specifiche}
Gli algoritmi fin d'ora considerati sono totalmente indipendenti dalle strutture e dalla natura dei dati da ordinare, mentre vi sono alcuni algoritmi che funzionano in maniera eccelsa quando in ingresso vi sono degli specifici pattern di valori.\\
Pertanto, in tal senso, non si contraddice il teorema appena dimostrato che afferma che un algoritmo di ordinamento generale (basato sui confronti) presenta una complessità media e peggiore almeno log-lineare, in quanto gli algoritmi di seguito esposti \textbf{non sono basati sui confronti}, in quanto \textbf{strettamente correlati alla natura dei dati} (e non al loro ordinamento iniziale).

\vspace{1em}
\subsection{Counting-Sort}
Si considerino $n=200000$ \textbf{valori interi positivi} da ordinare, i quali, tuttavia, sono tutti valori compresi tra $0$ e $k$, con $k << n$: è chiaro che i valori da ordinare saranno \textbf{duplicati molteplici volte}.\\
È possibile, naturalmente, effettuare delle generalizzazioni dell'algoritmo: se i numeri da ordinare andassero da $r$ a $r+k$, basterebbe sottrarre a ciascuno $r$ e si otterrebbe la configurazione di partenza (con valori comprersi tra $0$ e $k$); si potrebbe, inoltre, addattare l'algoritmo all'ordinamento non solo di numeri interi, ma anche di numeri razionali o irrazionali, codificandoli opportunamente.\\
Si propone di seguito il listato dell'algoritmo, il quale non rappresenta un ordinamento in loco, ma necessita di un array ausiliario $B$ per effettuare l'ordinamento:

\begin{algorithm}[H]
  \caption{Counting-Sort}
  \begin{algorithmic}[1]
    \State COUNTING-SORT($A$,$B$,$k$)
    \State \textbf{for} $i=0$ \textbf{to} $k$
    \Indent
      \State \textbf{do} $C[i]=0$
    \EndIndent
    \State \textbf{for} $j=1$ \textbf{to} $n$
    \Indent
      \State \textbf{do} $C \left[A[j]\right]=C \left[A[j]\right]+1$
    \EndIndent
    \State \textbf{for} $i=1$ \textbf{to} $k$
    \Indent
      \State \textbf{do} $C[i]=C[i]+C[i-1]$
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
In cui il numero di iterazione del primo ciclo è $k+1$, nel secondo ciclo è $n$, mentre nel terzo vi sono $k$ iterazioni: in totale si hanno $n+2k+1$ iterazioni.\\
All'inizio dell'algoritmo, con il primo ciclo for, si costruisce l'array $C$ come segue:
\[C=\underset{0}{\boxed{0}}\underset{1}{\boxed{0}}\underset{2}{\boxed{0}}\underset{3}{\boxed{0}}\]
e si supponga che l'array $A$ da ordinare sia
\[A=\underset{1}{\boxed{0}}\underset{2}{\boxed{3}}\underset{3}{\boxed{0}}\underset{4}{\boxed{3}}\underset{5}{\boxed{1}}\]
Nel secondo ciclo for si deve procedere ad incrementari i valori all'interno dell'array $C$, come mostrato di seguito:
\[C=\underset{0}{\boxed{2}}\underset{1}{\boxed{1}}\underset{2}{\boxed{0}}\underset{3}{\boxed{2}}\]
in cui semplicemente si osserva come all'interno di tale array sono contenuti i conteggi di ogni valore presente in $A$, corrispondenti alle posizioni dell'array $C$: la cella $C[0]$ memorizza il numero di $0$ all'interno dell'array $A$, la cella $C[1]$ memorizza il numero di $1$ all'interno dell'array $A$ e così via.\\
Nell'ultimo ciclo for si procede ad incrementare ogni elemento di ogni posizione con il valore della posizione precedente, ottenendo
\[C=\underset{0}{\boxed{2}}\underset{1}{\boxed{3}}\underset{2}{\boxed{3}}\underset{3}{\boxed{5}}\]
in cui in ogni posizione è contenuto il conteggio dei numeri che nell'array $A$ sono minore (o uguali) della posizione dell'array $C$: nella cella $C[0]$ viene memorizzato il numero di valori che nell'array $A$ sono minori (o uguali) di $0$, nella cella $C[1]$ viene memorizzato il numero di valori che nell'array $A$ sono minori (o uguali) di $1$, e così via. Per giungere a tale configurazione è stato effettuato uno sforzo di $n+2k+1$ (corrispondente ad una complessità $\Theta(n+k)$); si propone, adesso, la continuazione dell'algoritmo precednete:

\begin{algorithm}[H]
  \caption{Counting-Sort}
  \begin{algorithmic}[1]
    \State COUNTING-SORT($A$,$B$,$k$)
    \State \textbf{for} $i=0$ \textbf{to} $k$
    \Indent
      \State \textbf{do} $C[i]=0$
    \EndIndent
    \State \textbf{for} $j=1$ \textbf{to} $n$
    \Indent
      \State \textbf{do} $C \left[A[j]\right]=C \left[A[j]\right]+1$
    \EndIndent
    \State \textbf{for} $i=1$ \textbf{to} $k$
    \Indent
      \State \textbf{do} $C[i]=C[i]+C[i-1]$
    \EndIndent
    \State \textbf{for} $j=n$ \textbf{down to} $1$
    \Indent
      \State \textbf{do} $B[C[A[j]]]=A[j]$
      \Indent
        \State $C[A[j]] = C[A[j]]-1$
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
Con l'ultimo ciclo introdotto, il numero di iterazioni complessive dell'algoritmo diviene $2n+2k+1$, che corrisponde alla classe $\Theta(n+k)$.\\
L'ultimo ciclo (che va al contrario, anche se non cambierebbe nulla), semplicemente, va a collocare i valori di $A$ correttamente all'interno di $B$; avendo a disposizione gli array seguenti:
\[A=\underset{1}{\boxed{0}}\underset{2}{\boxed{3}}\underset{3}{\boxed{0}}\underset{4}{\boxed{3}}\underset{5}{\boxed{1}}\]
\[C=\underset{0}{\boxed{2}}\underset{1}{\boxed{3}}\underset{2}{\boxed{3}}\underset{3}{\boxed{5}}\]
\[B=\underset{1}{\boxed{0}}\underset{2}{\boxed{0}}\underset{3}{\boxed{0}}\underset{4}{\boxed{0}}\underset{5}{\boxed{0}}\]
Si procede come segue: posto $j=n$, l'algoritmo valuta che vi sono $C[A[j]]$ valori che sono minori o uguali a $A[j]$, per cui basta porre in $B[C[A[j]]]$ il valore $A[j]$ cercato e decrementare il conteggio in $C[A[j]]$, ottenendo:
\[A=\underset{1}{\boxed{0}}\underset{2}{\boxed{3}}\underset{3}{\boxed{0}}\underset{4}{\boxed{3}}\underset{5}{\boxed{1}}\]
\[C=\underset{0}{\boxed{2}}\underset{1}{\boxed{2}}\underset{2}{\boxed{3}}\underset{3}{\boxed{5}}\]
\[B=\underset{1}{\boxed{0}}\underset{2}{\boxed{0}}\underset{3}{\boxed{1}}\underset{4}{\boxed{0}}\underset{5}{\boxed{0}}\]
Procedendo con $j=n-1$ si ottiene
\[A=\underset{1}{\boxed{0}}\underset{2}{\boxed{3}}\underset{3}{\boxed{0}}\underset{4}{\boxed{3}}\underset{5}{\boxed{1}}\]
\[C=\underset{0}{\boxed{2}}\underset{1}{\boxed{2}}\underset{2}{\boxed{3}}\underset{3}{\boxed{4}}\]
\[B=\underset{1}{\boxed{0}}\underset{2}{\boxed{0}}\underset{3}{\boxed{1}}\underset{4}{\boxed{0}}\underset{5}{\boxed{3}}\]
Procedendo con $j=n-2$ si ottiene
\[A=\underset{1}{\boxed{0}}\underset{2}{\boxed{3}}\underset{3}{\boxed{0}}\underset{4}{\boxed{3}}\underset{5}{\boxed{1}}\]
\[C=\underset{0}{\boxed{1}}\underset{1}{\boxed{2}}\underset{2}{\boxed{3}}\underset{3}{\boxed{4}}\]
\[B=\underset{1}{\boxed{0}}\underset{2}{\boxed{0}}\underset{3}{\boxed{1}}\underset{4}{\boxed{0}}\underset{5}{\boxed{3}}\]
Procedendo con $j=n-3$ si ottiene
\[A=\underset{1}{\boxed{0}}\underset{2}{\boxed{3}}\underset{3}{\boxed{0}}\underset{4}{\boxed{3}}\underset{5}{\boxed{1}}\]
\[C=\underset{0}{\boxed{1}}\underset{1}{\boxed{2}}\underset{2}{\boxed{3}}\underset{3}{\boxed{3}}\]
\[B=\underset{1}{\boxed{0}}\underset{2}{\boxed{0}}\underset{3}{\boxed{1}}\underset{4}{\boxed{3}}\underset{5}{\boxed{3}}\]
Procedendo con $j=n-4$ si ottiene
\[A=\underset{1}{\boxed{0}}\underset{2}{\boxed{3}}\underset{3}{\boxed{0}}\underset{4}{\boxed{3}}\underset{5}{\boxed{1}}\]
\[C=\underset{0}{\boxed{0}}\underset{1}{\boxed{2}}\underset{2}{\boxed{3}}\underset{3}{\boxed{3}}\]
\[B=\underset{1}{\boxed{0}}\underset{2}{\boxed{0}}\underset{3}{\boxed{1}}\underset{4}{\boxed{3}}\underset{5}{\boxed{3}}\]

\vspace{1em}
\noindent
Per valutare la complessità globale dell'algoritmo, si parte dalla supposizione che $k << n$, ovvero $k = O(n)$; per cui, se la complessità dello sforzo dell'algoritmo è $\Theta(n+k)$, eseguendone la somma prevale il maggiore (e quindi lo sforzo più esoso) e quindi la \textbf{complessità diviene lineare}, ovvero $\Theta(n)$, sotto le opportune \textbf{condizioni di partenza}, ovviamente.

\vspace{1em}
\subsection{Bucket-Sort}
L'\textbf{ordinamento per contenitori} è estremamente diffuso. Esso si basa su due \textbf{strutture dati fondamenti}: \emph{array} e \emph{lista sequenziale}.\\
Gli array, come è noto, presentano una struttura sequenziale di \textbf{celle di memoria consecutive}, avente lunghezza $n$. Se si considera un array $A$ di $length(A)=n$ (con $n \geq 1$), allora il costo di posizionarsi in una qualsiasi posizione $0 \leq i \leq n$ è semplicemente $\Theta(1)$, effettuando un \textbf{accesso libero (o lineare o random)}.\\
Tale aspetto è estremamente significativo e vantaggioso; tuttavia, ciò comporta di sapere a priori la lunghezza dell'array (giacché si devono disporre $n$ celle di memoria consecutive), mostrandone la natura profondamente rigida: molto spesso, però, non è nota a priori la lunghezza dei dati da memorizzare ed è più conveniente che essa cresca progessivamente nel corso dell'esecuzione dinamica dell'algoritmo: tale esigenza viene soddisfatta dalla \textbf{lista sequenziale}.\\
La dichiarazione di una lista $L$ necessita di $3$ fondamentali informazioni:
\[\boxed{\text{NIL}}\boxed{\text{NIL}}\boxed{\text{O}}\]
in cui nelle prime due celle sono presenti due valori nulli per specificare il \textbf{puntatore al nodo iniziale} e il \textbf{puntatore al nodo finale} della lista, mentre nell'ultima cella è contenuta la l\textbf{lunghezza della lista}.\\
Quando viene eseguita l'istruzione INSERT($x$,$L$), viene creato un nodo avente due celle: un valore (o \textbf{chiave}) e un puntatore al nodo successivo. È chiaro che il puntatore al nodo successivo della lista punta a NIL, essendo l'ultimo nodo (ossia l'unico inserito). Le due cella della lista, invece, dopo aver inserito il primo nodo, punteranno allo stesso nodo, essendo l'unico della lista, mentre il contatore viene incrementato di $1$, e così via: ogni qualvolta verrà inserito un nuovo nodo, il nodo precedente punterà a quello successivo e il nuovo nodo punterà a NIL, mentre verrà aggiornato il puntatore della lista al nodo finale, così come il contatore del numero di nodi della lista.

\newpage
\noindent
\begin{center}
  30 Marzo 2022
\end{center}
Le due struttre dati fondamentali impiegabili in algoritmica sono l'array e la lista sequenziale.\\
Si tratta in ambedue i casi di liste, ma nel caso dell'array, è fondamentale sapere a priori di quante posizioni successive si necessita per la memorizzazione. L'array viene identificato con un nome simbolico che punta alla prima cella di memoria disponibile della sequenza di $n$ celle di memoria consecutive. Tuttavia, il costo di posizionarsi su una specifica cella di memoria per un array è $\Theta(1)$ (ovvero l'accesso ha un costo compreso tra due costanti positive $c_1 \leq c_2$): si parla, in questo caso, di \textbf{random-access} (anche definito \textbf{accesso libero} o \textbf{diretto}).\\
L'altra struttura dati prende il nome di \textbf{lista sequenziale}, per la quale non è necessario specificare a priori la lunghezza della lista, il che si rivela particolarmente utile quando non si conosce a priori la quantità di dati da dover memorizzare. La lista viene così dichiarata:
\[L \hspace{0.5em} \boxed{\text{NIL}}\boxed{\text{NIL}}\boxed{\text{O}}\]
in cui nelle prime due celle di memoria vengono memorizzati, rispettivamente, un puntatore che punta all'inizio della lista e un puntatore che punta alla fine della lista, mentre nella terza cella è presente la lunghezza della lista.\\
Per l'inserimento di un nuovo nodo è sufficiente impiegare la procedura INSERT($x$,$L$), grazie alla quale viene creata una doppia cella di memoria (una doppia-posizione), contente il valore da memorizzare e un puntatore alla cella successiva (che per un inserimento in coda contiene naturalmente NIL, all'inizio). I puntatori dell'inizio e fine lista devono essere oppurtanemente aggiornati (entrambi puntano allo stesso unico nodo, un quanto sia nodo iniziale che finale della lista), così come il contatore della dimensione (che viene incrementato di $1$), e così per ogni nuovo nodo che viene inserito.\\
Naturalmente, quando si necessita di cancellare un nodo in testa, si deve impiegare l'istruzione DELETE($L$), che provvede a cancellare il nodo in testa, aggiornando il puntatore all'inizio della lista e decrementando il contatore della lunghezza della lista: ovviamente tale operazione palesa il problema dell'\textbf{underflow} che si potrebbe verificare quando si tenta di cancellare il nodo in testa di una lista vuota, avente lunghezza nulla (vero è che come si può verificare l'underflow si può verificare l'\textbf{overflow}, anche se quest'ultimo costituisce un'eventualità assai più remota della prima).\\
Ciò che penalizza, tuttavia, la lista, è il fatto che il costo di accesso ad un'ipotetica cella di memoria è $O(n)$, in quanto l'\textbf{accesso può essere solamente sequenziale}.\\
Tuttavia, nonostante tale complessità, l'operazione di \textbf{join} favorisce la lista sequenziale anziché l'array: infatti, crare un solo array $A$ partendo da due array $A_1$ e $A_2$ (ossia effettuando il join $A = A_1 \circ A_2$) presenta un costo $\Theta(n_1+n_2)$, dove $n_1$ e $n_2$ sono gli elementi contenuti nei due array, in quanto bisogna inserire $n_1+n_2$ valori all'interno di una serie di celle sequenziali. Invece, la lista sequenziale presenta un costo per il join (ossia $L=L_1 \circ L_2$) che è semplicemente $\Theta(1)$, in quanto è sufficiente aggiornare il puntatore all'ultimo nodo della lista e per la lunghezza è sufficiente eseguire la somma tra i due contatori, quindi molto più conveniente.\\
La lista sequenziale così descritta, che prevede un inserimento in \textbf{coda}, presenta una logica di tipo \textbf{FIFO} (first-in first-out): rassomiglia, quindi, ad una coda che presenta i metodo ENQUEUE per l'inserimento e DEQUEUE per la cancellazione. Invece, se l'inserimento è in testa, si ottiene uno \textbf{stack} (o pila), che presenta una dinamica \textbf{LIFO} (last-in first-out).\\
Le liste bidirezionali, invece, prevedono di impiegare due puntatori per ogni nodo (uno che punta al nodo precedente e uno che punta al nodo successivo), che quindi consentono di percorerre la lista in due direzioni, per cui il costo di accesso diviene $O \left( \frac{n}{2} \right)$.

\vspace{1em}
\noindent
\textbf{Osservazione}: La speranza matematica è sempre additiva, mentre la varianza no: una condizione sufficiente perché lo sia è l'indipendenza di eventi.\\
Per esempio, se si considera la funzione deterministica $f(\mathcal{X}) = \alpha \cdot \mathcal{X} + \beta$, allora si ha che
\[\boxed{E\left[\alpha \cdot \mathcal{X} + \beta\right] = \alpha \cdot E\left[\mathcal{X}\right] + \beta}\]
e
\[\boxed{\text{var}\left[\alpha \cdot \mathcal{X} + \beta\right]=\alpha^2 \cdot \text{var}\left[\mathcal{X}\right]}\]
volendo recuperare la dimensione di $\mathcal{X}$, è sufficiente passare allo scarto quadratico medio, o alla deviazione standard:
\[\boxed{\text{sqm}\left[\alpha \cdot \mathcal{X} + \beta\right] = \sqrt{\alpha^2 \cdot \text{var}\left[\mathcal{X}\right]} = \vert \alpha \vert \cdot \text{sqm} \left[\mathcal{X}\right]}\]

\vspace{1em}
\noindent
Se si conidera un indicatore di successo $I$ (ossia un bit aleatorio che vale $1$ con probabilità $p$ e vale $0$ con probabilità $q=1-p$), è facile capire come $E \left[I\right] = p$, mentre $\text{var} \left[I\right]=pq$.\\
Se, invece, si considera, un numero $n$ di esperimenti (ripetuti nelle stesse condizioni), a cui vengono associati $n$ indicatori di successo $N = I_1 + I_2 + ... + I_n$, sommando il numero aleaorio di successi $N$, si giunge alla distribuzione binomiale seguente, per la quale, essendo la speranza matematica additiva, così come la varianza essendo esperimenti indipendenti l'uno dall'altro:
\[E \left[N\right] = np \hspace{1em} \text{e} \hspace{1em} \text{var} \left[N\right] = npq\]
Volendo determinare la speranza matematica di $N^2$, si può facilmente osservare che:
\[\text{var} \left[N\right] = E \left[(N-\mu)^2\right] = E[N^2] - \left[E[N]\right]^2\]
dove $\mu = E[N]$. Per cui è facile capire come
\[E[N^2] = \text{var}[N] + \left[E[N]\right]^2 \longrightarrow npq + n^2p^2 = np \cdot (q + np)\]
ma essendo la probabilità di insuccesso $q=1-p$ si ottiene che
\[npq + n^2p^2 = n p \cdot (q + np) = n p \cdot (1 - p + np)\]
Ma siccome in bucket-sort è fondamentale valutare la speranza matematica di $N^2$ quando la probabilità di successo $p=\frac{1}{n}$, si può facilmente capire come si semplificano la speranza matematica e la varianza, da cui
\[E[N]=np=n \cdot \frac{1}{n}=1\]
da cui
\[n p \cdot (1 - p + np) = n \cdot \frac{1}{n} \cdot \left(1 - \frac{1}{n} + n \cdot \frac{1}{n}\right) = 2 - \frac{1}{n} < 2\]
ottenenendo
\[\boxed{E \left[N^2 \right]=2 - \frac{1}{n} < 2}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: In bucket-sort, dovendo ordinare valori reali in un ipotetico intervallo $[\alpha, \beta]$ con $\alpha < \beta$, si può facilmente considerare, per semplicità, l'intervallo $[0,1]$, giacché la trasformazione lineare seguente
\[x_i=\frac{y_i - \alpha}{\beta - \alpha}\]
permette di ricondurre l'intervallo $[\alpha,\beta]$ all'intervallo $[0,1]$, con $x_i \in [0,1]$ quando $y_i \in [\alpha,\beta]$ (infatti, per $y_i=\alpha$ e $y_i=\beta$ si ottengono gli estremi dell'intervallo unitario): pertanto i due intervalli sono perfettamente equivalenti.\\
Tuttavia, il calcolatore non può considerare un intervallo continuo $[0,1]$, in quanto la precisione di rappresentazione è fortemente limitata, per cui il numero di valori rappresentabili nell'intervallo $[0,1]$ è \textbf{finito}.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che i valori numerici non hanno un'unica rappresentazione, in quanto il valore $1$ può anche essere scritto come $0.\overline{9}$, così come il valore $0.323$ può essere scritto come $0.322\overline{9}$; ciò, naturalmente, quando il numero di cifre decimali a disposizione è infinito.

\vspace{1em}
\noindent
\textbf{Osservazione}: Naturalmente, l'intervallo $[0,1]$ presenta la cardinalità del continuo, mentre il calcolatore non solo non è capace di avere una precisione infinita di rappresentazione, ma non è in grado nemmeno di garantire la precisione infinita razionale (e non continua) dell'intervallo $[0,1]$. Ciò che è possibile fare è fissare un numero massimo di cifre decimali (come $8$ nel caso di un byte), come $0.03211448$.\\
Pertanto, considerando la legge di distribuzione di probabilità continua sull'intervallo $[0,1]$, la probabilità di un sottointervallo $[a,b] \subset [0,1]$ è naturalmente calcolabile come l'integrale sull'intervallo $[a,b]$ della funzione costante $1$; tuttavia, tale risultato non può essere ottenuto da un calcolatore con una precisione finita, ma può essere fornita un'approssimazione di tale concetto estraendo in maniera uniforme $10^8$ valori (ossia sempre un numero finito, da $0.00000001$ a $0.99999999$) nell'intervallo $[0,1]$, escluso il valore estremo di destra, quindi $[0,1[$, ciascuno dei quali presenta una probabilità di $1$ su $10^8$ di essere estratto.\\
È intuibile capire che lavorando con $n >> 1$ valori da ordinare, l'algoritmo \textbf{bucket-sort} funziona particolarmente bene quando i valori da ordinare presentano una distribuzione quanto più uniforme nell'intervallo $[0,1[$ (dimodoché non vi siano sottointervalli di omogeneità eccessivamente difforme).\\
Di seguito si espone il listato dell'algoritmo \textbf{bucket-sort}:

\begin{algorithm}[H]
  \caption{Bucket-Sort}
  \begin{algorithmic}[1]
    \State BUCKET-SORT($A$)
    \State $n=\text{length}(A)$
    \State \textbf{for} $i=1$ \textbf{to} $n$
    \Indent
      \State \textbf{do} \textbf{insert} $A[i]$ \textbf{in} $B \left[ \left \lfloor n \cdot A[i] \right \rfloor \right]$
    \EndIndent
    \State \textbf{for} $i=0$ \textbf{to} $n-1$
    \Indent
      \State \textbf{do} \textbf{order} $B[i]$ \textbf{with} \emph{insertion-sort}
    \EndIndent
    \State \textbf{join lists} $B[0],B[1],...,B[n-1]$ in this order
  \end{algorithmic}
\end{algorithm}

\noindent
Per l'ordinamento, quindi, oltre all'array iniziale $A$, si impiega un secondo array $B$ nel quale, da $0$ a $n-1$, si inserisce inizialmente il valore NIL, e progressivamente viene creata una lista concatenata per ciascuna (non necessariamente) posizione di $B$ (ovvero nelle posizioni dell'array $B$ si inserirà un puntatore ad una lista sequenziale), tramite il primo ciclo for, in cui vi sono $n$ iterazioni, per uno sforzo complessivo di $\Theta(n)$, come mostrato di seguito:

\begin{flalign*}
   & \hspace{1.6em} A \hspace{6em} B\\
  \boxed{1} & \hspace{1em} \boxed{0.78} \hspace{2em} \boxed{0} \hspace{1em} \boxed{B[0]} \longrightarrow \boxed{0.17}\\
  \boxed{2} & \hspace{1em} \boxed{0.17} \hspace{2em} \boxed{1} \hspace{1em} \boxed{B[1]} \longrightarrow \boxed{0.39} \longrightarrow \boxed{0.26}\\
  \boxed{3} & \hspace{1em} \boxed{0.39} \hspace{2em} \boxed{2} \hspace{1em} \boxed{\text{NIL}}\\
  \boxed{4} & \hspace{1em} \boxed{0.26} \hspace{2em} \boxed{3} \hspace{1em} \boxed{B[3]} \longrightarrow \boxed{0.78}\\
  \boxed{5} & \hspace{1em} \boxed{0.94} \hspace{2em} \boxed{4} \hspace{1em} \boxed{B[4]} \longrightarrow \boxed{0.94}
\end{flalign*}

\vspace{1em}
\noindent
Pertanto, se i valori di $A$ sono disposti uniformemente nell'intervallo $[0,1]$, allora ogni bucket (ossia ogni posizione di $B$) presenterà un numero pressoché identico di valori (e siccome i bucket sono $n$ e i numeri sono $n$, su ogni bucket vi saranno $0$,$1$ o $2$ elementi). Per tale ragione è possibile impiegare l'algoritmo \textbf{insertion-sort} per ordinare i valori di ciascun bucket (anche se la complessità di insertion-sort è quadratica, per cui il prezzo da pagare è almeno $\Omega(n)$).\\
Dopodiché, avendo ordinato ciascun bucket, sicché i valori di ciascun bucket sono inferiori dei valori dei bucket successivi, sarà sufficiente effettuare il \textbf{join} di tutte le liste di ciascun bucket: siccome si devono effettuare $n-1$ join con complessità $\Theta(1)$, il costo di tale operazione è $\Theta(n)$.

\vspace{1em}
\noindent
\textbf{Osservazione}: Naturalmente, l'impiego di bucket-sort è stato subordinato ad una condizione vaga posta in principio: la supposizioni che i valori da ordinare siano disposti in maniera uniforme all'interno dell'intervallo $[0,1]$.\\
Al fine di stimare la complessità di bucket-sort, si può valutare la probabilità che i valori che devono essere ordinati siano uniformemente distribuiti nell'intervallo $[0,1[$ (in cui l'algoritmo bucket-sort funziona in maniera ottimale): in altre parole, impiegando la distribuzione uniforme, si generano a caso $n$ numeri sull'intervallo $[0,1[$, provvedendo ad ordinarli tramite bucket-sort, la cui complessità d'esecuzione dipende da quali numeri sono stati estratti, in altre parole si ha una \textbf{complessità aleatoria}, di cui si provvede a calcolare la speranza matematica, al fine di disporre di una stima della complessità dell'algoritmo.\\
Si impieghi, allora, la procedura RANDOM($0$,$0.99999999$) per estrarre $n$ volte, mediante la distribuzione uniforme, un numero reale nell'intervallo $[0,1[$ e si calcoli la probabilità che tale valore si trovi inserito all'interno del bucket $i$-esimo, avendo $n$ bucket (ossia $n$ valori da ordinare); si osservi che i bucket che vengono costruiti presentano tutti la medesima ampiezza all'interno della distribuzione uniforme, per cui ogni numero presenta la stessa probabilità di finire nel bucket $i$-esimo, ossia pari a
\[p=\frac{1}{n}\]
essendo $n$ i bucket. Allora considerando $N_i$ il numero di successi, ossia il numero di numeri che vengono inseriti all'interno del bucket $i$-esimo con probabilità $p=\frac{1}{n}$, per quando visto all'inizio, si ottiene che
\[E[N_i]=1 \hspace{1em} \text{e} \hspace{1em} E[N_i^2]=2-\frac{1}{n}<2\]
che porta a concludere che, tipicamente, in ogni bucket vi sarà contenuto uno e un solo valore (ovviamente tale risultato è aleatorio, in quanto vi è comunque una bassa probabilità che in un solo bucket vi cadano $n$ valori, ossia la totalità, per cui la complessità pessima di insertion-sort su $n$ elementi, verrebbe riflessa su bucket-sort).\\
Tralasciando la dimostrazione che $E\left[\Theta(f(n))\right] = \Theta \left(E()\right)$, si calcoli la speranza matematica di una complessità, sapendo che la speranza matematica della somma è la somma delle speranze matematiche e che la complessità $T$ dipende da tanto da $n$ quanto da una $n$-upla aleatoria $\mathcal{X}$ corrispondente alle $n$ estrazioni di numeri casuali che sono state effettuate, per cui:
\[E \left[T(n,\mathcal{X})\right] = \Theta(n) + \sum_{i=0}^{n-1} E \left[T_i(\mathcal{X})\right]\]
in cui figura $\Theta(n)$ dal momento che il primo ciclo for presenta una complessità lineare (essendo $n$ iterazioni); inoltre, è necessario eseguire la somma delle speranze matematica del costo di ordinamento del bucket $i$-esimo tramite insertion-sort, che è naturalmente un costo aleatorio che dipende da $\mathcal{X}$, ossia da quali numeri sono stati estratti e quali di tali numeri sono capitati nel bucket $i$-esimo. Dal momento che la complessità di insertion-sort, nel caso peggiore, è quadratica, è possibile effettuare una maggiorazione per la quale, al posto della complessità $T_i(\mathcal{X})$ si inserisce $\gamma_i \cdot N_i^2$, ove $\gamma_i$ è la costante che definisce la classe $\Theta(N_i^2)$ (in cui figura $N_i^2$ dal momento che insertion-sort viene applicato al bucket $i$-esimo, supponendo che la complessità sia la peggiore, quindi quadratica), ovvero:
\[\Theta(n) + \sum_{i=0}^{n-1} E \left[T_i(\mathcal{X})\right] \leq \Theta(n) + \sum_{i=0}^{n-1} E \left(\gamma_i \cdot N_i^2\right) = \Theta(n) + \sum_{i=0}^{n-1} \gamma \cdot E(N_i^2)\]
avendo sostituito $\gamma_i$ con il massimo tra tutti i $\gamma_i$, ossia $\gamma$. Per cui, nel caso peggiore, siccome la speranza matematica di $\mathcal{X}^2$ è minore di $2$, è facile capire come
\[\sum_{i=0}^{n-1} \gamma \cdot E(N_i^2) \leq n \cdot (\gamma \cdot 2)\]
avendo eseguito una somma che coinvolge $n$ addendi ciascuno $\leq \gamma \cdot 2$. Attraverso tale approccio si può ottenere la seguente maggiorazione della speranza matematica della complessità:
\[\Theta(n) + \gamma \cdot E(N_i^2) \leq \Theta(n) + n \cdot (\gamma \cdot 2)\]
Ma ma essendo anche $n \cdot (\gamma \cdot 2)$ una funzione lineare in $n$, si può concludere che la \textbf{speranza matematica della complessità si bucket-sort} è \textbf{lineare}, da cui:
\[\boxed{E \left[T(n,\mathcal{X})\right]=\Theta(n)}\]
che è un risultato straordinario, pur non avendo affermato che la complessità sia lineare.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che, siccome alla fine di bucket-sort si è provveduto a concatenare tutte le liste sequenziali puntate dalle posizioni dell'array $B$, ottenenendo una lista lunga $n$, eseguendo uno sforzo $\Theta(n)$ è anche possibile trasformare tale lista in un array di lunghezza $n$ con tutti i valori ordinati.\\
In realtà, se l'obiettivo finale è quello di ottenere un array, non serve nemmeno concatenare fra di loro tutte le liste: avendo la loro lunghezza (tramite i contatori), è sufficiente provvedere direttamente a inserire ciascuna sottolista all'interno dell'array, riservando un numero di celle consecutive già noto.

\vspace{1em}
\subsection{Riassunto degli algoritmi di ordinamento}
Al di là degli algoritmi di ordinamento bubble-sort e insertion-sort, un primo importante algoritmo di ordinamento è merge-sort, il quale presenta una complessità nel caso peggiore log-lineare, e che prevede di adottare un approccio \emph{dividi et impera}, in cui, tuttavia, le operazioni che vengono eseguite sono in serie, e non in parallelo. La complessità, di fatto, di merge sort, ad un passo generico $n$ è
\[T(n)=2 \cdot T \left(\frac{n}{2}\right) + f(n)\]
in cui la funzione $f(n)$ non è nota a priori, ma è nota solamente la sua classe $\Theta$.\\
Applicando il teorema maestro, si ha che
\[T(n) = 2 \cdot T \left(\frac{n}{2}\right) + n\]
e dividendo ambo i membri per $n$ si ottiene che
\[\frac{T(n)}{n} = \dfrac{T \left(\dfrac{n}{2}\right)}{\dfrac{n}{2}} + 1\]
e nel caso successivo (siccome ogni volta si divide l'array da ordinare in due parti) si ottiene che
\[T \left(\frac{n}{2}\right) = 2 \cdot T \left(\frac{n}{4}\right) + \frac{n}{2}\]
e dividendo per $\frac{n}{2}$ si ottiene:
\[\dfrac{T \left(\dfrac{n}{2}\right)}{\dfrac{n}{2}} = \dfrac{T \left(\dfrac{n}{4}\right)}{\dfrac{n}{4}} + 1\]
e così via, fino ad arrivare a
\[\frac{T(2)}{2}=T(1)+1\]
in cui progressivamente si esprime il termine a destra dell'uguaglianza precedente con il termine successivo; infatti, nella prima uguaglianza si ha
\[\frac{T(n)}{n} = \dfrac{T \left(\dfrac{n}{2}\right)}{\dfrac{n}{2}} + 1\]
ma $\frac{T \left(\frac{n}{2}\right)}{\frac{n}{2}}$ è noto essere
\[\dfrac{T \left(\dfrac{n}{2}\right)}{\dfrac{n}{2}} = \dfrac{T \left(\dfrac{n}{4}\right)}{\dfrac{n}{4}} + 1\]
e così via. Per cui, dividendo $n$ volte per $2$ si ottiene che
\[\frac{T(n)}{n}=T(1)+\log_2(n)\]
in cui $\log_2(n)$ deriva dalla somma di tutti gli $1$ che si ottengono sostituendo ad ogni espressione l'espressione successiva. Moltiplicando ambo i mmembri per $n$ si ottiene che
\[T(n)= c \cdot n + n \cdot \log(n) = n \cdot \log(n)\]
ottenendo una complessità loglineare, essendo prevalente la complessità peggiore nella somma.

\vspace{1em}
\noindent
Nel caso di \textbf{heap-sort}, la maggior parte delle operazioni viene svolta da \textbf{heapify}, il quale verifica se viene rispettato il vincolo di maximum-heap, andando a controllare tutti i sottoalberi di ogni nodo.\\
Avendo costruito una catasta con il valore massimo in cima, si procede a sostituirlo con l'ultimo, accorciando la catasta e applicando nuovamente heapify per far rispettare il vincolo, fino ad ottenere un ordinamento sui nodi dell'albero, che viene riflesso sull'array di partenza.\\
La complessità di build-heap è $O(n \cdot \log(n))$, così maggiorata nell'ipotesi che heapify abbia sempre complessità log-lineare. Tuttavia, tale maggiorazione potrebbe essere migliorata, giungendo ad affermare che la complessità è $\Theta(n)$, in quanto in build-heap gli heapify vengono richiamati a partire da vertici prossimi alle foglie, anzi quasi tutti i vertici sono prossimi alle foglie, per cui i sottoalberi che dovranno essere controllati saranno molto più contenuti rispetto alla totalità dell'albero di partenza: ha rilevanza specificare nel dettaglio la complessità di build-heap in quanto esso non viene impiegato solamente in heap-sort.

\newpage
\noindent
\begin{center}
  31 Marzo 2022
\end{center}
La maggiorazione della complessità di \textbf{build-heap} a $O(n \cdot \log(n))$ è esagerata, e sarebbe molto più utile dimostrare che la sua \textbf{complessità} è perfettamente \textbf{lineare}: naturalmente ciò non migliora la complessità di heap-sort che, nel suo complesso, presenta una complessità lineare; tuttavia, build-heap viene utilizzato anche in altri algoritmi, in cui la complessità lineare della procedura è rilevante nei termini della complessità globale.\\
Per farlo, bisogna supporre che nel caso di interesse, la derivata di una somma infinita (al più numerabile) di termini sia la somma delle derivate (che non sempre è vero nel caso di un numero di addendi infinito non numerabile). In particolare si ha che:
\[\sum_{i=0}^{n} q^i = \frac{1-q^{n+1}}{1-q} \hspace{1em} \text{con} \hspace{1em} q \neq 1\]
Se $0<q<1$, quando $n \to +\infty$, tale successione tende ad un valore ben preciso, ossia a
\[\lim_{n \to +\infty} \sum_{i=0}^{n} q^i = \lim_{n \to +\infty} \frac{1-q^{n+1}}{1-q} = \frac{1}{1-q}\]
e se $q=\frac{1}{2}$ allora tale limite è $2$.\\
Allora, derivando tale sommatoria rispetto a $q$ (essendo una somma finita, la derivata della somma è la somma delle derivate), si ottiene
\[\frac{1}{q} \sum_{i=1}^n i \cdot q^i = \frac{(1-q) \cdot (-(n+1) \cdot q^n) + (1-q^{n+1})}{(1-q)^2}\]
E tale derivata, quando $n \to +\infty$ si traduce semplicemente in
\[\frac{1}{q} \sum_{i=1}^\infty i \cdot q^i = \lim_{n \to +\infty} \frac{(1-q) \cdot (-(n+1) \cdot q^n) + (1-q^{n+1})}{(1-q)^2} \longrightarrow \frac{1}{q} \sum_{i=1}^\infty i \cdot q^i = \frac{1}{(1-q)^2}\]
e moltiplicando ambo i membri per $q$ si ottiene
\[\sum_{i=1}^\infty i \cdot q^i = \frac{q}{(1-q)^2}\]
per cui, ancora una volta, se $q=\frac{1}{2}$ allora tale limite è $2$.\\

\vspace{1em}
\noindent
\textbf{Osservazione}: Nell'algoritmo build-heap si deve considerare un albero binario (non necessariamente completo) sul quale viene eseguito heapify per ogni nodo (per ogni sottoalbero, vi sono due iterazioni da compiere: una per verificare se si deve operare, eventualmente fermandosi a questa se non si deve agire, l'altra per iniziare ad operare).\\
Al fine di ottenere una limitazione superiore del numero di iterazioni eseguite all'interno di build-heap, si suppone che l'albero binario sia completo (ovvera vi siano tutti i nodi in ogni livello), avente altezza totale $H$, e per ogni nodo è stato applicato heapify (anche alle foglie, per le quali non seriverebbe eseguire heapify), da cui
\[\# \text{iterazioni} \leq \sum_{h=0}^H 2^{H-h} \cdot (h+1)\]
in cui $2^{H-h}$ sono il numero di foglie presenti al livello $h$ dell'albero, mentre $h+1$ sono le iterazioni che heapify compie per ogni foglia del livello $h$ (dovendo controllare i sottoalberi sottostanti).\\
Naturalmente tale sommatoria può essere riscritta come
\[\sum_{h=0}^H 2^{H-h} \cdot (h+1) = \sum_{h=0}^H 2^{H-h} \cdot h + \sum_{h=0}^H 2^{H-h} = 2^H \cdot \sum_{h=0}^H h \cdot \frac{1}{2^h} + 2^H \cdot \sum_{h=0}^H \frac{1}{2^h}\]
e sicomme si sono ottenute due sommatorie finite, si può maggiorare ulteriormente, supponendo di aggiungere tutti gli infiniti termini per ottenere delle serie infinite, ottenendo, per $q=\frac{1}{2}$:
\[\sum_{h=1}^\infty h \cdot \frac{1}{2^h} = \frac{q}{(1-q)^1}=2\]
essendo la derivata della serie esposta in precedenza. Inoltre si
\[\sum_{h=0}^\infty \frac{1}{2^h} = \frac{1}{1-q}=2\]
che è la serie geometrica di ragione $q=\frac{1}{2}$. Dal momento che il in ambo i casi è $2$, si ottiene la maggiorazione seguente
\[2^H \cdot \underbrace{\sum_{h=0}^H h \cdot \frac{1}{2^h}}_{=2} + 2^H \cdot \underbrace{\sum_{h=0}^H \frac{1}{2^h}}_{=2} \leq 4 \cdot 2^H\]
È noto, inoltre, che l'altezza di un albero binario appartiene alla classe $\Theta(\log_2(n))$, in cui $n$ è il numero dei nodi, da cui si evince che, essendo $H$ l'altezza dell'albero considerato
\[4 \cdot 2^H \cong 4 \cdot 2^{c \cdot \log_2(n)} = \Theta(n)\]
in cui $c$ è la costante superiore che deriva dalla definzione di classe $\Theta(\log_2(n))$. Ecco che allora si evince come la \textbf{complessità di build-heap} sia esattamente \textbf{lineare}.

\vspace{1em}
\noindent
\textbf{Osservazione}: Nel caso dell'algoritmo quick-sort, come in merge-sort, si costruisce un albero che viene percorso dall'estrema destra fino in basso; tuttavia, la percorrenza dell'albero, a differenza di merge-sort, avviene in maniera irregolare; dal punto di vista operativo, invece, si considera un perno (o pivot) (scelto arbitrariamente) e due confini mobili identificati da due indici: l'indice $i$ divide i valori minori o uguali al perno da quelli maggiori, mentre l'indice $j$ divide i valori maggiori o uguali al perno da quelli che ancora non sono stati confrontati.\\
Al termine del sorting, il perno viene posizionato nella sua posizione definitiva (ossia nella posizione successiva a $i$), ripetendo tale procedura, in maniera ricorsiva, per ciascuna delle due parti in cui l'array da ordinare viene diviso. È chiaro che se la ripartizione avviene sempre in maniera squilibrata e non omogenea (come quando l'array è già ordinato), allora la complessità di quick-sort diventa quadratica (naturalmente nel caso peggiore).\\
La complessità di quick-sort, nel caso medio, invece, è log-lineare: per dimostrarlo, si sarebbe dovuto supporre che i numeri da ordinare fossero tutti diversi fra di loro e calcolare la somma delle complessità di ognuna delle $n!$ permutazioni di $n$ valori, moltiplicata per la probabilità che tale permutazione si presenti (ossia $\frac{1}{n!}$) e diviso per il numero delle permutazioni (pari a $n!$).\\
Tuttavia, una strada alternativa, ma ugualmente valida, è stata quella adottata, per la quale si prevedeva, di volta in volta, di estrarre casualmente il perno, dimodochè nessuna permutazione fosse favorita rispetto alle altre, e tramite il calcolo della speranza matematica, si è ottenuto che la complessità media di quick-sort è proprio log-lineare (mentre quella empirica è ottima). Inoltre, tale algoritmo per confronti, essendo un algoritmo di ordinamento generale, presenta una complessità nel caso generale e nel caso peggiore almeno log-lineare.

\vspace{1em}
\noindent
\textbf{Osservazione}: Passando ad algoritmi di ordinamento basati su strutture dati in ingresso ben precise, si è osservato che \textbf{counting-sort} presenta una complessità lineare: naturalmente, tale algoritmo si applica quando $k << n$ (anche se si potrebbe scrivere che $k = O(n)$, pur essendo una notazione fuorviante), tale per cui vi saranno da ordinare molti numeri uguali, con molte ripetizioni.\\
La prima fase dell'algoritmo prevede di inizializzare un array $C$ di $k$ posizioni a $0$, per poi effettuare $n$ iterazioni all'interno dell'array $A$ per contare quanti valori sono più piccoli di ciascuna posizione di $C$: lo sforzo compiuto è, naturalmente, dato da $\Theta(n+k)$.\\
Effettuando, poi, un nuovo ciclo di $n$ iterazioni (anche se al contrario, da $n$ a $1$, è irrilevante), si procede ad eseguire l'ordinamento non in loco, ma su un nuovo array $B$, andando progressivamente a decrementare, nell'array $C$, il contatore del numero di valori ancora disponibili, minori o uguali ad ogni posizione dell'array $C$.

\vspace{1em}
\noindent
\textbf{Osservazione}: L'algoritmo bucket-sort permette di ordinare valori reali compresi in un intervallo $[0,1]$ (o in qualsiasi intervallo, per quanto si è visto in precedenza); ovviamente, si suppone che i valori da ordinare siano distribuiti in maniera uniforme all'interno dell'intervallo $[0,1]$, ovvero i sottointervalli di lunghezza $\delta$ sono pressoché identici fra di loro.\\
Ovviamente il fatto di considerare un intervallo continuo $[0,1]$ è una mera approssimazione, dal momento che la precisione di rappresentazione per un calcolatore è sempre finita.\\
Il calcolo che è stato compiuto in precedenza, durante l'analisi dell'algoritmo, non serviva a determinare la complessità media di quick-sort; anzi, in prima analisi sono stati estratti, in maniera uniforme e casuale,  dei valori arbitrari nell'intervallo prescelto e poi è stata determinata la speranza matematica della complessità, permettendo di compensare l'ipotesi iniziale di uniforme distribuzione dei valori di partenza nell'intervallo prescelto. Tale calcolo ha permesso di pervenire ad un risultato molto favorevole: all'interno di ogni bucket ci si aspetta \textbf{tipicamente} di avere $1$ solo dei valori di partenza, come mostrato di seguito
\begin{flalign*}
   & \hspace{1.6em} A \hspace{6em} B\\
  \boxed{1} & \hspace{1em} \boxed{0.78} \hspace{2em} \boxed{0} \hspace{1em} \boxed{B[0]} \longrightarrow \boxed{0.17}\\
  \boxed{2} & \hspace{1em} \boxed{0.17} \hspace{2em} \boxed{1} \hspace{1em} \boxed{B[1]} \longrightarrow \boxed{0.39} \longrightarrow \boxed{0.26}\\
  \boxed{3} & \hspace{1em} \boxed{0.39} \hspace{2em} \boxed{2} \hspace{1em} \boxed{\text{NIL}}\\
  \boxed{4} & \hspace{1em} \boxed{0.26} \hspace{2em} \boxed{3} \hspace{1em} \boxed{B[3]} \longrightarrow \boxed{0.78}\\
  \boxed{5} & \hspace{1em} \boxed{0.94} \hspace{2em} \boxed{4} \hspace{1em} \boxed{B[4]} \longrightarrow \boxed{0.94}
\end{flalign*}
\noindent
All'interno dell'array $B$ saranno contenuti dei puntatori a delle liste concatenate e sequenziali: ogni valore dell'array $A$, a seconda del proprio valore, deve essere inserito all'interno di uno specifico bucket, secondo la regola seguente:
\[B \left[ \left \lfloor n \cdot A[j] \right \rfloor \right]\]
Naturalmente, ciascun bucket dovrà essere ordinato per mezzo di un algoritmo di ordinamento che funziona particolarmente bene quando i valori da ordinare sono ridotti in numero, in forza dell'uniforme distribuzione presa in considerazione (come \emph{insertion-sort}).\\
Alla fine, concatenando fra di loro tutte le liste di bucket, si ottiene l'array di valori perfettamente ordinamento; naturalmente, anziché concatenare le liste, siccome le liste considerate presentano dei contatori che permettono di valutare il numero di elementi in esse contenute, sarebbe possibile anche creare un array di lunghezza pari alla somma di tutti i contatori in cui inserire i valori di ogni lista, pagando sempre un prezzo $\Theta(n)$ (con $n$ lunghezza dell'array).

\newpage
\section{Algoritmi sui grafi}
L’origine storica della teoria dei grafi viene fatta risalire al 1736, anno in cui il matematico svizzero Eulero risolse il problema dei ponti di K\"{o}nigsberg.\\
Le prime applicazioni della teoria risalgono a Kirchoff nel 1847, in relazione allo studio delle reti elettriche e a Kerli, dieci anni dopo, nello studio di problemi di chimica organica. A partire dagli anni $'40$, a seguito dell'avvento del computer, la teoria ha conosciuto uno sviluppo impetuoso, sollecitato da numerose e importanti applicazioni che riguardan la ricerca operativa, computer science, reti di calcolatori e di comunicazione, statistica, cluster analysis e biologia.

\vspace{1em}
\noindent
\subsection{Problema di Eulero}
Per quanto si è detto, Eulero fu il primo ad introdurre il concetto di grafo per la risoluzione del problema dei ponti di K\"{o}nigsberg: tale città era attraversata dal fiume Pregel, che la divideva in quattro parti: due aree principali (A e B) e due isole (C e D). Le quattro zone della città erano collegate tra loro da sette ponti, come mostrato in Figura \ref{fig:problema_eulero_ponti}.
Ovviamente, si usa l'imperfetto, in quanto all'epoca di Eulero la città di K\"{o}nigsberg apparteneva alla Prussia orientale, mentre attualmente appartiene alla Russia ed è nota con il nome di Kaliningrad e, a seguito dei bombardamenti della Seconda Guerra Mondiale, alcuni dei suoi antichi ponti non esistono più oggi.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$A$};
    \node[main node] (b) [below of=a] {$B$};
    \node[main node] (c) [above of=a] {$C$};
    \node[main node] (d) [right of=a] {$D$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge [bend left, out=60, in=120, out looseness=1, in looseness=1] node[left] {$c$} (c)
      (a) edge [bend left, out=-60, in=240, out looseness=1, in looseness=1] node[left] {$d$} (c)
      (b) edge [bend left, out=60, in=120, out looseness=1, in looseness=1] node[left] {$a$} (a)
      (b) edge [bend left, out=-60, in=240, out looseness=1, in looseness=1] node[left] {$b$} (a)
      (a) edge node[above] {$e$} (d)
      (c) edge [bend left, out=50, in=140, out looseness=1, in looseness=1] node[left] {$g$} (d)
      (b) edge [bend left, out=-50, in=-140, out looseness=1, in looseness=1] node[left] {$f$} (d);
  \end{tikzpicture}
  \caption{Problema di Eulero e dei ponti di K\"{o}nigsberg}
  \label{fig:problema_eulero_ponti}
\end{figure}

\noindent
Il compito che Eulero si era prefisso era quello di organizzare un \textbf{percorso chiuso} che impieghi tutti i ponti una e una sola volta; dopo i suoi studi, Eulero pervenne ad una soluzione molto chiara: \textbf{non esiste} alcun percorso chiuso di tale tipo.\\
Osservando il grafo (che non è un grafo semplice, ma un multigrafo) rappresentato in Figura \ref{fig:problema_eulero_ponti}, il collegamento tra i nodi $A$ e $C$ e tra i nodi $A$ e $B$ presenta una molteplicità doppia: in un grafo semplice non è possibile avere una molteplictià $\geq 2$.\\
Inoltre in un multigrafo è possibile avere dei \textbf{loop} (o \emph{cappi}), ossia degli archi chiusi su uno stesso nodo, mentre in un grafo semplice no.\\
La definizione di \textbf{grado} (o \textbf{valenza}, dall'inglese \emph{degree}) di un vertice è presto detta:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{GRADO DI UN VERTICE}}\\
    \parbox{\linewidth}{Il \textbf{grado} di un vertice è il numero di archi che incidono sul vertice considerato; per esempio, in Figura \ref{fig:problema_eulero_ponti}, si ha che $\text{deg}(C)=3$.\vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\newpage
\noindent
Mentre la definizione di \textbf{grafo connesso} è:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{GRAFO CONNESSO}}\\
    \parbox{\linewidth}{Un grafo, inoltre, si dice \textbf{connesso} se esiste un percorso di \textbf{archi consecutivi} che permette di collegare fra di loro ogni coppia di vertici.\vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\noindent
E di seguito si espone la definizione di \textbf{grafo euleriano}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{GRAFO EULERIANO}}\\
    \parbox{\linewidth}{Un grafo si dice \textbf{euleriano} se esise un \textbf{percorso chiuso} (ovvero un percorso che parte e termina nel medesimo vertice) che impiega tutti gli archi una e una sola volta.\\
    In particolare, Eulero dimostrò che un grafo è euleriano (ossia esiste un percorso euleriano) \textbf{se e solo se} è connesso e se il \textbf{grado di tutti i vertici è pari}.\vspace{3mm}}\\
    \hline
\end{tabularx}
\vspace{1em}

\noindent
\textbf{Osservazione}: Nell'$800$ \textbf{Fleury} propose un algoritmo che prevede, in prima battuta, una fase decisionale che permette di determinare se il grafo è euleriano o meno (grazie alla dimostrazione di Eulero, è facile verificare se il grafo è connesso e calcolare il grado di ogni vertice al fine di determinare se questo è pari).\\
Dopodichè l'algoritmo permette di determinare almeno un percorso euleriano, dal momento che ve ne potrebbero essere molteplici. La velocità d'esecuzione dell'algoritmo è significativa, in quanto sta nell'ordine del numero dei vertici e degli archi, ossia ha una complessità lineare.

\vspace{1em}
\noindent
\subsection{Algoritmo di Hamilton}
Il problema di Hamilton, invece, è il seguente: partendo da un vertice, si costruisca un percorso chiuso che unisca tutti i vertici una e una sola volta; se un percorso di tale tipo esiste, allora il grafo si dice \textbf{hamiltoniano}.\\
È chiaro che tale problema riguarda un grafo semplice, e non un multigrafo, in quanto avere molteplicità maggiore di $1$ non è utile alla risoluzione del problema, dal momento che scegliere un arco piuttosto che un altro per collegare due nodi equivale sempre al collegamento di due nodi.\\
Inoltre, verificare la correttezza di una soluzione al problema è molto semplice, mentre la determinazione di una soluzione del problema è molto più complessa: si dice, infatti, che l'algoritmo è $np$ \textbf{completo}, ovvero la risoluzione del problema non può che avvenire tramite algoritmi di complessità esponenziale (praticamente inutilizzabile); esattamente come per vertex-cover (o il problema del commesso viaggiatore), il quale è $np$ completo, ci si deve accontentare di algoritmi approssimati.

\vspace{1em}
\noindent
\subsection{Teorema delle strette di mano - Handshake theorem}
Si consideri un multigrafo, tale da includere:
\begin{itemize}
  \item molteplicità $\geq 2$ per i collegamenti tra i vertici (in quanto è possibile stringere le mani pià volte);
  \item loop (o cappi) per i vertici (in quanto una persona può stringersi le mani da solo).
\end{itemize}
In quest'ultimo caso, è fondamentale osservare come un loop, ossia un arco chiuso in uno stesso nodo, contribuisce di $2$ al grado del vertice (in quanto è come se vi fossino due archi che incidono sul medesimo vertice).

\newpage
\noindent
Si consideri, allora, il grafo seguente:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$B$};
    \node[main node] (b) [below left of=a] {$A$};
    \node[main node] (c) [below right of=a] {$C$};
    \node[main node] (d) [below of=a] {$D$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {$\alpha$} (b)
      (a) edge node [left] {$\beta$}  (c)
      (a) edge [bend left, out=-60, in=240, out looseness=1, in looseness=1] node[left] {$\gamma$} (c);
  \end{tikzpicture}
  \caption{Problema delle strette di mano}
  \label{fig:problema_strette_mano}
\end{figure}

\noindent
Al fine di analizzare tale grafo, si scrivano i vertici in colonna tante volte quant'è il loro grado:
\begin{flalign*}
  \boxed{A} \hspace{1em} - \hspace{1em} \boxed{\alpha}\\
  \boxed{B} \hspace{1em} - \hspace{1em} \boxed{\alpha}\\
  \boxed{B} \hspace{1em} - \hspace{1em} \boxed{\beta}\\
  \boxed{B} \hspace{1em} - \hspace{1em} \boxed{\gamma}\\
  \boxed{C} \hspace{1em} - \hspace{1em} \boxed{\beta}\\
  \boxed{C} \hspace{1em} - \hspace{1em} \boxed{\gamma}\\
\end{flalign*}
Analizzando la colona di sinistra, è facile capire che essa rappresenta la somma del grado di tutti i vertici, ossia
\[\sum_{i=1}^n \text{deg}(v_i)\]
Invece, se si considera la colonna di destra, è immediato capire come in essa ogni arco compaia due volte, in quanto presenta due estremi, ossia
\[2 \cdot \vert \xi \vert\]
Pertanto, il teorema delle afferma che \textbf{la somma del grado dei vertici è sempre pari al doppio del numero di archi}:
\[\boxed{\sum_{i=1}^n \text{deg}(v_i) = 2 \cdot \vert \xi \vert}\]
che permette anche di capire come la somma dei gradi sia sempre un numero pari.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che in un ipercubo $n$-dimensionale, i vertici presentano coordinate binarie di $n$ bit (per $n=3$ si ha $000$, $001$ e così via fino a $111$): pertanto, è facile capire come i vertici sono in tutto $2^n$ (nel caso di $n=3$, i vertici sono $2^3=8$).

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \draw[thick](2,2,0)--(0,2,0)--(0,2,2)--(2,2,2)--(2,2,0)--(2,0,0)--(2,0,2)--(0,0,2)--(0,2,2);
    \draw[thick](2,2,2)--(2,0,2);
    \draw[gray,dashed](2,0,0)--(0,0,0)--(0,2,0);
    \draw[gray,dashed](0,0,0)--(0,0,2);

    \draw[gray!20](-0.5,-0.05,-0.2) node{010};
    \draw(-1.25,1.25,0)             node{001};
    \draw(-1.25,-0.75,0)            node{000};
    \draw(-1.35,1.2,-2.1)           node{011};
    \draw(2.4,-0.05,-0.2)           node{110};
    \draw(1.8,-0.75,-0.2)           node{100};
    \draw(1.7,1.2,-2)               node{111};
    \draw(1.7,1.25,0)               node{101};
  \end{tikzpicture}
  \caption{Ipercubo tridimensionale}
  \label{fig:ipercubo_tridimensionale}
\end{figure}

\noindent
Per quanto concerne gli spigoli, è fondamentale osservare come ogni spigolo collega due vertici le cui coordinate differiscono solamente per un bit (per esempio, i vertici $001$ e $011$ sono collegati fra di loro da uno spigolo): ciò permette di capire come, dato un vertice, è possibile creare al massimo $n$ collegamenti da esso (ovvero vi sono $n$ vertici ad esso adiacenti), cambiando di volta in volta una coordinata, per cui il numero totale degli spigoli è
\[\frac{2^n \cdot n}{2} = 2^{n-1} \cdot n\]
Da tale ragionamento si evince come il massimo della distanza fra due vertici è pari a $n$ (in quanto è sufficiente alterare ad uno ad uno gli $n$ bit di rappresentazione di ogni vertice). Se la rappresentazione di un ipercubo $n$-dimensionale riguardasse il collegamento tra $2^n$ vertici al fine di garantirne la comunicazione, con tale soluzione si avrebbe che alcune linee di comunicazione sarebbero eccessivamente occupate, favorendone delle altre.\\
Per la risoluzione del problema dell'eccessivo traffico su alcune linee, si potrebbe estrarre del tutto a caso un nuovo nodo il quale farà da tramite nella comunicazione tra i due nodi collegati da una linea eccessivamente trafficata: per esempio, dati $A$ e $B$ due nodi che comunicano frequentemente, è possibile estrarre un terzo nodo $C$ che permetta ad $A$ e a $B$ di comunicare, ma indirettamente, tramite il collegamento $A-C-B$.\\
Tale approccio, chiaramente, potrebbe raddoppiare la distanza del collegamento di comunicazione, ma tramite un semplice calcolo di probabilità si riuscirebbe agevolmente a capire come con tale approccio il traffico si distribuirebbe in maniera uniforme su tutta la rete.

\vspace{1em}
\noindent
\textbf{Osservazione}: Il numero degli archi in un grafo semplice è sempre
\[0 \leq \vert \xi \vert \leq \frac{k \cdot \left(k-1\right)}{2}\]
con $k$ il numero dei vertici.
Nell'ipotesi in cui il grafo dovesse essere connesso, è facile capire che se esso è completo, allora è anche connesso, per cui la limitazione di destra permane inalterata. Inoltre, si dimostrerà che un grafo connesso conterrà sempre un albero al suo interno ed è noto che un albero presenta un numero di rami pari al numero di vertici meno $1$ (basta, infatti, eliminare ad uno ad uno i rami e le conseguenti foglie attaccate per accorgersene, in cui l'unica foglia che rimarrà in vita sarà la radice). Pertanto si ottiene che, in un \textbf{grafo connesso} il numero degli archi è pari a
\[\Theta(\vert v \vert) = \vert v \vert - 1 \leq \vert \xi \vert \leq \frac{\vert v \vert \cdot \left(\vert v \vert - 1\right)}{2} = \Theta(\vert v \vert^2)\]

\newpage
\noindent
\begin{center}
  1 Aprile 2022
\end{center}
Si dimostri che se due funzioni definitivamente infinite appartengono alla stessa classe di complessità $\Theta$, allora anche i loro logaritmi appartengono alla medesima classe $\Theta$.\\
In altre parole, è noto che se $f(x) = \Theta(g(x))$, allora $\exists c_1,c_2,\widetilde{x} \in \mathbb{R}^+$ tali che
\[\hspace{1em} c_1 \cdot g(x) \leq f(x) \leq c_2 \cdot g(x) \hspace{1em} \forall x \geq \widetilde{x}\]
Allora applicando i logaritmi a tale diseguaglianza si ottiene che
\[\log(c_1) + \log(g(x)) \leq \log(f(x)) \leq \log(c_2) + \log(g(x))\]
Per cui è sufficiente considerare
\[\frac{\log(g(x))}{2} \leq \log(f(x)) \leq 2 \cdot \log(g(x))\]
essendo $g(x)$ infinita e conseguentemente anche $\log(g(x))$ è definitivamente infinito. Pertanto, $\log(g(x)) \geq \log(c_1)$ definitivamente, per qualche $x \geq \widetilde{\widetilde{x}}$.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si consideri una coda, allora il metodo di inserimento di un nodo $x$ all'interno della coda $Q$ (inizialmente eventualmente vuota, ossia $Q=\varnothing$) prende il nome di ENQUEUE($x$,$Q$), mentre $x=$DEQUEUE($Q$) restituisce il nodo cancellato.\\
Nell'utilizzo della coda, in tale corso, si farà sempre in modo che la sua lunghezza sia sempre inferiore (o eventualmente uguale) a $n$. Inoltre, è possibile anche impiegare un array per realizzare una \textbf{coda circolare}, in cui l'ultimo elemento punta al primo, come mostrato di seguito, nel caso di una coda di lunghezza massima $12$:
\[Q=\overset{1}{\boxed{\text{NIL}}}\boxed{\text{NIL}}\boxed{\textcolor{white}{I}...\textcolor{white}{I}}\underset{7}{\boxed{\textcolor{white}{I}15\textcolor{white}{I}}}\boxed{\textcolor{white}{I}6\textcolor{white}{I}}\boxed{\textcolor{white}{I}9\textcolor{white}{I}}\boxed{\textcolor{white}{I}8\textcolor{white}{I}}\underset{11}{\boxed{\textcolor{white}{I}4\textcolor{white}{I}}}\overset{12}{\boxed{\text{NIL}}}\]
in cui è immediato capire come la testa della coda (dall'inglese \emph{head}) sia in corrispondenza del valore $15$ (in posizione $7$), mentre la coda della stessa (dall'inglese \emph{tail}) è in corrispondenza dell'ultimo NIL (in posizione $12$).\\
Quando la coda è vuota (ossia $Q=\varnothing$) la testa e la coda coincidono, ovvero $h=t=1$. Al fine di accedere ai diversi elementi dell'array è necessario eseguire una \textbf{somma circolare} $\oplus$ (ovvero la somma di due posizioni modulo la lunghezza dell'array, in questo caso $\% 12$):
\[10 \oplus 4 = 14 \% 12 = 2\]
con la sola eccezione che prevede di sostituire con $12$ l'eventuale resto $0$ di tale operazione. Per esempio, nel caso seguente
\[Q=\overset{1}{\boxed{\text{NIL}}}\boxed{\text{NIL}}\boxed{\textcolor{white}{I}...\textcolor{white}{I}}\underset{7}{\boxed{\textcolor{white}{I}15\textcolor{white}{I}}}\boxed{\textcolor{white}{I}6\textcolor{white}{I}}\boxed{\textcolor{white}{I}9\textcolor{white}{I}}\boxed{\textcolor{white}{I}8\textcolor{white}{I}}\underset{11}{\boxed{\textcolor{white}{I}4\textcolor{white}{I}}}\overset{12}{\boxed{\text{NIL}}}\]
l'operazione ENQUEUE($x$,$Q$) prevede di inserire il nodo $x$ nell'ultima posizione (ovvero $12$), provvedendo ad incrementare la posizione della coda:

\begin{algorithm}[H]
  \caption{ENQUEUE($x$,$Q$)}
  \begin{algorithmic}[1]
    \State $Q[\text{tail}]=x$
    \State $\text{tail}=\text{tail} \oplus 1$
  \end{algorithmic}
\end{algorithm}

\noindent
Naturalmente, nel caso preso in esame, $\text{tail} \circ 1 = (12+1) \% 12=1$.\\
Se, invece, si deve cancellare un nodo in testa alla coda, si usa l'operazione DEQUEUE, la quale prevede di restituire il nodo in testa e incrementare la posizione della testa:

\begin{algorithm}[H]
  \caption{DEQUEUE($Q$)}
  \begin{algorithmic}[1]
    \State $x=Q[\text{head}]$
    \State $\text{head}=\text{head} \oplus 1$
    \State \textbf{return} $x$
  \end{algorithmic}
\end{algorithm}

\noindent
Per conoscere la lunghezza $l$ della coda, invece, nel caso in cui $h \leq t$, sarà semplicemente data da $l=t-h$, in caso contrario si ha $l=n-h+t$.\\
Naturalmente, nell'utilizzo di tale sistema, si dovrebbero controllare i due casi di \emph{underflow} (che si ha quando si deve eseguire DEQUEUE($Q$) con $Q=\varnothing$ e, quindi, $l=0$) e \emph{overflow} (che si ha quando si deve eseguire ENQUEUE($x$,$Q$) quando $l=n$).\\
Tuttavia, nell'utilizzo di tale sistema, all'interno del corso, non si verificherà nessuno dei due eventi.

\vspace{1em}
\noindent
\textbf{Osservazione}: Parlando di grafi e dovendo inserire all'interno di una coda i vertici degli stessi, è facile capire come tale coda non conterrà mai, al suo interno, un numero maggiore di $n$ nodi.\\
Inoltre, mentre nel caso degli algoritmi di ordinamento, la complessità di descrizione dell'input è dettata solamente da $n$ (con $n$ pari al numero dei valori da ordinare), ignorando completamente altri fattori di complessità (come, per esempio, la definizione del numero di cifre decimali di ogni numero da ordinare): la formulazione più corretta, in tal senso, è \textbf{complessità in funzione di $\boldsymbol{n}$} (e non \emph{complessità assoluta}, o \emph{bit-complexity}).\\
Pertanto, nel caso dei grafi, sarebbe più conveniente provvedere a definire la complessità di inserimento dell'input \textbf{in funzione del numero dei vertici $\boldsymbol{\vert v \vert}$ e del numero degli archi $\boldsymbol{\vert \xi \vert}$}. A tal proposito, è bene ricordare come i grafi semplici
\begin{itemize}
  \item non presentano loop (che contribuiscono di $2$ al grado del vertice coinvolto);
  \item gli archi non hanno una moltpelicità $\geq 2$, ma sempre unitaria o nulla;
  \item gli archi non presentano ordinamenti (ossia se esiste un arco $A-B$, allora esiste anche l'arco $B-A$).
\end{itemize}
Il modo più immediato al fine di descrivere l'input dei vertici e degli archi di uun grafo semplice è quello di impiegare una \textbf{lista di adiacenza}. Per esempio, nel caso del grafo di seguito esposto

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (a) {$A$};
    \node[main node] (b) [right of=a] {$B$};
    \node[main node] (c) [right of=b] {$C$};
    \node[main node] (d) [above of=b] {$D$};

    \path[every node/.style={font=\sffamily\small}]
      (a) edge node [left] {} (d)
      (b) edge node [left] {} (d);
  \end{tikzpicture}
  \caption{Descrizione di un grafo tramite una lista di adiacenza}
  \label{fig:descrizione_grafo_lista_adiacenza}
\end{figure}

\noindent
La lista di adiacenza associata al grafo sarebbe la seguente
\[\boxed{\textcolor{white}{I}a\textcolor{white}{;}}\boxed{\textcolor{white}{I};\textcolor{white}{;}}\boxed{\textcolor{white}{I}d\textcolor{white}{;}}\boxed{\textcolor{white}{I};\textcolor{white}{;}}\boxed{\textcolor{white}{I}b\textcolor{white}{;}}\boxed{\textcolor{white}{I};\textcolor{white}{;}}\boxed{\textcolor{white}{I}d\textcolor{white}{;}}\boxed{\textcolor{white}{I};\textcolor{white}{;}}\boxed{\textcolor{white}{I}c\textcolor{white}{;}}\boxed{\textcolor{white}{I};\textcolor{white}{;}}\boxed{\textcolor{white}{I};\textcolor{white}{;}}\boxed{\textcolor{white}{I}d\textcolor{white}{;}}\boxed{\textcolor{white}{I};\textcolor{white}{;}}\boxed{\textcolor{white}{I}a\textcolor{white}{;}}\boxed{\textcolor{white}{I}b\textcolor{white}{;}}\boxed{\textcolor{white}{I};\textcolor{white}{;}}\]
In cui, semplicemente, dato ogni nodo, si indicano tutti i nodi ad esso adiacenti (ossia collegati tramite un arco); la logica di descrizione è la seguente: si specifica il nodo interessat, si pone un separatore (\quotes{;}), si specificano tutti i nodi ad esso adiacenti e poi si pone un nuovo separatore (\quotes{;}) e dopodichè si procede con il nodo successivo.\\
È immediato capire come la complessità di tale descrizione dell'input (ossia il numero di caselle impiegate) è data da
\[\boxed{3 \cdot \vert v \vert + 2 \cdot \vert \xi \vert}\]
in quanto ogni vertice è presente $3$ volte nella descrizione (una prima volta per specificare quale vertice si sta considerando, un punto e virgola ad indicare che tale vertice è stato visitato e un'ultima volta per indicare che la lista delle adiacenze di tale vertice è terminata), mentre gli archi vengono contati due volte (in quanto uno stesso arco viene considerato nella descrizione di ciascuno dei due vertici che collega); ciò porta a concludere che la complessità di descrizione dell'input, in funzione del numero di vertici e archi, è $\Theta \left(\vert v \vert + \vert \xi \vert \right)$.\\
Nel caso di presenza di cappi sui nodi, giacché questi incidono su un solo vertice, essi vengono contati una sola volta nella descrizione delle adiacenze, per cui si avrebbe che la complessità è
\[3 \cdot \vert v \vert + 2 \cdot \vert \xi \vert - \#\text{loop}\]
Ma si avrebbe sempre che $2 \cdot \vert \xi \vert - \#\text{loop} \geq \vert \xi \vert$, in quanto il numero dei cappi è al massimo pari al numero degli archi, per cui dal punto di vista della notazione $\Theta$ sarebbe esattamente equivalente.\\
Nel caso gli archi presentassero moltiplicità $\geq 2$, allora nella descrizione dell'input, oltre a definire il vertice interessato e la sua adiacenza, ad ogni collegamento dovrà essere associata una nuova informazione che contiene la molteplicità del collegamento, ossia il numero di archi che connettono due stessi nodi; pertanto, il numero degli archi dovrebbe essere raddoppiato, ma ancora una volta tale modifica è assolutamente ininfluente dal punto di vista della notazione $\Theta$. In conclusione, anche combinando tutte e due le casistiche precedentemente esposte, si avrebbe sempre che la complessità della descrizione dell'input per un generico grafo, sia semplice che multigrafo, è
\[\boxed{\Theta(\vert v \vert + \vert \xi \vert}\]

\noindent
\textbf{Osservazione}: Si osservi che nella definizione della complessità di descrizione dell'input per un generico grafo, $\vert \xi \vert$ indica il numero di archi, ignorando la molteplicità, ossia il numero del tipo di collegamenti tra nodi distinti, ovviamente.

\vspace{1em}
\noindent
Un altro modo per descrivere l'input è la \textbf{matrice di adiacenza}, nella quale si pone $1$ solamente quando si ha un arco che collega i nodi considerati, $0$ altrimenti. Nel caso del grafo di Figura \ref{fig:descrizione_grafo_lista_adiacenza} si ha

\begin{table}[H]
  \centering
  \rowcolors{1}{white}{white}
  \setlength{\tabcolsep}{5pt}
  \renewcommand{\arraystretch}{1.3}
  \begin{tabular}{c|c|c|c|c|}
    & $a$ & $b$ & $c$ & $d$\\
    \hline
    $a$ & $0$ & $0$ & $0$ & $1$\\
    \hline
    $b$ & $0$ & $0$ & $0$ & $1$\\
    \hline
    $c$ & $0$ & $0$ & $0$ & $0$\\
    \hline
    $d$ & $1$ & $1$ & $0$ & $0$\\
    \hline
  \end{tabular}
  \caption{Matrice di adiacenza}
  \label{tab:matrice_adiacenza}
\end{table}

\noindent
Ovviamente, nel caso di un grafo semplice, senza cappi e con molteplicità al più unitaria, tale rappresentazione è sovrabbondante: la diagonale principale è ovviamente data da tutti $0$ ed è ovviamente simmetrica.\\
Tuttavia, se vi sono cappi, allora la diagonale principale non sarà tutta nulla e se vi è molteplicità, allora la posto di scrivere $0$ e $1$, si specifica il numero di archi che collegano due nodi. Naturalmente, poi, se gli archi presentano un'ordinamento, ossia sono orientati, allora semplicemente il collegamento $A-B$ verrà denotato con $1$, mentre il collegamento $B-A$ verrà indicato con $0$, supponendo $A$ e $B$ due vertici collegati da un arco che da $A$ va in $B$.\\
Pertanto, la matrice di adiacenza è una struttura estremamente flessibile, grazie alla quale è possibile non solo descrivere gli archi, ma anche i cappi, la molteplicità e l'ordinamento; in questo caso, tuttavia, la complessità è data da $\vert v \vert^2 + 2 \cdot \vert v \vert$ (in quanto oltre alle celle si considerano anche le intestazioni di riga e di colonna), ovvero è
\[\boxed{\Theta(\vert v \vert^2)}\]
Tale evidenza porterebbe ad eliminare la seconda soluzione in favore della prima, in quanto, per esempio, in un albero il numero degli archi è lineare con il numero dei vertici (ovvero è $\vert v \vert - 1$), per cui $\Theta(\vert v \vert + \vert \xi \vert = \Theta(\vert v \vert)$; se, invece, si considera un grafo denso, tale per cui il numero degli archi è
\[\frac{\vert v \vert \cdot \left(\vert v \vert - 1\right)}{2} = \Theta(\vert v \vert^2)\]
ossia è quadratico nel numero dei vertici, la complessità di descrizione dell'input è in ogni caso quadratica: pertanto, nel caso di un \textbf{grafo denso} impiegare l'una o l'altra descrizione è equivalente, mentre nel caso di un \textbf{grafo sparso}, come un albero, è più conveniente, in termini di complessità, impiegare la lista di adiacenza (anche se molto spesso si impiega la matrice di adiacenza in ogni caso, essendo esso più completa ed efficiente della lista di adiacenza).

\vspace{1em}
\subsection{Breadth-First-Search (BFS)}
Si consideri un grafo semplice $\mathcal{G}$ nel quale viene messo in evidenza un vertice, denominato \textbf{source} ($s$); per comodità di comprensione, si può interpretare $\mathcal{G}$ come un grafo connesso, tale per cui da ogni vertice esiste un percorso che porta a qualunque altro vertice.\\
La visita di un grafo in ampiezza viene di seguito esposta tramite l'algoritmo \textbf{breadth-first-search}, il quale prevede di partire dal vertice sorgente $s$ e considerare tutte le adiacenze dei vertici successivi fino a concludere la visita di tutti i vertici del grafo, scoprendo di volta in volta vertici a distanze incrementali e costruendo, di fatto, un albero di radice $s$ che viene inserito all'interno del grafo che, se il grafo è connesso, diviene un \textbf{albero ricoprente} (dall'inglese \emph{spanning tree}); per tale ragione la visita in ampiezza può essere impiegata per controllare se un grafo è connesso.\\
Al fine di semplificare la visualizzazione delle operazioni, si considera la seguente notazione:
\begin{itemize}
  \item White: vertice mai visitato;
  \item Gray: vertice visitato ma non ancora abbandonato (ovvero bisogna ancora processare il vertice);
  \item Black: vertice visitato e abbandonato.
\end{itemize}
Di seguito si espone la fase preparatoria dell'algoritmo Breadth-First-Search:

\begin{algorithm}[H]
  \caption{Breadth-First-Search}
  \begin{algorithmic}[1]
    \State BREADTH-FIRST-SEARCH($\mathcal{G}$,$s$)
    \State \textbf{for} $u \in v[\mathcal{G}]-\{s\}$
    \Indent
      \State \textbf{do} \emph{color$[u]=$} WHITE
      \Indent
          \State $d[u]=\infty$
          \State $\pi[u]=$ NIL
      \EndIndent
    \EndIndent
    \State \emph{color$[s]=$} GRAY
    \State $d[s]=0$
    \State $\pi[s]=$ NIL
    \State $Q=\varnothing$
  \end{algorithmic}
\end{algorithm}

\noindent
In cui semplicemente si impostano tutti i vertici al colore WHITE (tramite l'istruzione \emph{color$[u]=$} WHITE), tranne la sorgente, sulla quale si inizia ad operare; inoltre, si fa in modo che la distanza di ogni vertice dalla sorgente sia inizialmente infinita ($d[u]=\infty$) e il progenitore di ogni vertice sia NIL ($\pi[u]=$ NIL), eccezion fatta per la sorgente che ha distanza nulla. Inoltre viene anche creata una coda $Q$ nella quale verranno inseriti i vertici, ma che inizialmente è vuota, ossia $Q=\varnothing$ (che se si volesse realizzare tramite un array, si dovrebbe disporre di un array di dimensione $\vert v \vert + 1$, con $\vert v \vert$ numero dei vertici).\\
Taluna è la fase preparatoria, mentre di seguito si espone la continuazione del listato:

\begin{algorithm}[H]
  \caption{Breadth-First-Search}
  \begin{algorithmic}[1]
    \State [...]

    \State ENQUEUE($Q$,$s$)
    \State \textbf{while} $Q \neq \varnothing$
    \Indent
      \State \textbf{do} $u=$ DEQUEUE($Q$)
      \Indent
        \State \textbf{for} $v \in$ Adj[$u$]
        \Indent
          \State \textbf{do} \textbf{if} \emph{color}$[v]=$ WHITE
          \Indent
              \State \textbf{then} \emph{color}$[v]=$ GRAY
              \Indent
                \Indent
                  \State $d[v]=d[u]+1$
                  \State $\pi[v]=u$
                  \State ENQUEUE($Q$,$v$)
                \EndIndent
              \EndIndent
          \EndIndent
        \EndIndent
        \State \emph{color}$[u]=$ BLACK
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\newpage
\noindent
\textbf{Esempio}: Si consideri il grafo seguente:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node,fill=black!25] (s) [] {$s$};
    \node[main node] (a) [above left of=s] {$a$};
    \node[main node] (b) [above = 4em of s] {$b$};
    \node[main node] (c) [above right of=s] {$c$};
    \node[main node] (d) [above = 4em of a] {$d$};
    \node[main node] (e) [above right of=a] {$e$};
    \node[main node] (f) [above right of=b] {$f$};
    \node[main node] (g) [above left of=e] {$g$};
    \node[main node] (h) [above right of=e] {$h$};
    \node[] (s1) [right = 0.5em of s] {$[0,\text{NIL}]$};

    \path[every node/.style={font=\sffamily\small}]
      (s) edge node [left] {} (a)
      (s) edge node [left] {} (b)
      (s) edge node [left] {} (c)
      (a) edge node [left] {} (d)
      (a) edge node [left] {} (e)
      (b) edge node [left] {} (e)
      (b) edge node [left] {} (f)
      (c) edge node [left] {} (f)
      (e) edge node [left] {} (g)
      (e) edge node [left] {} (h);
  \end{tikzpicture}
  \caption{Esempio visita in ampiezza}
  \label{fig:esempio_visita_ampiezza}
\end{figure}

\noindent
Nella fase preliminare, come anticipato, tutti i vertici presentano colore WHITE, hanno distanza infinita (eccezion fatta per la radice, che ha distanza nulla e colore GRAY) e progenitore NIL; inoltre, viene creata una coda $Q=\varnothing$ nella quale viene inserito il nodo $s$.\\
Iniziando ad operare all'interno del ciclo while (fintantochè la coda non si svuota), si estrae il nodo $s$ e si analizza tutta l'adiacenza di $s$, lavorando prima su $a$, poi su $b$ e infine su $c$:
\begin{enumerate}
  \item Siccome il vertice $a$ è ancora WHITE, esso viene colorato di GRAY, la sua distanza dalla radice viene impostata a $d[a]=0+1=1$ e il suo progenitore diviene $\pi[a]=s$, procedendo a mettere in coda $a$ per una sua successiva analisi;
  \item Siccome il vertice $b$ è ancora WHITE, esso viene colorato di GRAY, la sua distanza dalla radice viene impostata a $d[b]=0+1=1$ e il suo progenitore diviene $\pi[b]=s$, procedendo a mettere in coda anche $b$ per una sua successiva analisi;
  \item Siccome il vertice $c$ è ancora WHITE, esso viene colorato di GRAY, la sua distanza dalla radice viene impostata a $d[c]=0+1=1$ e il suo progenitore diviene $\pi[c]=s$, procedendo a mettere in coda anche $c$ per una sua successiva analisi;
\end{enumerate}
Avendo visitato tutta l'adiacenza di $s$, esso viene colorato di BLACK, ottenendo:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node,fill=black!75] (s) [] {$s$};
    \node[main node,fill=black!25] (a) [above left of=s] {$a$};
    \node[main node,fill=black!25] (b) [above = 4em of s] {$b$};
    \node[main node,fill=black!25] (c) [above right of=s] {$c$};
    \node[main node] (d) [above = 4em of a] {$d$};
    \node[main node] (e) [above right of=a] {$e$};
    \node[main node] (f) [above right of=b] {$f$};
    \node[main node] (g) [above left of=e] {$g$};
    \node[main node] (h) [above right of=e] {$h$};
    \node[] (s1) [right = 0.5em of s] {$[0,\text{NIL}]$};
    \node[] (a1) [right = 0.5em of a] {$[1,s]$};
    \node[] (b1) [right = 0.5em of b] {$[1,s]$};
    \node[] (c1) [right = 0.5em of c] {$[1,s]$};

    \path[every node/.style={font=\sffamily\small}]
      (s) edge node [left] {} (a)
      (s) edge node [left] {} (b)
      (s) edge node [left] {} (c)
      (a) edge node [left] {} (d)
      (a) edge node [left] {} (e)
      (b) edge node [left] {} (e)
      (b) edge node [left] {} (f)
      (c) edge node [left] {} (f)
      (e) edge node [left] {} (g)
      (e) edge node [left] {} (h);
  \end{tikzpicture}
  \caption{Esempio visita in ampiezza dopo un primo ciclo while}
  \label{fig:esempio_visita_ampiezza_1}
\end{figure}

\noindent
Procedendo, siccome la coda $Q=\{a,b,c\}$ non è vuota, si estrae il nodo $a$ e si analizza tutta la sua adiacenza, procedendo prima con $d$, poi con $e$ e infine con $s$:
\begin{enumerate}
  \item Siccome il vertice $d$ è ancora WHITE, esso viene colorato di GRAY, la sua distanza dalla radice viene impostata a $d[d]=1+1=2$ e il suo progenitore diviene $\pi[d]=a$, procedendo a mettere in coda $d$ per una sua successiva analisi;
  \item Siccome il vertice $e$ è ancora WHITE, esso viene colorato di GRAY, la sua distanza dalla radice viene impostata a $d[e]=1+1=2$ e il suo progenitore diviene $\pi[e]=a$, procedendo a mettere in coda anche $e$ per una sua successiva analisi;
  \item Siccome il vertice $s$ è BLACK, esso viene tralasciato;
\end{enumerate}
Avendo visitato tutta l'adiacenza di $a$, esso viene colorato di BLACK, ottenendo:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node,fill=black!75] (s) [] {$s$};
    \node[main node,fill=black!75] (a) [above left of=s] {$a$};
    \node[main node,fill=black!25] (b) [above = 4em of s] {$b$};
    \node[main node,fill=black!25] (c) [above right of=s] {$c$};
    \node[main node,fill=black!25] (d) [above = 4em of a] {$d$};
    \node[main node,fill=black!25] (e) [above right of=a] {$e$};
    \node[main node] (f) [above right of=b] {$f$};
    \node[main node] (g) [above left of=e] {$g$};
    \node[main node] (h) [above right of=e] {$h$};
    \node[] (s1) [right = 0.5em of s] {$[0,\text{NIL}]$};
    \node[] (a1) [right = 0.5em of a] {$[1,s]$};
    \node[] (b1) [right = 0.5em of b] {$[1,s]$};
    \node[] (c1) [right = 0.5em of c] {$[1,s]$};
    \node[] (d1) [right = 0.5em of d] {$[2,a]$};
    \node[] (e1) [right = 0.5em of e] {$[2,a]$};

    \path[every node/.style={font=\sffamily\small}]
      (s) edge node [left] {} (a)
      (s) edge node [left] {} (b)
      (s) edge node [left] {} (c)
      (a) edge node [left] {} (d)
      (a) edge node [left] {} (e)
      (b) edge node [left] {} (e)
      (b) edge node [left] {} (f)
      (c) edge node [left] {} (f)
      (e) edge node [left] {} (g)
      (e) edge node [left] {} (h);
  \end{tikzpicture}
  \caption{Esempio visita in ampiezza dopo un secondo ciclo while}
  \label{fig:esempio_visita_ampiezza_2}
\end{figure}

\noindent
Procedendo in questo modo, alla fine si ottiene il grafo seguente:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node,fill=black!75] (s) [] {$s$};
    \node[main node,fill=black!75] (a) [above left of=s] {$a$};
    \node[main node,fill=black!75] (b) [above = 4em of s] {$b$};
    \node[main node,fill=black!75] (c) [above right of=s] {$c$};
    \node[main node,fill=black!75] (d) [above = 4em of a] {$d$};
    \node[main node,fill=black!75] (e) [above right of=a] {$e$};
    \node[main node,fill=black!75] (f) [above right of=b] {$f$};
    \node[main node,fill=black!75] (g) [above left of=e] {$g$};
    \node[main node,fill=black!75] (h) [above right of=e] {$h$};
    \node[] (s1) [right = 0.5em of s] {$[0,\text{NIL}]$};
    \node[] (a1) [right = 0.5em of a] {$[1,s]$};
    \node[] (b1) [right = 0.5em of b] {$[1,s]$};
    \node[] (c1) [right = 0.5em of c] {$[1,s]$};
    \node[] (d1) [right = 0.5em of d] {$[2,a]$};
    \node[] (e1) [right = 0.5em of e] {$[2,a]$};
    \node[] (f1) [right = 0.5em of f] {$[2,b]$};
    \node[] (g1) [right = 0.5em of g] {$[3,e]$};
    \node[] (h1) [right = 0.5em of h] {$[3,e]$};

    \draw[color=red, ultra thick] (s) -- (a);
    \draw[color=red, ultra thick] (s) -- (b);
    \draw[color=red, ultra thick] (s) -- (c);
    \draw[color=red, ultra thick] (a) -- (d);
    \draw[color=red, ultra thick] (a) -- (e);
    \draw[color=red, ultra thick] (b) -- (f);
    \draw[color=red, ultra thick] (e) -- (g);
    \draw[color=red, ultra thick] (e) -- (h);
    \path[every node/.style={font=\sffamily\small}]
      (b) edge node [left] {} (e)
      (c) edge node [left] {} (f);
  \end{tikzpicture}
  \caption{Esempio visita in ampiezza conclusa}
  \label{fig:esempio_visita_ampiezza_3}
\end{figure}

\noindent
Ecco che alla fine dell'esecuzione dell'algoritmo (a seguito del fatto che $Q=\varnothing$) è stato ottenuto esattamente un albero, in cui si specifica la distanza dalla sorgente e il progenitore per ciascun vertice, in cui sono evidenti i rami d'albero, evidenziati in Figura \ref{fig:esempio_visita_ampiezza_3}.\\
Inoltre, se tutti i vertici sono stati visitati (ovvero nessun vertice presenta ancora l'istanza $\infty$), è stato anche dimostrato che tale grafo è connesso; non solo, ma l'albero che ne deriva è anche il cosiddetto \textbf{spanning tree}, ovvero un albero di visita del grafo che impiega il minor numero di archi (che, nel caso di un albero, è $\vert v \vert - 1$). In altre parole, affinché un grafo sia connesso, si necessità di almeno $\vert v \vert - 1$ archi, al fine di costruire l'albero.

\vspace{1em}
\noindent
\textbf{Osservazione}: Al fine di valutare la complessità di tale algoritmo è necessario impiegare il \textbf{metodo dell'agglomerazione}, il quale permette di determinare il numero di iterazioni di cicli annidati: per esempio, se si considerano due cicli for annidati, il primo di lunghezza $n$ e il secondo di lunghezza $m$, allora il numero totale di iterazioni è $n \times m$; ma se le iterazioni del secondo ciclo for sono variabili, e sono $m_i$ (ossia dipendono da $i$), allora, ovviamente, le iterazioni eseguite sono $m_1+m_2+...+m_n$.\\
Naturalmente, giacché in tale algoritmo il ciclo while esterno esegue un'iterazione per ognuno degli $n$ vertici, mentre il ciclo for interno esegue un numero di iterazioni pari al numero di vertici nell'adiacenza di ogni vertice, è facile capire come il numero di iterazioni totali dell'algoritmo è
\[\boxed{\# \text{iterazioni} = \sum_{i=1}^n \vert \text{Adj}(u_i) \vert}\]
Ma naturalmente, sommare le adiacenze di un vertice significa sommare il grado di ciascun vertice, da cui
\[\boxed{\# \text{iterazioni} = \sum_{i=1}^n \vert \text{deg}(u_i) \vert}\]
Ma è anche noto che la somma dei gradi di ciascun vertice è pari a $2 \cdot \vert \xi \vert$, in quanto per ogni nodo, uno stesso arco viene contato due volte, per cui
\[\boxed{\# \text{iterazioni} = 2 \cdot \vert \xi \vert}\]
Nell'ipotesi in cui il grafo $\mathcal{G}$ sia connesso, è noto che $\vert \xi \vert \geq \vert v \vert - 1$, per cui $\vert \xi \vert = \Omega(\vert v \vert)$. Per cui si ha che
\[\vert v \vert + \vert \xi \vert - 1 \leq 2 \cdot \vert \xi \vert \leq 2 \cdot \left(\vert v \vert + \vert \xi \vert \right)\]
che permette di concludere come la complessità dell'algoritmo sia lineare, ovvero $\Theta(\vert v \vert + \vert \xi \vert)$; ciò, naturalmente, nell'ipotesi in cui il grafo $\mathcal{G}$ sia connesso: se esso non fosse connesso, invece, il numero delle operazioni da eseguire potrebbe diminuire anche drasticamente (in quanto l'algoritmo BFS opera fintantochè c'è un percorso di nodi consecutivi da analizzare), il che porta a concludere come la complessità della visita in ampiezza di un ipotetico grafo $\mathcal{G}$ sia $O(\vert v \vert + \vert \xi \vert)$.

\vspace{1em}
\subsection{Depth-First-Search (DFS)}
Per visualizzare la logica dell'algoritmo di visita dei vertici di un grafo in profondità, si parta dall'ipotesi che il grafo da visitare sia un albero: più precisamente, l'obiettivo è quello di allontanarsi dalla sorgente il prima possibile, andando a percorrere il grafo dalla distanza maggiore, per poi tornare indietro.

\vspace{1em}
\noindent
\textbf{Osservazione}: L'algoritmo di Fleury per la risoluzione del problema di Eulero prevede di verificare se esista un percorso euleriano per visitare tutti i nodi del grafo impiegando tutti gli archi una e una sola volta.\\
Naturalmente, nell'ipotesi in cui il \textbf{grafo sia connesso}, si procede a decrementare tutti i gradi dei vertici ogni volta che si esce o si entra in uno di tali vertici, fintantochè le valenze residue degli stessi non sono tutte nulle. Si capisce, dunque, per costruzione, come tale problema abbia soluzione solo quando \textbf{i gradi dei vertici sono tutti pari} (in quanto entrando e uscendo ogni volta da ogni nodo, il grado diminuisce di $2$, per cui esso deve essere necessariamente pari).\\
In altre parole, alla base dell'algoritmo di Fleury si pone la tecnica che preve di costruire dei mini-cicli euleriani, connettendoli fra di loro fino a che non si esaurisce l'intero grafo (se vi sono valenze ancora non nulle, essendo il grafo connesso, significa che vi sono archi ancora inesplorati).

\newpage
\noindent
\begin{center}
  6 Aprile 2022
\end{center}
La visita in profondità visita l'albero cercando di allontanarsi il più possibile dalla radice e non compie alcun tentativo al fine di determinare la profondità dell'albero, a differenza del metodo di visita per ampiezza.\\
L'algoritmo DFS opera senza specificare l'origine del grafo su cui si sta lavorando, come mostrato di seguito:

\begin{algorithm}[H]
  \caption{Depth-First-Search}
  \begin{algorithmic}[1]
    \State DEPTH-FIRST-SEARCH($G$)
    \State \textbf{for} $u \in v$
    \Indent
      \State \textbf{do} \emph{color$[u]=$} WHITE
      \Indent
          \State $\pi[u]=$ NIL
      \EndIndent

    \State ... continua ...

    \EndIndent
    \State \emph{color$[s]=$} GRAY
    \State $d[s]=0$
    \State $\pi[s]=$ NIL
    \State $Q=\varnothing$

    \State ENQUEUE($Q$,$s$);
    \State \textbf{whilte} $Q \neq 0$
    \Indent
      \State \textbf{do} $u=$ DEQUEUE($Q$)
      \Indent
        \State \textbf{for} $v \in$ Adj[$u$]
        \Indent
          \State \textbf{do} \textbf{if} \emph{color}$[v]=$ WHITE
          \Indent
              \State \textbf{then} \emph{color}$[v]=$ GRAY
              \Indent
                \Indent
                  \State $d[v]=d[u]+1$
                  \State $\pi[v]=u$
                  \State ENQUEUE($Q$,$v$)
                \EndIndent
              \EndIndent
          \EndIndent
        \EndIndent
        \State DEQUEUE($Q$)
        \State \emph{color}$[u]=$ BLACK
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\vspace{1em}
\noindent
Proseguendo con la visita del vertice $u$ si ottiene

\begin{algorithm}[H]
  \caption{Depth-First-Search-Visit}
  \begin{algorithmic}[1]
    \State DEPTH-FIRST-SEARCH-VISIT($u$)
    \State \emph{color$[u]=G$}
    \State \emph{time}$=$\textbf{time}$+1$
    \State d$[u]=time$
    \State \textbf{for} $v \in \text{Adj}[u]$
    \Indent
      \State \textbf{do} \textbf{if} \emph{color$[u]=$} WHITE
      \Indent
          \State $\pi[v]=u$
          \State DFS-Visit$(v)$
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che con una visita in profondità non solo può cambiare la foresta, ma possono cambiare anche gli alberi che popolano la foresta, a seconda di come vengono descritti i nodi del grafo, come mostrato nel seguente esempio:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]%[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (u) {$u$};
    \node[main node] (v) [right of=u] {$v$};
    \node[main node] (w) [right of=v] {$w$};
    \node[main node] (x) [below of=u] {$x$};
    \node[main node] (y) [right of=x] {$y$};
    \node[main node] (z) [right of=y] {$z$};

    \path[every node/.style={font=\sffamily\small}]
      (u) edge node [left]  {} (v)
      (u) edge node [left]  {} (x)
      (v) edge node [right] {} (w)
      (w) edge node [right] {} (z)
      (x) edge node [left]  {} (y)
      (y) edge node [left]  {} (w);
  \end{tikzpicture}
  \caption{Esempio con visita in profondità}
  \label{fig:esempio_visita_profondita}
\end{figure}

\vspace{1em}
\noindent
\textbf{Osservazione}: Nonostante con l'algoritmo di visita in ampiezza si conoscevano le distanze e le profondità, all'interno di tale algoritmo sono presenti comunque dei valori che devono essere opportunamente considerati al fine di ricostruire l'albero di visita.

\vspace{1em}
\subsection{Teorema delle parentesi}
Il teorema delle parentesi prevede di considerare solamente le combinazioni sintatticche aventi significato delle parentesi tonde e graffe: $([])$, $[()]$, $[]()$ e $()[]$.\\
Quando si visita un nodo $u$, si ottiene che
\[(u) < d(v) < f(v) < f(u)\]
e quando si visita $v$, invece, si ottiene che
\[v < d(u) < f(u) < f(v)\]
a cui si associano le parentesi seguenti (considerando $[]$ al nodo $v$ e $()$ al nodo $u$)
\begin{enumerate}
  \item $([])$, che significa che il vertice quadro è discendente dal vertice tondo;
  \item $[()]$, che significa che il vertice tondo è discendente dal vertice quadro.
\end{enumerate}
Naturalmente, si sarebbero potute considerare anche altre due configurazioni: $[]()$ e $()[]$, da cui non si può evincere nessuna relazione di parentela.

\vspace{1em}
\subsection{Teorema del cammin bianco}
Si supponga di considerare due vertici $u$ e $v$, tale per cui $d(u) < d(v)$ (ovvero all'istante $d(u)$, il vertice $v$ non è ancora stato scoperto): allora, se esiste un \textbf{cammmino bianco} tra i due vertici (ovvero esiste un percorso composto solamente da vertici bianchi), allora il teorema afferma che $v$ sarà necessariamente un vertice discendente da $u$.\\
Pertanto, se $v$ è un discendete di $u$, allora necessariamente ad un certo istante di tempo esiste un cammino bianco tra $u$ e $v$; tuttavia non è ovvio il contrario, ed è ciò che afferma il teorema.

\vspace{1em}
\noindent
\textbf{Dimostrazione}: Si supponga che esista un cammino bianco tra $u$ e $v$ e che $v$ non sia, per assurdo, un vertice discendente di $u$, mentre un vertice $w$ tale che $d(u) < d(w) < d(v)$ invece sia un suo discendente; allora, applicando il teorema delle parentesi si evince che
\[d(u) \leq d(w) < f(w) \leq f(u)\]
in cui è necessario porre il segno di minore uguale in quanto non è possibile escludere che il primo vertice discendente di $u$ e diverso da $v$ sia proprio il vertice $u$.\\
Gungendo al vertice $w$, non è determinabile a priori l'istante di tempo in cui si esaminerà l'arco $w-v$; in tale istante di tempo, si possono palesare tre casi possibili
\begin{enumerate}
  \item il colore di $v$ è nero, per cui si ha che $f(v)<f(w)$, per cui $v$ è un discendente di $u$;
  \item il colore di $v$ è grigio, per cui si ha che $d(v)<f(w)$, ma allora $v$ è ancora un discendente di $u$;
  \item il colore di $v$ è bianco, ma allora da $w$ si passa a $v$ e quindi è implicito come $v$ sia un discendete di $u$, essendo un discendente di $w$ che è un discendente di $u$.
\end{enumerate}

\vspace{1em}
\noindent
\textbf{Osservazione}: Si consideri un grafo aciclico, ossia privo di cicli (considerando come ciclo un percorso chiuso che considera due nodi).\\
Allora un grafo di propedeucità è un grafo in cui si ha una gerarchia fra i vertici, ossia non è possibile passare da un vertice a qualsiasi altro, ma bisogna rispettare, appunto, una \textbf{propedeucità}.\\
All'interno di tale grafo, naturalmente, non è possibile considerare dei cicli, perché altrimenti non si potrebbero rispettare le propedeucità. Al fine di visitare la totalità del grafo, è possibile impiegare una visita di profondità:

\vspace{1em}
\noindent
Allora, analizzando tale lista e considerando gli istanti di scoperta dei vertici al contrario si ottiene una sequenza di visita che rispetta tutte le propedeucità previste dal primo grafo.

\vspace{1em}
\noindent
\subsection{Teorema pesante}
Si consideri una visita in profondità e l'analisi di un arco da $u$ in $v$: allora, per il teorema delle parentesi si può avere
\[d(u) < f(u) < d(v) < f(v)\]
la quale non è una configurazione possibile con un collegamento tra $u$ e $v$: prima di terminare l'analisi di $u$ (ossia $f(v)$) è necessario analizzare tutta la giacenza di $u$ e quindi anche lo stesso $v$.\\
Considerando una seconda configurazione
\[d(v) < f(v) < d(u) < f(u)\]
la quale è una configurazione lecita e compatibile, in quanto prima si termina l'analisi di $v$ e poi si scopre ed analizza $u$: pertanto da tale sequenza di scoperta non si può determinare la discendenza di nessuno dei due nodi $u$ e $v$: quest'ultimo è un \textbf{arco di taglio}.\\
Un'ulteriore configurazione è la seguente:
\[d(u) < d(v) < f(v) < f(u)\]
Allora la scoperta dell'arco $u-v$ può avvenire in due possibili situazioni: quando si scopre $u$, per tra $u$ e $v$ si ha un cammino bianco; oppure quando si analizza $v$ e poi $u$, nel qual caso si otterrebbe che $v$ ormai è già stato analizzato e quindi il cammino sia nero.

\vspace{1em}
\noindent
\textbf{Osservazione}: Chiaramente, se il grafo fosse orientato, di tutte le configurazioni compatibili considerate l'unica possibile è la penultima.

\vspace{1em}
\noindent
Ovviamente, un'ultima combinazioni ammissibile dal teorema delle parentesi è la seguente:
\[d(v) < d(u) < f(u) < f(v)\]
allora tale configurazione è ammissibile sia che l'albero sia connesso oppure no, in quanto prima si viene a conoscere il vertice $v$ e solo dopo il vertice $u$.

\newpage
\noindent
\begin{center}
  7 Aprile 2022
\end{center}
Gli archi orientati, ossia i rami degli alberi di una foresta, si pongono alla base della visita in profondità.\\

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (u) {$u$};
    \node[main node] (v) [left of=u] {$v$};
    \node[main node] (w) [left of=v] {$w$};
    \node[main node] (x) [left of=w] {$x$};
    \node[main node] (y) [above of=x] {$y$};
    \node[main node] (z) [right of=y] {$z$};
    \node[main node] (s) [right of=z] {$s$};
    \node[main node] (t) [right of=s] {$t$};

    \path[every node/.style={font=\sffamily\small}]
      (u) edge node [left]  {} (v)
      (v) edge node [left]  {} (w)
      (w) edge node [left]  {} (x)
      (y) edge node [left]  {} (x)
      (z) edge node [left]  {} (y)
      (x) edge node [left]  {} (z)
      (s) edge node [left]  {} (z)
      (t) edge node [left]  {} (v)
      (t) edge node [left]  {} (u)
      (u) edge node [left]  {} (t)
      (z) edge node [left]  {} (w)
      (s) edge node [left]  {} (w)
      (v) edge node [left]  {} (s);
  \end{tikzpicture}
  \caption{Esempio con visita in profondità}
  \label{fig:esempio_visita_profondita_1}
\end{figure}

\vspace{1em}
\noindent
Per l'analisi di tale grafo si parte dal vertice che per primo compare nella descrizione del grafo. Nel caso considerato, si considera come $1^\circ$ il vertice $s$, giacché la descrizione è $s \vert z$; passando a $z$ si scopre tale vertice per $2^\circ$; conoscendo la descrizione $x \vert y$ si procede a scoprire $y$ per $3^\circ$; seguendo la descrizione $y \vert x$ si scopre il nodo $x$ per $4^\circ$; dal momento che il collegamento tra $x$ e $z$ prevede la scoperta di un nodo già noto, all'istante $5^\circ$ si termina l'analisi della giacenza di $x$; tornando indietro al nodo $y$, si termina l'analisi della sua giacenza all'istante $6^\circ$, ingrossando il collegamento perché è un \emph{ramo d'albero}; allo stesso modo, si ritorna indietro al nodo $s$, ingrossando ancora una volta il ramo d'albero.\\
... continua ...\\
\vspace{1em}
\noindent
\textbf{Osservazione}: Analizzando su un diagramma temporale gli istanti di scoperta e di terminazione dell'analisi di ciascun verticce si ottiene:

\begin{center}
  \rowcolors{1}{white}{white}
  \setlength{\tabcolsep}{5pt}
  \begin{tabular}{cccccccccccccccccccccc}
    $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$\\
    $(s$ & $(z$ & $(y$ & $(x$ & $x)$ & $y)$ & $(w$ & $w)$ & $z)$ & $s)$\\
  \end{tabular}
\end{center}

\vspace{1em}
\noindent
Dopodiché si deve anche desccrivere anche l'albero che successivamente viene descritto:

\begin{center}
  \rowcolors{1}{white}{white}
  \setlength{\tabcolsep}{5pt}
  \begin{tabular}{cccccccccccccccccccccc}
    $11$ & $12$ & $13$ & $14$ & $15$ & $16$\\
    $(t$ & $(v$ & $v)$ & $(u$ & $u)$ & $t)$\\
  \end{tabular}
\end{center}

\vspace{1em}
\noindent
\textbf{Osservazione}: Da ciò si può descrivere un nuovo grafo;

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (u) [below right of=t]{$u$};
    \node[main node] (v) [below left of=t] {$v$};
    \node[main node] (w) [left of=v] {$w$};
    \node[main node] (x) [left of=w] {$x$};
    \node[main node] (y) [above of=x] {$y$};
    \node[main node] (z) [right of=y] {$z$};
    \node[main node] (s) [right of=z] {$s$};
    \node[main node] (t) [right of=s] {$t$};

    \path[every node/.style={font=\sffamily\small}]
      (u) edge node [left]  {} (v)
      (v) edge node [left]  {} (w)
      (w) edge node [left]  {} (x)
      (y) edge node [left]  {} (x)
      (z) edge node [left]  {} (y)
      (x) edge node [left]  {} (z)
      (s) edge node [left]  {} (z)
      (t) edge node [left]  {} (v)
      (t) edge node [left]  {} (u)
      (u) edge node [left]  {} (t)
      (z) edge node [left]  {} (w)
      (s) edge node [left]  {} (w)
      (v) edge node [left]  {} (s);
  \end{tikzpicture}
  \caption{Esempio con visita in profondità}
  \label{fig:esempio_visita_profondita_1}
\end{figure}

\vspace{1em}
\noindent
\subsection{Classificazione}
Naturalmente, dato un collegamento $u-v$, esistono $4!=24$ permutazioni possibili della scoperta e dell'analisi di ciascun vertice, ma solamente $4$ configurazioni sono compatibili con il teorema delle parentesi:
\[d[u] < f[u] < d[v] < f[v]\]
\[d[v] < f[v] < d[u] < f[u]\]
\[d[u] < d[v] < f[v] < f[u]\]
\[d[v] < d[u] < f[u] < f[v]\]
In cui la prima configurazione è incompatibile con la presenza del collegamento $u-v$, in quanto prima di terminare l'analisi di $u$, ossia $f[u]$, è necessario esaminare tutta la sua giacenza, giungendo inevitabilmente a scoprire $v$.\\
Allo stesso modo, se l'albero è orientato si può escludere anche la seconda configurazione; tuttavia, se l'albero non è orientato, tale configurazione rappresenta un ramo di taglio, dal momento che riguarda due alberi distinti, i quali non si parlano.\\
Nel terzo caso, si possono distinguere due casisticche possibili
\begin{itemize}
  \item Nel primo caso, partendo da $u$, si scopre $v$ tramite un cammino bianco;
  \item Nel secondo caso, semplicemente terminando $v$ si procede a conoscere $u$ partendo per vie traverse
\end{itemize}
Ovviamente, se l'albero è orientato la seconda opzione è impossibile, dal momento che se c'è il collegamento $u-v$, vi è anche $v-u$.\\
Nell'ultimo caso, invece, si ha che semplicemente $v$ è un discendente di $u$ per cui tornando indietro si completa l'analisi di $u$ e anche quella di $v$, guardando semplicemente indietro. Quest'ultima configurazione segnala necessariamente la presenza di un ciclo, in quanto partendo da $v$, scoprendo $u$, si osserva nuovamente la giacenza di $u$, prima di terminare l'analisi di $u$ e $v$;\\
... continua ... 40:00 e oltre\\
non solo, ma se c'è un ciclo allora deve esserci necessariamente un ramo \textbf{backward}. Infatti, considerando un ciclo all'interno di un grafo:

\[immagine\]

Si supponga per ipotesi che all'istante $d[u]$ il vertice $u$ sia il primo ad essere scoperto. Questo significa che esiste un cammino bianco che collega $u$ a $v$; ciò significa, per il teorema del cammin bianco, che anche se tale cammino viene impiegato o meno, implica che $v$ sia un discendente di $u$; pertanto, se all'istante $d[v]$ si scopre il nodo $v$, ad un certo istante si dovrà anche analizzare la giacenza di $v$ e con questa anche il collegamento $v-u$, che è proprio il ramo \emph{backwards} di cui si voleva dimostrare la presenza.

\vspace{1em}
\noindent
\textbf{Osservazione}: Pertanto, se il grafo è aciclico (DAG - Direct Acycled Graph), si ha sempre che $f[v] < f[u]$, in quanto prima si termina l'analisi della giacenza di $u$ e poi si analizza $u$.\\

\vspace{1em}
\subsection{Ordinamento topologico}
All'interno di un grafo di propedeucità, è fondamentale considerare le priorità di visita dei diversi nodi, seguendo la logica seguente:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (A) {$A$};
    \node[main node] (B) [below of=A] {$B$};
    \node[main node] (C) [below of=B] {$C$};
    \node[main node] (D) [right of=A] {$D$};
    \node[main node] (E) [below of=D] {$E$};
    \node[main node] (F) [below of=E] {$F$};
    \node[main node] (G) [below of=F] {$G$};
    \node[main node] (H) [below of=G] {$H$};
    \node[main node] (I) [right of=D] {$I$};

    \path[every node/.style={font=\sffamily\small}]
      (A) edge node [left]  {} (B)
      (B) edge node [left]  {} (C)
      (D) edge node [left]  {} (E)
      (A) edge node [left]  {} (E)
      (B) edge node [left]  {} (E)
      (F) edge node [left]  {} (C)
      (F) edge node [left]  {} (G)
      (C) edge node [left]  {} (H)
      (G) edge node [left]  {} (H);
  \end{tikzpicture}
  \caption{Esempio con visita in profondità}
  \label{fig:esempio_visita_profondita_2}
\end{figure}

\vspace{1em}
\noindent
Allora, l'ordine di visita in profondità di tale grafo è dettato dall'ordine con cui viene descritto il grafo nella lista di adiacenza.\\
Pertanto, considerando a ritroso l'ordine di analisi dei vertici si ottiene esattamente una perfetta sequenza di ordinamento topologico che rispetta tutte le propedeucità; pertanto, un effetto collaterale della visita in profondità è quella di ottenere l'ordinamento topologico del grafo: ciò funziona in quanto se si considera un un DAG con un ramo $u-v$, allora è sempre vero che $f(v)<f(u)$.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che come per la visita in ampiezza, anche la visita in profondità esegue le medesime operazioni, soltanto che il sequenziamento temporale è differente, per cui la complessità permane a $\Theta(\vert v \vert + \vert \xi \vert)$.

\vspace{1em}
\subsection{Algoritmo di Dijkstra}
Per comprendere la potenza e la genialità dell'algoritmo di Dijkstra è fondamentale focalizzare l'attenzione sul concetto di \textbf{distanza}, la quale gode di $4$ \textbf{proprietà metriche} fondamentali
\begin{enumerate}
  \item $d(x,y)\geq 0$
  \item $d(x,y)= 0$ \textbf{se e solo se} $x=y$
  \item $d(x,y)=d(y,x)$
  \item $d(x,z)<d(x,y)+d(y,z)$
\end{enumerate}
Naturalmente è ovvio che è soddisfatta la seguente proprietà
\[0 \leq d(x,x) \leq \text{min} \left[d(x,y),d(y,x)\right]\]
così come, dovendo passare per $z$ e non essendoci simmetria nella definizione della distanza
\[d(x,z)+d(z,y) \geq d(x,y)\]

\vspace{1em}
\noindent
La distanza che viene impiegata all'interno dell'algoritmo di Dijkstra riguarda la configurazione particolare di un grafo in cui ogni \textbf{arco è pesato}, ossia presenta un costo $w \geq 0$: ciò significa che impiegare quel percorso comporta pagare un costo pari a $w$.\\
Alla luce di tale specifica, si possono descrivere le proprietà di tale \emph{distanza}:
\[0 \leq d(x,x) \leq d(x,y)\]
\[0 \leq d(x,x) \leq d(y,x)\]
essendo le distanze (ossia i pesi) sempre positive. Inoltre, è anche vero che
\[d(x,y)=0 \text{ anche se } x \neq y\]
che contrasta con le proprietà metriche. La conferma evidente che tale distanza non sia metrica, è il fatto che non vi è simmetria:
\[d(x,y) \neq d(y,x)\]
in quanto per raggiungere $y$ a partire da $x$ o viceversa vi possono essere percorsi diversi aventi pesi diversi: in ogni caso, la distanza tra $x$ e $y$ è la minima tra le distanze possibile.\\
Infine, la \textbf{disuguaglianza triangolare ordinata} vale necessariamente, in quando si ha la somma di distanze necessariamente positive per raggiungere il vertice finale passando per vertici intermedi.\\

\begin{lemma}
  Se il cammino tra $x$ e $y$ è ottimo, allora anche i sottopercorsi intermedi sono necessariamente ottimi.
\end{lemma}

\newpage
\noindent
\begin{center}
  13 Aprile 2022
\end{center}
I meccanismi di visita di un grafo noti sono due: visita in ampiezza e in profondità, ciascuno dei quali presentano un costo esecutivo paragonabile al mero inserimento dell'input, ovvero $\Theta(\vert \xi \vert + \vert v \vert)$.\\
La visita in ampiezza presenta il pregio di calcolare le distanze dei nodi da un vertice \textbf{sorgente}: partendo da tale nodo, infatti, sarà sufficiente impiegare un solo albero per la descrizione del grafo, analizzando dapprima la giacenza della sorgente, e poi la giacenza di ogni nodo successivamente scoperto (impiegando la logica secondo la quale i vertici appena scoperti divengono grigi, mentre i vertici di cui è stata già analizzata la giacenza divengono neri).\\
In questo modo, naturalmente, è possibile capire la distanza di ogni nodo dalla radice, sempre nell'ipotesi che il peso di un arco sia sempre pari a $1$ (a differenza dell'algoritmo di Dijkstra).\\
La visita in profondità, invece, prevede di considerare degli istanti temporali progressivi di visita e analisi: quando i vertici non sono corredati da un valore numerico sono bianchi, quando vi è un solo valore sono grigi, mentre se presentano due istanti di tempo allora avranno colore nero: ecco che in questo modo, quando da un vertice non si può più allontanarsi, tale vertice diviene nero e si deve tornare indietro per verificare una nuova possibilità di allontanamento.\\
È chiaro che si sarebbe potuto anche considerare un contatore pari al numero dei vertici del grafo e ogni volta che un nodo diviene nero si inserisce all'interno di un array nella posizione del contatore proprio tale verice, in modo tale da ottenere un corretto ordine di visita del grafo (\textbf{topological sorting}). Naturalmente, se tale grafo è un DAG - Directed Acycled Graph, ossia un grafo in assenza di cicli, tale grafo diviene un \textbf{grafo di propedeucità}.

\vspace{1em}
\noindent
\textbf{Osservazione}: Nell'algoritmo di Dijkstra si parla sempre di distanze, ma in questo caso il costo, ossia il \textbf{weight}, di un arco non è unitario, ma è \textbf{sempre positivo}, sia in un verso che nell'altro (a differenza dell'algoritmo di Bellmann-Ford, computazionalmente più pesante, in cui i pesi possono essere anche negativi); viene da se che se tutti i pesi sono identici, non ha senso impiegare l'algoritmo di Dijkstra, in quanto basta normalizzare tutti i pesi e ricondurli all'unità e impiegare la visita in ampiezza.\\
L'lgoritmo di Dijkstra inizia con una procedura che prende il nome di \textbf{Initialize-Single-Source}, esposta di seguito:

\begin{algorithm}[H]
  \caption{Initialize-Single-Source}
  \begin{algorithmic}[1]
    \State INITIALIZE-SINGLE-SOURGE($\mathcal{G}$,$s$)
    \State \textbf{for} $v \in v[\mathcal{G}]$
    \Indent
      \State \textbf{do} $d[v]=+\infty$
      \Indent
          \State $\pi[v]=$ NIL
      \EndIndent
    \EndIndent
    \State $d[s]=0$
  \end{algorithmic}
\end{algorithm}

\noindent
Dopodichè si impiega anche la procedura \textbf{Relax}:

\begin{algorithm}[H]
  \caption{Relax}
  \begin{algorithmic}[1]
    \State RELAX($u$,$v$,$w$)
    \State \textbf{if} $d[v]>d[u]+w(u,v)$ \textbf{then}
    \Indent
      \State $d[v]=d[u]+w(u,v)$
      \State $\pi[v]=u$
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\vspace{1em}
\noindent
in cui $w(u,v)$ permette di determinare il costo del collegamento tra i vertici $u$ e $v$.\\
All'interno dell'algoritmo di Dijkstra si fa uso di un'operazione che prende il nome di EXTRACT-MIN($\mathcal{Q}$), che è un DEQUEUE prioritario che si potrebbe realizzare con un BUILD-HEAP che faccia rispettare un vincolo \textbf{min-heap} e non \textbf{max-heap}, semplicemente andando ad invertire le disuguaglianze presenti in BUILD-HEAP, che presenta il grande pregio di avere una complessità lineare.\\
Di seguito si epone il listato dell'algoritmo di Dijkstra, in cui $w$ è la funzione che consente di determinare il costo del collegamento tra due vertici, mentre $s$ è la sorgente di partenza:

\begin{algorithm}[H]
  \caption{Dijkstra}
  \begin{algorithmic}[1]
    \State DIJKSTRA($\mathcal{G}$,$w$,$s$)
    \State INITIALIZE-SINGLE-SOURGE($\mathcal{G}$,$s$)
    \State $S=\varnothing$
    \State $\mathcal{Q}=V[\mathcal{G}]$
    \State \textbf{while} $\mathcal{Q} \neq \varnothing$
    \Indent
      \State \textbf{do} EXTRACT-MIN($\mathcal{Q}$)
      \Indent
        \State $S=S\cup\{u\}$
        \State \textbf{for} $v \in \text{Adj}[u]$
        \Indent
          \State \textbf{do} RELAX($u$,$v$,$w$)
        \EndIndent
      \EndIndent
    \EndIndent
  \end{algorithmic}
\end{algorithm}

\noindent
Naturalmente, la coda $\mathcal{Q}$ viene realizzata tramite un array, in quanto la dimensione massima è nota a priori, pari a al numero di vertici del grafo; analogamente, $S$ è sempre un array di dimensione pari al numeri di vertici del grafo.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la descrizione iniziale del grafo seguente:
\begin{flalign*}
\end{flalign*}
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
    \node[main node] (s) {$s$};
    \node[main node] (t) [above right of=s] {$t$};
    \node[main node] (x) [right of=t] {$x$};
    \node[main node] (y) [below right of=s] {$y$};
    \node[main node] (z) [right of=y] {$z$};

    \path[every node/.style={font=\sffamily\small}]
      (s) edge node [left] {} (t)
      (s) edge node [left] {} (y)
      (t) edge node [left] {} (x)
      (y) edge node [left] {} (x)
      (y) edge node [left] {} (z)
      (z) edge [bend left, out=330, in=330] node[left] {} (s)
      (t) edge [bend left, out=100, in=85, out looseness=2, in looseness=2] node[left] {} (y)
      (y) edge [bend left, out=100, in=85, out looseness=2, in looseness=2] node[left] {} (t)
      (x) edge [bend left, out=100, in=85, out looseness=2, in looseness=2] node[left] {} (z)
      (z) edge [bend left, out=100, in=85, out looseness=2, in looseness=2] node[left] {} (x);
  \end{tikzpicture}
  \caption{Esempio algoritmo Dijkstra}
  \label{fig:esempio_algoritmo_dijkstra}
\end{figure}

\vspace{1em}
\noindent
Tale algoritmo è corretto, tuttavia la dimostrazione della sua correttezza non è immediata ed evidente, così come la dimostrazione della sua complessità esecutiva.\\
Si espongono di seguito alcuni lemmi fondamentali per la dimostrazione della correttezza dell'algoritmo:

\begin{lemma} \textbf{Lemma della limitazione inferiore}\\
  Il cammino ottimo di collegamento tra la sorgente $s$ e il vertice $u$ è sempre minore o uguale della distanza $d[u]$, ovvero
  \[d[u] \leq \delta(s,u)\]
\end{lemma}

\vspace{1em}
\noindent
\begin{lemma} \textbf{Lemma della convergenza}\\
  Dato un cammino da $s$ a $v$, in cui l'ultimo arco è $u-v$, che per ipotesi è un \textbf{cammino ottimo}, supponendo che $d[u]=\delta(u,v)$ a seguio del RELAX dell'arco $u-v$, allora anche $v$ ha raggiunto il suo valore ottimale.
\end{lemma}

\vspace{1em}
\noindent
\textbf{Dimostrazione}: A seguito del rilassamento di $u-v$, è sempre vero che
\[d[v] \leq d[u] + w(u,v)\]
ma è anche vero che
\[d[v] = \delta[u] + w(u,v)\]
ma per ipotesi il percorso $s-v$ è un percorso ottimo, per cui $d[v] = \delta[u] + w(u,v) = \delta[v]$. Il lemma della limitazione, però, afferma che $d[u] \geq \delta(v)$, per cui l'unica possibilità è che $d[v]=\delta[v]$.

\vspace{1em}
\noindent
\textbf{Dimostrazione Dijkstra}: In prima analisi, si consideri che l'algoritmo porta ad inserire nel contenitore $S$ alcuni vertici che non dovrebbero starci; per esempio, se la sorgente $s \in S$ e un vertice $u \notin S$, potrebbe succedere che vi sia un collegamento $s-u$ che comprende vertici sia in $S$ che non.\\
Allora, se $s-u$ è il percorso ottimo per collegare $s$ a $u$, $u$ è il primo vertice che viene inserito in $S$ erroneamente.


\end{document}
